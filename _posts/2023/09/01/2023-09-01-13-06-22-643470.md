---
layout: post
title: "[파이썬] 웹 스크래핑과 크롤링 기본 절차"
description: " "
date: 2023-09-01
tags: [python]
comments: true
share: true
---

웹 스크래핑과 크롤링은 인터넷 상에서 웹 페이지의 데이터를 추출하는 기술입니다. Python은 웹 스크래핑과 크롤링 작업에 매우 유용한 프로그래밍 언어로 알려져 있습니다. 만약 웹 데이터를 추출하고 싶다면, 다음과 같은 기본 절차를 따를 수 있습니다:

1. **라이브러리 가져오기**: Python에는 웹 스크래핑을 위해 사용할 수 있는 다양한 라이브러리들이 있습니다. 그 중에서도 `requests`, `beautifulsoup4`, `selenium` 등이 자주 사용됩니다. 필요한 라이브러리를 가져옵니다:

```python
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
```

2. **웹 페이지에 접속하기**: 스크래핑하려는 웹 페이지에 접속해야 합니다. `requests` 라이브러리를 사용하여 GET 요청을 보내고, 해당 페이지의 HTML을 가져옵니다:

```python
url = "https://example.com"
response = requests.get(url)
html = response.text
```

3. **데이터 추출하기**: `beautifulsoup4` 라이브러리를 사용하여 웹 페이지의 HTML에서 원하는 데이터를 추출합니다. 이 라이브러리를 사용하면 웹 페이지를 파싱하고, 요소를 검색하거나 조작할 수 있습니다. 예를 들어, 모든 링크를 추출하려는 경우 다음과 같이 작성할 수 있습니다:

```python
soup = BeautifulSoup(html, "html.parser")
links = soup.find_all("a")

for link in links:
    print(link.get("href"))
```

4. **자동화를 위해 브라우저 사용하기**: 웹 페이지가 동적으로 로딩되는 경우, JavaScript를 실행해야 하는 경우에는 `selenium` 라이브러리를 사용할 수 있습니다. 이 라이브러리를 사용하면 웹 브라우저를 자동으로 조작하여 데이터를 추출할 수 있습니다. 예를 들어, 웹 페이지의 스크롤을 자동으로 내리고 추가 데이터를 로드하려는 경우, 다음과 같이 작성할 수 있습니다:

```python
driver = webdriver.Chrome()
driver.get(url)

# 웹 페이지 스크롤하기
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

# 추가 데이터 로드 후 추출하기
html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
# 데이터 추출 작업...
```

5. **데이터 저장하기**: 추출한 데이터를 원하는 형식으로 저장할 수 있습니다. 예를 들어, CSV 파일로 저장하고 싶은 경우 `csv` 라이브러리를 사용하여 데이터를 파일에 작성할 수 있습니다:

```python
import csv

data = [["Title", "Link"], ["Title 1", "Link 1"], ["Title 2", "Link 2"]]
filename = "data.csv"

with open(filename, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(data)
```

위의 기본 절차를 따라 웹 스크래핑과 크롤링 작업을 수행할 수 있습니다. Python의 강력한 라이브러리들을 활용하여 웹 데이터를 추출하고 분석하는 데에 유용하게 사용할 수 있습니다.
---
layout: post
title: "[파이썬] 텍스트 분류를 위한 CNN 및 RNN 기반 모델"
description: " "
date: 2023-09-05
tags: [python]
comments: true
share: true
---

텍스트 분류는 자연어 처리의 중요한 과제 중 하나로, 주어진 텍스트를 여러 개의 사전 정의된 카테고리로 분류하는 작업입니다. 이러한 텍스트 분류 작업을 위해 CNN(Convolutional Neural Network) 및 RNN(Recurrent Neural Network) 기반 모델을 사용할 수 있습니다. 이 블로그 포스트에서는 파이썬을 사용하여 CNN 및 RNN 기반 텍스트 분류 모델을 구현하는 방법을 알아보겠습니다. 

## CNN을 사용한 텍스트 분류 모델
CNN은 이미지 처리에 주로 사용되는 신경망 구조이지만, 텍스트 분류에도 효과적으로 적용될 수 있습니다. 아래는 CNN을 사용하여 텍스트 분류를 수행하는 예제 코드입니다.

```python
import tensorflow as tf
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense
from tensorflow.keras.models import Sequential

# 데이터 로드 및 전처리
data = load_text_data()
labels = load_labels()

tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences)

# 모델 구성
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=padded_sequences.shape[1]))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(units=len(labels), activation='softmax'))

# 모델 컴파일 및 학습
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(padded_sequences, labels, epochs=10, batch_size=32)
```

위 코드에서는 텍스트 데이터를 로드한 후, `Tokenizer`를 사용하여 텍스트를 숫자로 변환합니다. 그리고 숫자로 변환된 텍스트 데이터를 패딩 처리하여 모든 시퀀스의 길이를 동일하게 만든 후, CNN 기반의 텍스트 분류 모델을 구성합니다. 마지막으로, 모델을 컴파일하고 학습시킵니다.

## RNN을 사용한 텍스트 분류 모델
RNN은 순서가 있는 데이터, 특히 텍스트와 같은 시퀀스 데이터 처리에 효과적인 신경망 구조입니다. 아래는 RNN을 사용하여 텍스트 분류를 수행하는 예제 코드입니다.

```python
import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Sequential

# 데이터 로드 및 전처리
data = load_text_data()
labels = load_labels()

tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences)

# 모델 구성
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=padded_sequences.shape[1]))
model.add(LSTM(units=128))
model.add(Dense(units=len(labels), activation='softmax'))

# 모델 컴파일 및 학습
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(padded_sequences, labels, epochs=10, batch_size=32)
```

위 코드에서는 CNN과 유사하게 텍스트 데이터를 준비한 후, RNN 기반의 텍스트 분류 모델을 구성하고 학습합니다. RNN은 LSTM(Long Short-Term Memory) 셀을 사용하여 시퀀스 데이터의 의미를 잘 이해할 수 있습니다.

CNN과 RNN은 각각의 장단점이 있으며, 텍스트 분류 작업에 따라 사용할 모델을 선택할 필요가 있습니다. 적절한 모델 선택과 하이퍼파라미터 조정을 통해 좋은 성능을 얻을 수 있습니다.
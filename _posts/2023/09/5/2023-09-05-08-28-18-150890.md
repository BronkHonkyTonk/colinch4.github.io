---
layout: post
title: "[파이썬] Word2Vec, GloVe, FastText 등의 임베딩 모델"
description: " "
date: 2023-09-05
tags: [python]
comments: true
share: true
---

In Natural Language Processing (NLP), word embedding models are used to represent words as numerical vectors. These models capture the semantic meaning and relationships between words, enabling machines to understand and process natural language.

In this blog post, we will explore three popular word embedding models: Word2Vec, GloVe, and FastText, and discuss how to use them in Python.

## 1. Word2Vec
Word2Vec, developed by Google, is a widely used word embedding algorithm that learns word representations from a large corpus of text data. It uses neural networks to train continuous word vectors by predicting the context words given a target word or vice versa.

### Example code using Word2Vec:

```python
from gensim.models import Word2Vec
sentences = [['I', 'like', 'cats'], ['I', 'like', 'dogs'], ['Dogs', 'are', 'cute']]
model = Word2Vec(sentences, size=100, window=5, min_count=1)
vector = model.wv['dogs']
similar_words = model.wv.most_similar('dogs')
```

## 2. GloVe
GloVe, short for Global Vectors for Word Representation, is another popular method for generating word embeddings. It combines the advantages of both global matrix factorization techniques and local context window methods. GloVe learns word vectors by factorizing the co-occurrence matrix of words.

### Example code using GloVe:

```python
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors
glove_file = 'glove.6B.100d.txt'
word2vec_file = 'glove.6B.100d.word2vec'
glove2word2vec(glove_file, word2vec_file)
model = KeyedVectors.load_word2vec_format(word2vec_file, binary=False)
vector = model['dog']
similar_words = model.most_similar('dog')
```

## 3. FastText
FastText, developed by Facebook AI Research, is an extension of Word2Vec that incorporates subword information. It represents words as bags of character n-grams and uses a hierarchical softmax to compute word probabilities. FastText is particularly effective for handling out-of-vocabulary words and capturing morphological information.

### Example code using FastText:

```python
from gensim.models import FastText
sentences = [['I', 'like', 'cats'], ['I', 'like', 'dogs'], ['Dogs', 'are', 'cute']]
model = FastText(sentences, size=100, window=5, min_count=1)
vector = model.wv['dogs']
similar_words = model.wv.most_similar('dogs')
```

## Conclusion
Word2Vec, GloVe, and FastText are powerful word embedding models that can be used to improve the performance of various NLP tasks such as text classification, sentiment analysis, and machine translation. By converting words into numerical vectors, these models enable machines to understand and process human language more effectively.

Remember to preprocess your text data before training these models to obtain better word representations. Experiment with different parameters and compare the performance of these models on your specific NLP task to find the best fit.

With the availability of pre-trained models, it is now easier than ever to incorporate word embeddings into your projects and leverage the power of natural language understanding. So, go ahead and give these embedding models a try!
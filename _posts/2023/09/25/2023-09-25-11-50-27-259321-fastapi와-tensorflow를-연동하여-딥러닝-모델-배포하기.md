---
layout: post
title: "FastAPI와 TensorFlow를 연동하여 딥러닝 모델 배포하기"
description: " "
date: 2023-09-25
tags: [FastAPI]
comments: true
share: true
---

딥러닝 모델을 실제 서비스로 배포하기 위해서는 웹 애플리케이션 프레임워크와 딥러닝 프레임워크 사이의 연동이 필요합니다. 

이번 포스트에서는 FastAPI 웹 프레임워크와 TensorFlow 딥러닝 프레임워크를 연동하여 딥러닝 모델을 배포하는 방법을 알아보겠습니다.

### FastAPI란?

FastAPI는 Python 기반의 빠르고 (Fast) 간단한 (API) 웹 애플리케이션 프레임워크입니다. 

장점:
- 강력한 유효성 검사 기능을 제공하여 안정적인 API를 작성할 수 있습니다.
- 자동 생성된 API 문서를 제공하여 편리한 API 문서화를 지원합니다.
- 빠른 성능을 자랑하며, ASGI 서버를 사용하여 높은 동시성 처리를 지원합니다.

### TensorFlow란?

TensorFlow는 딥러닝을 위한 오픈소스 프레임워크로서, 다양한 딥러닝 모델을 구축하고 훈련시킬 수 있습니다.

장점:
- 다양한 플랫폼 (CPU, GPU, TPU)에서 실행할 수 있어 확장성과 성능을 보장합니다.
- 다양한 딥러닝 알고리즘과 레이어를 제공하여 모델 구축을 용이하게 합니다.
- TensorFlow Serving을 통해 손쉬운 모델 배포가 가능합니다.

### FastAPI와 TensorFlow 연동하기

1. 필요한 패키지 설치하기

FastAPI와 TensorFlow 패키지를 설치합니다.

```python
pip install fastapi tensorflow
```

2. FastAPI 애플리케이션 작성하기

FastAPI를 사용하여 간단한 애플리케이션을 작성합니다.

```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def hello():
    return {"message": "Hello, World!"}
```

3. 딥러닝 모델 로딩하기

TensorFlow를 사용하여 딥러닝 모델을 로딩합니다.

```python
import tensorflow as tf

model = tf.keras.models.load_model("model.h5")
```

4. 모델 예측하기

FastAPI 엔드포인트에서 입력 데이터를 받아 모델을 이용하여 예측합니다.

```python
@app.post("/predict")
def predict(data: List[float]):
    data = tf.expand_dims(data, axis=0)
    pred = model.predict(data)
    return {"prediction": pred}
```

5. FastAPI 서버 실행하기

FastAPI 애플리케이션을 실행하고, 브라우저나 API 클라이언트를 통해 모델에 대한 예측을 요청할 수 있습니다.

```python
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
```

위와 같은 방법으로 FastAPI와 TensorFlow를 연동하여 딥러닝 모델을 배포할 수 있습니다. 

딥러닝 모델을 웹 애플리케이션에 통합함으로써 쉽고 편리하게 모델을 사용하고, API를 통해 예측 결과를 손쉽게 얻을 수 있습니다.

#FastAPI #TensorFlow
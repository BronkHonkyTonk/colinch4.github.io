---
layout: post
title: "[파이썬] PyTorch 메모리 최적화 기법"
description: " "
date: 2023-09-07
tags: [python,PyTorch]
comments: true
share: true
---

PyTorch는 딥러닝을 위한 강력한 프레임워크로 알려져 있습니다. 그러나 대규모 모델 및 큰 데이터셋을 다룰 때 메모리 사용량이 큰 문제가 될 수 있습니다. 이러한 메모리 제한을 극복하고 모델의 효율성을 높이기 위해 몇 가지 메모리 최적화 기법을 소개하겠습니다.

## 1. Gradient Accumulation

Gradient Accumulation은 메모리를 절약할 수 있는 효과적인 기법입니다. 일반적으로 매 번 backward 호출 시 가중치 업데이트를 수행하게 되는데, Gradient Accumulation을 사용하면 여러 배치의 그래디언트를 누적하여 업데이트를 수행합니다. 이를 통해 더 큰 배치 크기를 사용하면서도 메모리 사용을 감소시킬 수 있습니다.

```python
optimizer.zero_grad()
for i, (inputs, labels) in enumerate(data_loader):
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    if (i+1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

- `optimizer.zero_grad()`는 그래디언트를 초기화합니다.
- `optimizer.step()`은 가중치 업데이트를 수행합니다.
- `optimizer.zero_grad()`는 다음 배치를 위해 그래디언트를 초기화합니다.

## 2. In-place 연산 피하기

PyTorch는 텐서의 자동 미분을 지원하기 위해 in-place(원본 텐서를 바로 수정하는) 연산을 사용합니다. 하지만 in-place 연산은 메모리 사용을 증가시킬 수 있으므로 가능한 피하는 것이 좋습니다. 이를 위해 연산 결과를 새로운 변수에 할당하거나 `.clone()` 메서드를 사용하여 새로운 텐서로 복사할 수 있습니다.

```python
x = torch.randn(3, 3)
# 피하기
y = torch.relu(x)
# 추천
y = torch.relu(x).clone()
```

## 3. 메모리 핀

PyTorch는 기본적으로 CPU와 GPU 사이의 데이터 복사를 자동으로 처리합니다. 하지만 데이터가 너무 커서 메모리 복사에 시간이 오래 걸릴 경우에는 메모리 핀을 사용하여 CPU와 GPU 사이의 복사를 피할 수 있습니다. 메모리 핀을 사용하면 데이터가 CPU 메모리에 고정되어 GPU로 직접 접근할 수 있습니다.

```python
x = torch.randn(3, 3).pin_memory()
```

- `.pin_memory()` 메서드를 사용하여 메모리 핀을 설정합니다.

## 4. 데이터 로딩 최적화

대규모 데이터셋을 로딩할 때도 메모리 사용량을 줄일 수 있는 최적화 방법이 있습니다.

- 데이터셋이 너무 커서 메모리에 한 번에 로드할 수 없을 경우, `torch.utils.data.Dataset`과 `torch.utils.data.DataLoader`를 사용하여 배치 단위로 데이터를 로드합니다.
- `num_workers` 매개변수를 사용하여 데이터 로딩에 병렬 처리를 사용할 수 있습니다. 그러나 너무 많은 작업자를 설정하면 오히려 성능이 저하될 수 있으니 조정이 필요합니다.

```python
dataset = MyDataset(...)
data_loader = DataLoader(dataset, batch_size=64, num_workers=4)
```

위의 기법들을 사용하여 PyTorch 모델의 메모리 사용량을 최적화할 수 있습니다. 큰 모델과 데이터셋을 다룰 때는 메모리 최적화가 중요한 요소이므로, 적절한 기법을 선택하여 메모리 사용량을 효과적으로 관리하는 것이 좋습니다.
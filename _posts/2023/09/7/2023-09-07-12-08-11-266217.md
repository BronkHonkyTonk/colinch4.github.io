---
layout: post
title: "[파이썬] lightgbm 자동 하이퍼파라미터 최적화"
description: " "
date: 2023-09-07
tags: [lightgbm]
comments: true
share: true
---

LightGBM은 기계 학습 알고리즘으로, 비교적 빠른 훈련 속도와 높은 성능으로 알려져 있습니다. 하지만, LightGBM의 하이퍼파라미터를 최적화하는 것은 어려운 작업일 수 있습니다. 많은 하이퍼파라미터 조합을 시도하고, 각각의 조합에 대해 모델을 훈련 및 평가해야하기 때문입니다.

이러한 문제를 해결하기 위해 자동 하이퍼파라미터 최적화를 사용할 수 있습니다. 자동 최적화 도구는 주어진 하이퍼파라미터 공간을 탐색하고, 최적의 조합을 찾는 데 도움을 줍니다. 이번 블로그 포스트에서는 Python에서 LightGBM의 자동 하이퍼파라미터 최적화를 어떻게 수행할 수 있는지에 대해 알아보겠습니다.

## Hyperopt를 사용한 자동 최적화

Hyperopt는 Python에서 자동 하이퍼파라미터 최적화를 수행할 수 있는 라이브러리 중 하나입니다. 다음은 Hyperopt를 사용하여 LightGBM의 자동 최적화를 수행하는 예제 코드입니다:

```python
import lightgbm as lgb
from hyperopt import fmin, tpe, hp, Trials

# 최적화할 하이퍼파라미터 공간 정의
space = {
    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),
    'objective': hp.choice('objective', ['regression', 'binary', 'multiclass']),
    'learning_rate': hp.loguniform('learning_rate', -6, 0),
    'num_leaves': hp.quniform('num_leaves', 2, 50, 1),
    'max_depth': hp.choice('max_depth', [-1, 3, 5, 10]),
    'min_child_samples': hp.quniform('min_child_samples', 1, 20, 1)
}

# 목적 함수 정의
def objective(params):
    # LightGBM 모델 훈련 및 검증
    model = lgb.train(params, train_set, valid_sets=[valid_set])
    # 검증 세트에 대한 RMSE 계산
    rmse = np.sqrt(mean_squared_error(y_valid, model.predict(X_valid)))
    return rmse

# 최적화 실행
trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)
```

위의 코드에서, `space` 변수는 최적화할 하이퍼파라미터의 공간을 정의합니다. `objective` 함수는 주어진 하이퍼파라미터를 사용하여 LightGBM 모델을 훈련하고, 검증 세트에 대한 RMSE를 계산하는 목적 함수입니다. `fmin` 함수를 사용하여 최적화를 실행하고, 최적의 하이퍼파라미터 조합을 찾을 수 있습니다.

## Optuna를 사용한 자동 최적화

Optuna는 또 다른 자동 하이퍼파라미터 최적화 라이브러리입니다. 다음은 Optuna를 사용하여 LightGBM의 자동 최적화를 수행하는 예제 코드입니다:

```python
import lightgbm as lgb
import optuna

# 목적 함수 정의
def objective(trial):
    # 하이퍼파라미터 선택
    boosting_type = trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss'])
    objective = trial.suggest_categorical('objective', ['regression', 'binary', 'multiclass'])
    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)
    num_leaves = trial.suggest_int('num_leaves', 2, 50)
    max_depth = trial.suggest_int('max_depth', -1, 10)
    min_child_samples = trial.suggest_int('min_child_samples', 1, 20)

    # LightGBM 모델 훈련 및 검증
    model = lgb.LGBMRegressor(boosting_type=boosting_type, objective=objective,
                              learning_rate=learning_rate, num_leaves=num_leaves,
                              max_depth=max_depth, min_child_samples=min_child_samples)
    model.fit(X_train, y_train)
    
    # 검증 세트에 대한 RMSE 계산
    rmse = np.sqrt(mean_squared_error(y_valid, model.predict(X_valid)))
    return rmse

# 최적화 실행
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
```

위의 코드에서, `objective` 함수는 Optuna가 최적화를 위해 호출하는 목적 함수입니다. `suggest_` 메서드를 사용하여 하이퍼파라미터를 선택합니다. `create_study` 함수를 사용하여 최적화를 위한 Optuna Study 객체를 생성하고, `optimize` 메서드를 사용하여 최적화를 실행합니다.

## 결론

이번 블로그 포스트에서는 LightGBM의 자동 하이퍼파라미터 최적화를 위해 두 가지 도구인 Hyperopt와 Optuna에 대해 알아보았습니다. 이러한 도구들을 사용하면 많은 시간과 노력을 들이지 않고도 LightGBM 모델의 최적 하이퍼파라미터 조합을 찾을 수 있습니다. 

자동 최적화를 통해 모델 성능을 향상시키고, 더 나은 예측 결과를 얻을 수 있습니다. 추가로, 본인의 데이터와 문제에 맞는 다양한 하이퍼파라미터 조합을 실험해보는 것도 좋은 방법입니다.
---
layout: post
title: "[파이썬] requests 웹 크롤링 대상 사이트의 로봇 배제 표준 처리"
description: " "
date: 2023-09-07
tags: [python,requests]
comments: true
share: true
---

로봇 배제 표준은 웹사이트 소유자가 그 사이트에 접근하는 웹 로봇들에게 특정한 행동 규칙을 적용하는 규약입니다. 일부 웹사이트는 웹 크롤러가 접근할 수 없도록 로봇 배제 표준을 설정하여 크롤링을 제한할 수 있습니다. `robots.txt` 파일은 웹사이트 루트 디렉터리에 위치하고, 크롤러에게 접근 규칙을 알려줍니다.

Python의 `requests` 라이브러리는 HTTP 요청을 보내고 웹사이트의 내용을 크롤링하는 데 사용됩니다. 이번 튜토리얼에서는 `requests`를 사용하여 웹 크롤링 대상 사이트의 로봇 배제 표준 처리하는 방법을 알아보겠습니다.

## 1. `robots.txt` 파일 확인

로봇 배제 표준 처리를 하기 전에 해당 사이트의 `robots.txt` 파일을 확인해야 합니다. 웹 사이트의 `robots.txt` 파일에는 크롤러에 대한 접근 규칙과 제한사항이 기술되어 있습니다. `robots.txt` 파일은 일반적으로 웹 사이트의 루트 디렉터리에 위치하며, `robots.txt` 파일의 경로는 `https://example.com/robots.txt`와 같이 표시됩니다.

`robots.txt` 파일을 확인하여 웹 사이트에서 크롤링을 허용하거나 제한하는 방식을 파악할 수 있습니다. 크롤러는 `User-agent` 지시자를 통해 자신을 식별하고, `Disallow` 지시자를 통해 접근이 제한된 디렉터리를 확인할 수 있습니다.

## 2. `requests`를 이용한 크롤링

웹 크롤링을 위해 `requests` 라이브러리를 설치해야 합니다. 다음 명령을 사용하여 `requests`를 설치합니다.

```shell
pip install requests
```

```python
import requests

def get_page(url):
    response = requests.get(url)
    html_content = response.text
    return html_content

# 크롤링 대상의 URL
url = "https://example.com"

# robots.txt 파일 URL
robots_url = url + "/robots.txt"

# robots.txt 파일 내용 가져오기
robots_content = get_page(robots_url)

print(robots_content)
```

위의 코드에서 `get_page` 함수는 주어진 URL로 HTTP GET 요청을 보내고, 웹 페이지의 내용을 가져옵니다. `response.text`를 통해 HTML 내용을 추출할 수 있습니다.

`url` 변수는 웹 크롤링 대상 사이트의 URL을 저장하고, `robots_url` 변수에서 `"/robots.txt"`를 추가하여 `robots.txt` 파일의 URL을 생성합니다. `get_page` 함수에 `robots_url`을 전달하여 `robots.txt` 파일의 내용을 가져옵니다.

## 3. 로봇 배제 표준 확인

위의 코드를 실행하면 `robots.txt` 파일의 내용이 출력될 것입니다. 이를 통해 사이트에서 어떤 디렉터리가 크롤링이 허용되는지 또는 제한되는지 확인할 수 있습니다. 

`robots.txt` 파일의 내용은 웹 크롤링을 수행할 때 유용한 정보를 제공할 수 있으므로, 크롤링 전에 항상 확인하는 것이 좋습니다.

## 요약

Python의 `requests` 라이브러리를 사용하여 웹 크롤링 대상 사이트의 로봇 배제 표준 처리 방법을 알아보았습니다. `robots.txt` 파일을 확인하여 웹 사이트에서 크롤링을 허용하거나 제한하는 방식을 파악하는 것은 크롤링 작업을 수행할 때 중요한 과정입니다. 이를 통해 효율적이고 규칙적인 크롤링 작업을 수행할 수 있습니다.
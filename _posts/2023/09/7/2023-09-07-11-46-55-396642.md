---
layout: post
title: "[파이썬] fastai 원-핫 인코딩 및 임베딩"
description: " "
date: 2023-09-07
tags: [fastai]
comments: true
share: true
---

In natural language processing (NLP) tasks, **text preprocessing** plays a crucial role in converting raw text data into a format that machine learning algorithms can understand. One common technique used in NLP is **one-hot encoding**. The fastai library provides convenient functions for performing one-hot encoding and **embedding** in Python.

## 원-핫 인코딩 (One-Hot Encoding)

One-hot encoding is a technique used to convert categorical variables, such as words or characters, into binary vectors. Each category is represented by a unique index, and the corresponding binary vector has all values set to zero except for the index representing the category. This technique is often used to represent words or characters in text data.

To perform one-hot encoding in fastai, you can use the `one_hot` function from the `fastai.text.transform` module. Here's an example:

```python
from fastai.text.transform import one_hot

text = "Hello World!"
encoded_text = one_hot(text, max_vocab=10)
print(encoded_text)
```

Output:
```
tensor([5, 6])
```

In the above example, the `one_hot` function converts the text "Hello World!" into a one-hot encoded representation. The `max_vocab` parameter determines the maximum number of unique categories (vocabulary) in the encoded representation.

## 임베딩 (Embedding)

Embedding is another technique used in NLP to represent words or characters in a continuous vector space. It maps each word or character to a low-dimensional vector, where similar words or characters are closer to each other in the vector space. Embeddings are often used as input features for machine learning models in NLP tasks.

The fastai library provides an `Embedding` module, which can be used to create embeddings from one-hot encoded representations. Here's an example:

```python
from fastai.text.models import Embedding

vocab_size = 10
embedding_dim = 5
emb = Embedding(vocab_size, embedding_dim)
print(emb.forward(torch.LongTensor([5, 6])))
```

Output:
```
tensor([[ 0.0321,  0.1598, -0.4714,  0.1458,  0.1803],
        [-0.3738, -0.0382,  0.4591,  0.1469, -0.1423]], grad_fn=<EmbeddingBackward>)
```

In the above example, an `Embedding` module is created with a vocabulary size of 10 and an embedding dimension of 5. The `forward` method takes a tensor of one-hot encoded representations as input and returns the corresponding embeddings.

By using these functions and modules from the fastai library, you can easily perform one-hot encoding and embedding in your NLP tasks. These techniques are essential for transforming raw text data into a format that can be used for machine learning algorithms.
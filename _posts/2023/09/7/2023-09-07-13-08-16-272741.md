---
layout: post
title: "[파이썬] requests 웹 크롤러 작성 시 주의사항"
description: " "
date: 2023-09-07
tags: [requests]
comments: true
share: true
---

웹 크롤링은 데이터를 수집하고 분석하는 데 매우 유용한 기술입니다. 파이썬의 `requests` 라이브러리는 웹 크롤링 작업에 많이 사용되어 왔습니다. 웹 크롤러를 작성할 때에는 몇 가지 주의사항을 염두에 두는 것이 좋습니다. 

## 1. 웹사이트의 로봇 제약 규칙을 준수하자
로봇 배제 표준인 `robots.txt` 파일은 웹사이트 관리자가 크롤러가 액세스 가능하거나 제한되는 URL을 지정하는 파일입니다. `robots.txt` 파일을 확인하여 크롤러가 허용되는 URL에만 액세스하도록 해야합니다. `requests`를 사용하여 웹사이트에 요청하기 전에 반드시 로봇 제약 규칙을 확인하는 것이 좋습니다.

```python
import requests

def check_robots_txt(url):
    response = requests.get(url + "/robots.txt")
    if response.status_code == 200:
        print(response.text)
    else:
        print("robots.txt 파일을 찾을 수 없습니다.")

check_robots_txt("https://www.example.com")
```

## 2. 너무 많은 요청을 보내지 않기
웹사이트의 서버 부하를 줄이기 위해 너무 많은 요청을 보내지 않는 것이 중요합니다. 너무 자주 요청하는 경우, 웹사이트는 크롤링 작업을 감지하고 IP를 차단할 수 있습니다. 따라서 크롤링 작업을 실행할 때는 angstrom
일정한 간격을 두고 요청을 보내야 합니다.

```python
import time
import requests

def crawl_website(url):
    while True:
        response = requests.get(url)
        # 크롤링 작업 수행
        time.sleep(5)  # 5초 동안 대기

crawl_website("https://www.example.com")
```

## 3. 예외 처리하기
크롤링 작업 도중 오류가 발생할 수 있으므로 예외 처리를 통해 이를 처리해야 합니다. `requests`의 요청 작업은 예외를 발생시킬 수 있으므로 이를 적절히 처리해야합니다.

```python
import requests

def crawl_website(url):
    try:
        response = requests.get(url)
        # 크롤링 작업 수행
    except requests.exceptions.RequestException as e:
        print("오류가 발생했습니다:", e)

crawl_website("https://www.example.com")
```

위의 주의사항을 염두에 두면서 `requests`를 사용하여 웹 크롤러를 작성해보세요. 안전하고 효율적인 크롤링 작업을 수행할 수 있을 것입니다.
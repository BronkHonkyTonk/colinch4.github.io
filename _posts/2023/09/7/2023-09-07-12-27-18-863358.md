---
layout: post
title: "[파이썬] Keras 미세 조정 및 적용"
description: " "
date: 2023-09-07
tags: [python,Keras]
comments: true
share: true
---

Keras is a popular deep learning framework that allows us to build and train neural networks with ease. One of the powerful capabilities of Keras is the ability to perform fine-tuning, a technique that enables us to take a pre-trained model and adapt it for a new task or dataset by training some of its layers while keeping the learned weights of other layers intact.

In this blog post, we will explore the process of fine-tuning in Keras and discuss how to apply it to improve the performance of our models. We will cover the following topics:

1. Understanding Fine-tuning
2. Preparing the Pre-trained Model
3. Freezing Layers
4. Training the Model
5. Evaluating the Model
6. Unfreezing Layers
7. Fine-tuning the Model
8. Conclusion

Let's dive in!

## 1. Understanding Fine-tuning

Fine-tuning is a transfer learning technique where we leverage the features learned by a pre-trained model on a large dataset to improve the performance of a model on a smaller dataset or a different task. It allows us to take advantage of the pre-trained model's ability to extract meaningful features while adapting it to our specific problem.

## 2. Preparing the Pre-trained Model

To perform fine-tuning, we first need to select a pre-trained model. Keras provides several popular pre-trained models such as VGG16, ResNet50, and InceptionV3. We can load a pre-trained model using the `keras.applications` module. For example, to load the VGG16 model:

```python
from keras.applications import VGG16

model = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
```

Here, we specify the weights parameter as "imagenet" to load the pre-trained weights, include_top=False to exclude the fully connected layers at the top of the network, and input_shape as the desired input shape for our data.

## 3. Freezing Layers

Once we have the pre-trained model, we can freeze some of its layers to prevent their weights from being updated during training. Freezing layers is useful when we have limited training data or when we want to keep the learned representations from the pre-trained model intact. We can freeze layers by setting their `trainable` attribute to False. For example, to freeze the first five layers of the VGG16 model:

```python
for layer in model.layers[:5]:
    layer.trainable = False
```

## 4. Training the Model

After preparing the pre-trained model and freezing the desired layers, we can proceed to train the model on our dataset. We need to compile the model with an optimizer, loss function, and optional evaluation metrics before training. For example:

```python
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))
```

Here, `x_train` and `y_train` are the training data and labels, and `x_val` and `y_val` are the validation data and labels.

## 5. Evaluating the Model

After training, we can evaluate the performance of our model on a test set or any unseen data. We can use the `evaluate` method of the model to compute the loss and metrics. For example:

```python
loss, accuracy = model.evaluate(x_test, y_test)
print("Loss:", loss)
print("Accuracy:", accuracy)
```

## 6. Unfreezing Layers

If we find that the performance of our model is not satisfactory, we can unfreeze some or all of the previously frozen layers and fine-tune them along with the newly added layers. We can unfreeze layers by setting their `trainable` attribute to True. For example, to unfreeze all the layers of the VGG16 model:

```python
for layer in model.layers:
    layer.trainable = True
```

## 7. Fine-tuning the Model

After unfreezing the desired layers, we can continue training the model to further improve its performance. We can use a smaller learning rate for fine-tuning to prevent drastic changes to the pre-trained weights. For example:

```python
model.compile(optimizer=Adam(lr=0.0001), loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))
```

## 8. Conclusion

In this blog post, we have explored the process of fine-tuning in Keras and discussed how to apply it to improve the performance of our models. We have covered the steps of preparing the pre-trained model, freezing and unfreezing layers, training the model, evaluating its performance, and fine-tuning for further improvement.

Fine-tuning is a powerful technique that can save time and computational resources by building upon pre-trained models. It allows us to leverage the knowledge learned from large datasets and apply it to new tasks or smaller datasets. So, if you have a specific problem to solve, consider giving fine-tuning a try with Keras!
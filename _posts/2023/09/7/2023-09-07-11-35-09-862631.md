---
layout: post
title: "[파이썬] lightgbm 불균형 데이터셋 처리"
description: " "
date: 2023-09-07
tags: [python,lightgbm]
comments: true
share: true
---

불균형 데이터셋은 클래스 레이블 간에 불균형이 있는 경우를 의미합니다. 예를 들어, 이진 분류 문제에서 한 클래스의 샘플 수가 다른 클래스에 비해 월등히 많은 경우를 말합니다. 이러한 불균형 데이터셋을 다룰 때, 일반적인 머신러닝 모델은 저조한 성능을 보일 수 있습니다. 그러나, LightGBM 알고리즘은 불균형 데이터셋을 효과적으로 처리할 수 있는 기능을 제공합니다.

LightGBM은 Gradient Boosting Decision Tree(GBDT) 알고리즘을 기반으로 하며, 비대칭 데이터셋을 처리하기 위한 여러 가지 기법을 사용합니다. 이를 통해 불균형 데이터셋에서 성능을 향상시킬 수 있습니다. 이제 몇 가지 LightGBM의 주요 기능을 살펴보겠습니다.

## 1. 클래스 가중치 설정

클래스 가중치는 LightGBM에서 사용되는 중요한 기법 중 하나입니다. 이는 손실 함수를 계산할 때 각 클래스의 중요성을 반영하기 위해 사용됩니다. 불균형 데이터셋에서는 소수 클래스에 더 높은 가중치를 할당하여 모델이 소수 클래스를 올바르게 학습하도록 유도할 수 있습니다.

```python
import lightgbm as lgb
import numpy as np

# 클래스 가중치 설정
class_weights = [0.2, 0.8]  # 예시: 0번 클래스 가중치는 0.2, 1번 클래스 가중치는 0.8
lgb_train = lgb.Dataset(X_train, y_train, weight=np.array([class_weights[label] for label in y_train]))

# 모델 학습
params = {
    'objective': 'binary',
    'metric': 'binary_logloss'
}
model = lgb.train(params, lgb_train)
```

## 2. 언더샘플링과 오버샘플링

불균형 데이터셋에는 언더샘플링과 오버샘플링이라는 두 가지 주요 기법이 있습니다. 언더샘플링은 다수 클래스의 샘플을 제거하여 클래스 간 균형을 맞추는 방법입니다. 오버샘플링은 소수 클래스의 샘플을 복제하거나 재조합하여 데이터셋의 크기를 증가시키는 방법입니다.

LightGBM은 `is_unbalance` 파라미터를 통해 자동으로 언더샘플링을 수행할 수 있습니다. 이는 소수 클래스 샘플에 높은 가중치를 부여하여 클래스 간 균형을 조절하는 방법입니다.

```python
import lightgbm as lgb

# 언더샘플링 수행
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'is_unbalance': True  # 소수 클래스에 높은 가중치 부여
}
model = lgb.train(params, lgb_train)
```

오버샘플링을 수행하기 위해는 샘플을 복제하거나 합성하는 라이브러리를 사용해야 합니다. 예를 들어, `imbalanced-learn` 패키지의 `RandomOverSampler` 클래스를 사용할 수 있습니다. 이를 통해 소수 클래스의 샘플을 복제하여 클래스 간 균형을 맞출 수 있습니다.

```python
from imblearn.over_sampling import RandomOverSampler
import lightgbm as lgb

# 오버샘플링 수행
ros = RandomOverSampler()
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)
lgb_train_resampled = lgb.Dataset(X_train_resampled, y_train_resampled)

# 모델 학습
params = {
    'objective': 'binary',
    'metric': 'binary_logloss'
}
model = lgb.train(params, lgb_train_resampled)
```

## 3. 조정된 임계값

클래스 불균형이 존재할 때, 모델의 출력은 보통 다수 클래스로 편향됩니다. 이를 보정하기 위해 클래스 분류 임계값을 조정할 수 있습니다. 임계값을 조정하면 모델의 정밀도(Precision)와 재현율(Recall)을 조절할 수 있습니다.

```python
import lightgbm as lgb

# 모델 예측
y_pred = model.predict(X_test)

# 임계값 조정
threshold = 0.5
y_pred_adjusted = [1 if pred >= threshold else 0 for pred in y_pred]
```

불균형 데이터셋에서는 정밀도와 재현율을 모두 고려하는 F1-score나 AUC를 평가 지표로 사용하는 것이 좋습니다.

## 결론

불균형 데이터셋은 일반적인 머신러닝 모델에 도전적인 문제일 수 있습니다. 하지만 LightGBM 알고리즘은 불균형 데이터셋을 처리하기 위한 다양한 기능을 제공합니다. 클래스 가중치 설정, 언더샘플링 및 오버샘플링, 그리고 임계값 조정과 같은 기법을 활용하여 불균형 데이터셋에서의 성능을 향상시킬 수 있습니다.
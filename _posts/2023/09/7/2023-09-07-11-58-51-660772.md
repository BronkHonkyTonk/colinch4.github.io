---
layout: post
title: "[파이썬] lightgbm 다중 입력 데이터 학습"
description: " "
date: 2023-09-07
tags: [lightgbm]
comments: true
share: true
---

LightGBM is a powerful gradient boosting framework that is widely used in machine learning and data analysis tasks. It is known for its efficiency, speed, and accurate predictions. One of its key features is the ability to handle multiple-input data, allowing us to train models with various types of features.

In this blog post, we will explore how to use LightGBM to train models using multiple-input data in Python. 

## Installing LightGBM

Before we get started, let's make sure LightGBM is installed. You can install it using the following command:

```python
pip install lightgbm
```

## Loading Multiple-Input Data

To train models using multiple-input data, we first need to load the data into our Python environment. Multiple-input data refers to a dataset where each sample contains multiple features of different types.

Let's say we have a dataset with two types of features: numerical features and categorical features. We can load this dataset using the pandas library in Python:

```python
import pandas as pd

# Load numerical features
numerical_data = pd.read_csv('numerical_data.csv')

# Load categorical features
categorical_data = pd.read_csv('categorical_data.csv')
```

The `numerical_data` and `categorical_data` variables now contain the numerical and categorical features, respectively.

## Preprocessing Multiple-Input Data

Next, we need to preprocess the data before feeding it into the LightGBM model. This involves handling missing values, scaling numerical features, and encoding categorical features.

For handling missing values, we can use the `fillna()` method in pandas. For scaling numerical features, we can use the `StandardScaler` from the scikit-learn library. And for encoding categorical features, we can use one-hot encoding or label encoding.

```python
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

# Handle missing values
numerical_data = numerical_data.fillna(0)

# Scale numerical features
scaler = StandardScaler()
scaled_numerical_data = scaler.fit_transform(numerical_data)

# Label encode categorical features
encoder = LabelEncoder()
encoded_categorical_data = categorical_data.apply(encoder.fit_transform)
```

Now, we have the preprocessed numerical and categorical data ready for training our LightGBM model.

## Training LightGBM Model with Multiple-Input Data

To train a LightGBM model with multiple-input data, we need to combine the numerical and categorical features into a single dataset. We can do this using the `concat()` function in pandas.

```python
# Combine numerical and categorical features
combined_data = pd.concat([scaled_numerical_data, encoded_categorical_data], axis=1)
```

Now, we can split the combined dataset into training and testing sets and train our LightGBM model:

```python
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(combined_data, target_variable, test_size=0.2, random_state=42)

# Create the LightGBM dataset
train_data = lgb.Dataset(X_train, label=y_train)

# Set the hyperparameters
params = {'objective': 'binary',
          'metric': 'binary_logloss'}

# Train the model
model = lgb.train(params, train_data, num_boost_round=100)
```

Once the model is trained, we can make predictions on new data using the `predict()` method:

```python
# Make predictions on test data
y_pred = model.predict(X_test)
```

## Conclusion

In this blog post, we learned how to train LightGBM models using multiple-input data in Python. We covered loading multiple-input data, preprocessing the data, and training the model. LightGBM's ability to handle multiple-input data allows us to build more powerful models that can handle various types of features.
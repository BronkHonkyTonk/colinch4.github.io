---
layout: post
title: "[파이썬] requests 기반의 웹 스크래퍼 작성 팁"
description: " "
date: 2023-09-07
tags: [requests]
comments: true
share: true
---

웹 스크래핑은 웹페이지에서 데이터를 추출하는 프로세스로, 다양한 목적으로 사용될 수 있습니다. 파이썬에서는 `requests` 라이브러리를 활용하여 웹 스크래퍼를 작성할 수 있습니다. 이번 블로그 포스트에서는 `requests`를 기반으로 웹 스크래퍼를 작성하는 팁을 알아보겠습니다.

## 1. `requests` 라이브러리 설치

먼저, `requests` 라이브러리를 설치해야 합니다. 아래의 명령어를 터미널에서 실행하여 라이브러리를 설치할 수 있습니다.

```shell
pip install requests
```

## 2. 웹페이지 요청

`requests` 라이브러리를 사용하여 웹페이지에 GET 요청을 보낼 수 있습니다. 아래의 예제 코드는 `requests`를 사용하여 Google 홈페이지에 GET 요청을 보내는 방법을 보여줍니다.

```python
import requests

# Google 홈페이지에 GET 요청 보내기
response = requests.get("https://www.google.com")

# 응답 상태 코드 확인하기
if response.status_code == 200:
    print("요청이 성공적으로 처리되었습니다.")
else:
    print("요청에 실패하였습니다.")
```

## 3. HTML 파싱

웹페이지의 HTML을 파싱하여 필요한 데이터를 추출하는 것이 스크래퍼의 핵심입니다. `requests` 라이브러리는 웹페이지의 HTML을 `response.text` 속성을 통해 문자열 형태로 얻을 수 있습니다. 이후에는 다양한 HTML 파싱 라이브러리를 활용하여 데이터를 추출할 수 있습니다. 대표적인 라이브러리로는 `BeautifulSoup`이 있습니다.

```python
from bs4 import BeautifulSoup

# response.text로부터 BeautifulSoup 객체 생성
soup = BeautifulSoup(response.text, "html.parser")

# 필요한 데이터 추출
title = soup.title.text
print(f"페이지 제목: {title}")
```

## 4. CSS 선택자 사용

CSS 선택자를 활용하면 원하는 요소를 더 쉽게 추출할 수 있습니다. `BeautifulSoup` 라이브러리는 CSS 선택자를 사용할 수 있는 `select` 메서드를 제공합니다. 아래의 예제 코드는 CSS 선택자를 이용하여 Google 검색 결과 링크의 제목을 추출하는 방법을 보여줍니다.

```python
# CSS 선택자를 이용하여 링크 제목 추출
links = soup.select(".r > a")

for link in links:
    title = link.text
    print(title)
```

## 5. 스크래퍼의 에러 처리

웹 스크래퍼에서는 다양한 에러가 발생할 수 있으므로 적절한 에러 처리가 필요합니다. `requests` 라이브러리는 `try-except` 문을 통해 에러 처리를 할 수 있습니다. 아래의 예제 코드는 스크래퍼에서 발생할 수 있는 `requests.exceptions.RequestException` 에러를 처리하는 방법을 보여줍니다.

```python
try:
    response = requests.get(url)
    response.raise_for_status()
    
    # 스크래핑 로직 작성
except requests.exceptions.RequestException as e:
    print(f"에러 발생: {e}")
```

## 마무리

이번 블로그 포스트에서는 `requests`를 기반으로 웹 스크래퍼를 작성하는 팁을 알아보았습니다. `requests` 라이브러리를 활용하여 웹페이지에 GET 요청을 보내고, HTML을 파싱하여 데이터를 추출하는 방법을 위주로 다루었습니다. 추가적인 기능 및 고급 스크래핑 기법에 대해서는 관련 문서와 예제를 참고하시기 바랍니다.

Happy scraping!
---
layout: post
title: "[파이썬] xgboost GPU 활용 가속화"
description: " "
date: 2023-09-07
tags: [xgboost]
comments: true
share: true
---

![xgboost-logo](https://xgboost.readthedocs.io/en/latest/_static/logo.png)

## Introduction

xgboost (eXtreme Gradient Boosting) is a popular machine learning library known for its high performance and accuracy. It is widely used in various domains such as finance, healthcare, and internet advertising. However, as datasets grow larger and more complex, the training time of xgboost models can become a bottleneck.

To overcome this limitation, xgboost introduced GPU acceleration, which leverages the power of graphics processing units (GPUs) to significantly speed up the training process. In this blog post, we will explore how to utilize GPU acceleration in xgboost using Python.

## Prerequisites

To follow along with this tutorial, you will need the following:

- Python installed on your machine (preferably Python 3)
- xgboost library installed (`pip install xgboost`)
- CUDA-enabled GPU (Compute Capability 3.5 or higher)

## GPU Support in xgboost

Starting from version 0.90, xgboost introduced GPU support by utilizing the CUDA library. This allows xgboost to leverage the parallel processing power of GPUs, resulting in faster model training and inference.

To enable GPU support in xgboost, you need to install the CUDA toolkit and the corresponding GPU drivers on your machine.

## GPU Accelerated Training with xgboost

Let's dive into an example of how to use GPU acceleration in xgboost for training a model. We will use the popular Titanic dataset for binary classification.

```python
import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Load the dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert the dataset to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Define the xgboost parameters
params = {'objective': 'binary:logistic', 'gpu_id': 0, 'tree_method': 'gpu_hist'}

# Train the xgboost model
xgb_model = xgb.train(params, dtrain, num_boost_round=100)

# Evaluate the model
predictions = xgb_model.predict(dtest)
```

In the example above, we first load the breast cancer dataset using `sklearn.datasets.load_breast_cancer` and split it into training and test sets using `train_test_split`. Then, we convert the datasets to the `DMatrix` format, which is a xgboost-specific binary data format optimized for fast processing.

Next, we define the xgboost parameters. The `'gpu_id': 0` parameter specifies the GPU device to be used for training. The `'tree_method': 'gpu_hist'` parameter instructs xgboost to use the GPU histogram-based algorithm for training.

Finally, we train the xgboost model using the `xgb.train` function and evaluate the model by making predictions on the test set using `.predict`.

## Conclusion

In this blog post, we have explored how to leverage GPU acceleration in xgboost for faster model training. By utilizing the power of GPUs, we can significantly reduce the training time of xgboost models, allowing us to train larger and more complex models efficiently.

While GPU acceleration can greatly improve the performance of xgboost, it's worth noting that not all operations can be accelerated on GPUs. Therefore, it's important to carefully select the appropriate parameters and algorithms to maximize the benefits of GPU acceleration.

I hope this blog post has provided you with a good understanding of how to utilize GPU acceleration in xgboost. Happy modeling!
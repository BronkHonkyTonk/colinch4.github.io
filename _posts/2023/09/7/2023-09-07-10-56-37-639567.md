---
layout: post
title: "[파이썬] xgboost 벤치마크 및 경쟁 모델과의 비교"
description: " "
date: 2023-09-07
tags: [python,xgboost]
comments: true
share: true
---

xgboost는 최적화된 그래디언트 부스팅 알고리즘이며, 많은 데이터 과학자들과 머신러닝 엔지니어들에게 널리 사용되고 있는 머신러닝 프레임워크입니다. 이번 글에서는 xgboost의 성능을 다른 경쟁 모델과 비교해보고자 합니다.

## 벤치마크 데이터 소개

벤치마크를 위해 MNIST 이미지 데이터셋을 사용하겠습니다. MNIST는 손으로 쓴 숫자들로 구성된 대표적인 데이터셋으로, 이미지 분류 문제에 대한 벤치마크로 널리 사용됩니다.

## 데이터 전처리

데이터를 불러와서 전처리를 해야합니다. 이 단계에서는 이미지 데이터를 특징 벡터로 변환하고, 레이블을 원-핫 인코딩하여 분류 문제에 적합한 형태로 변환합니다.

```python
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# MNIST 데이터셋 불러오기
mnist = fetch_openml('mnist_784', version=1, cache=True)

# 특징 벡터와 레이블 분리
X = mnist.data
y = mnist.target

# 데이터 정규화
X = X / 255.0

# 레이블을 원-핫 인코딩
lb = LabelBinarizer()
y = lb.fit_transform(y)

# 데이터를 학습용과 테스트용으로 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## 모델 학습과 비교

xgboost 뿐만 아니라, 다른 인기 있는 머신러닝 모델들과 성능을 비교해보겠습니다. 여기서는 일반화 성능을 평가하기 위해 교차 검증을 사용합니다.

```python
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier

# 다른 머신러닝 모델들 생성
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Random Forest', RandomForestClassifier(n_estimators=100)),
    ('Support Vector Machine', SVC()),
    ('XGBoost', XGBClassifier())
]

# 각 모델의 성능 평가
for name, model in models:
    scores = cross_val_score(model, X_train, np.argmax(y_train, axis=1), cv=5)
    print(f"{name}의 정확도 평균: {scores.mean():.4f}")
```

## 결과 분석

다른 머신러닝 모델들과의 경쟁에서, xgboost(XGBClassifier)는 높은 정확도를 보여주고 있음을 알 수 있습니다. 따라서 MNIST 이미지 분류 문제에 대해서는 xgboost를 선택하는 것이 좋을 것입니다.

## 결론

이번 글에서는 xgboost를 포함한 다른 머신러닝 모델들과의 성능 비교를 통해 xgboost의 우수한 성능을 확인해보았습니다. xgboost는 다양한 문제에 활용될 수 있고, 경쟁 모델들과의 비교 분석을 통해 모델 선택에 도움을 줄 수 있습니다.
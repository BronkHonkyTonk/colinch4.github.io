---
layout: post
title: "[파이썬] requests 웹페이지 구조 변경에 대응하는 팁"
description: " "
date: 2023-09-07
tags: [python,requests]
comments: true
share: true
---

![Requests Logo](https://requests.readthedocs.io/en/master/_static/requests-sidebar.png)

여러분은 파이썬에서 웹 스크래핑이나 데이터 수집 작업을 수행할 때 requests 라이브러리를 사용하는 일이 있을 것입니다. requests는 파이썬 개발자들 사이에서 널리 사용되는 간편하면서도 강력한 HTTP 요청 라이브러리입니다. 하지만 웹페이지의 구조 변경으로 인해 기존에 동작하던 코드가 작동하지 않을 수도 있습니다.

이 글에서는 requests를 사용하여 웹페이지 구조 변경에 대응하는 팁을 소개하겠습니다. 이러한 팁을 활용하면 웹 스크래핑 작업을 유지하고 업데이트하는 데 도움이 될 것입니다.

## 1. 에러 처리하기

requests를 사용할 때 항상 예외 처리를 해야합니다. 웹페이지 구조가 변경되면 요청이 실패할 수 있습니다. 이러한 경우에 예외 처리를 통해 프로그램이 중단되지 않도록 할 수 있습니다. 아래는 requests를 사용할 때 발생할 수 있는 몇 가지 에러 처리 방법의 예입니다:

```python
import requests

try:
    response = requests.get(url)
    response.raise_for_status()
except requests.exceptions.HTTPError as errh:
    print("HTTP Error:", errh)
except requests.exceptions.ConnectionError as errc:
    print("Connection Error:", errc)
except requests.exceptions.Timeout as errt:
    print("Timeout Error:", errt)
except requests.exceptions.RequestException as err:
    print("Something went wrong:", err)
```

이렇게 예외 처리를 함으로써 요청이 실패했을 때 어떤 에러가 발생하는지 알 수 있습니다. 이를 통해 문제를 해결하기 위한 조치를 취할 수 있습니다.

## 2. HTML 파싱하기

requests를 사용하여 웹페이지의 HTML을 가져왔다면, 그 다음 단계는 해당 HTML에서 필요한 데이터를 추출하는 것입니다. 이를 위해 보통 BeautifulSoup과 같은 HTML 파싱 라이브러리를 사용합니다. 만약 웹페이지의 구조가 변경되었다면, HTML 파싱 작업도 변경되어야 할 수 있습니다.

이를 위해 다양한 HTML 파싱 기술과 선택자를 사용하여 원하는 데이터를 추출하세요. 아래는 BeautifulSoup과 CSS 선택자를 사용한 간단한 예입니다:

```python
from bs4 import BeautifulSoup

# requests로 가져온 HTML
html = requests.get(url).content

# BeautifulSoup을 사용하여 HTML 파싱
soup = BeautifulSoup(html, 'html.parser')

# CSS 선택자를 사용하여 원하는 데이터 추출
data = soup.select('div.container > h1')
print(data.text)
```

이 예제에서는 `div.container` 하위에 있는 모든 `h1` 요소를 추출하고, 그 중 첫 번째 요소의 텍스트를 출력합니다. 이런 식으로 HTML 파싱 로직을 변경하여 웹페이지 구조 변경에 대응할 수 있습니다.

## 3. User-Agent 헤더 설정하기

일부 웹사이트는 requests와 같은 스크래핑 도구를 차단하기 위해 User-Agent 헤더를 사용합니다. 이 헤더는 웹사이트에 서버에게 요청을 보낸 클라이언트의 정보를 알려주는 역할을 합니다. 

requests는 기본적으로 자체 User-Agent를 사용하지만, 사이트에서 이를 검사할 경우 스크랩을 차단할 수 있습니다. 이럴 때는 requests의 User-Agent 헤더를 변경하여 웹사이트에 있는 일반 사용자처럼 보이도록 할 수 있습니다. 

```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'
}

response = requests.get(url, headers=headers)
```

위의 예제에서는 User-Agent 헤더를 Mozilla Firefox의 User-Agent로 설정하였습니다. 이를 일반적인 웹 브라우저의 User-Agent로 변경해보세요.

## 마치며

requests는 뛰어난 기능과 간편한 API를 제공하여 웹 스크래핑 작업을 용이하게 만들어줍니다. 하지만 웹페이지의 구조 변경으로 인해 작동하지 않을 수도 있습니다. 이러한 상황에서는 본문에서 소개한 팁을 활용하여 문제를 해결하고 업데이트하는 것이 중요합니다.

요청을 처리하는 과정에서 발생하는 예외를 처리하고, HTML 파싱 방법을 조정하며, User-Agent 헤더를 설정하여 웹사이트에 차단되지 않도록 하세요. 이런 작업을 통해 안정적이고 효율적인 웹 스크래핑 작업을 수행할 수 있습니다.

Happy scraping with requests! 🚀
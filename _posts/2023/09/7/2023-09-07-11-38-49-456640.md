---
layout: post
title: "[파이썬] PyTorch 자동 기울기 계산(Autograd)"
description: " "
date: 2023-09-07
tags: [PyTorch]
comments: true
share: true
---

PyTorch는 딥러닝 모델을 학습시키기 위한 강력한 도구로, **자동 기울기 계산(Autograd)** 기능을 제공합니다. 이 기능을 사용하면 모델의 파라미터에 대한 기울기(gradient)를 자동으로 계산할 수 있습니다. 이를 통해 효율적이고 간단한 모델 학습이 가능해집니다.

## Autograd의 작동 원리
Autograd는 계산 그래프를 기반으로 동작합니다. 계산 그래프란 연산의 결과와 상호 의존성을 그래프 형태로 표현한 것입니다. PyTorch의 모든 연산은 계산 그래프의 노드로 표현되며, 이 노드들은 텐서와 관련된 함수와 연결되어 있습니다.

계산 그래프가 구성되어 있는 상태에서 모델의 출력을 얻으면, Autograd는 역전파(backpropagation) 알고리즘을 통해 각 노드의 기울기를 자동으로 계산합니다. 이 기울기는 모델의 파라미터를 업데이트하는 데 사용됩니다.

## Autograd를 사용한 예제

아래는 PyTorch의 Autograd를 사용하여 간단한 선형 회귀 모델을 학습하는 예제입니다.

```python
import torch

# 학습에 사용할 데이터
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]])

# 모델 파라미터 초기화
w = torch.tensor([[0.0]], requires_grad=True)
b = torch.tensor([[0.0]], requires_grad=True)

# 학습률과 반복 횟수 설정
learning_rate = 0.01
num_epochs = 1000

# 학습
for epoch in range(num_epochs):
    # Forward pass: 모델 예측
    y_pred = torch.matmul(x_train, w) + b
    
    # 손실 계산
    loss = torch.mean((y_pred - y_train) ** 2)
    
    # Autograd를 사용하여 기울기 계산
    loss.backward()
    
    # 파라미터 업데이트
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
        
        # 기울기 초기화
        w.grad.zero_()
        b.grad.zero_()
    
    # 로그 출력
    if (epoch+1) % 100 == 0:
        print(f'Epoch {epoch+1}, Loss: {loss.item()}')

# 학습된 파라미터 출력
print(f'학습된 파라미터 - w: {w.item()}, b: {b.item()}')
```

위 예제에서는 먼저 학습에 사용할 데이터를 생성합니다. 그 다음으로 모델의 파라미터를 초기화하고, 학습률과 반복 횟수를 설정합니다.

학습이 진행되는 동안 for 루프를 통해 모델의 Forward pass를 진행하고, 손실을 계산합니다. 그리고 Autograd의 `backward()` 메서드를 호출하여 기울기를 계산합니다. 이제 파라미터를 업데이트하기 위해 경사하강법을 사용합니다. 이때 `torch.no_grad()`를 사용하여 파라미터 업데이트에는 Autograd를 사용하지 않도록 설정한 후에, 기울기를 초기화합니다.

마지막으로 학습된 파라미터를 출력합니다.

Autograd의 뛰어난 기능은 복잡한 모델의 학습에도 적용될 수 있습니다. 자동 기울기 계산은 딥러닝 모델의 학습을 더욱 효율적으로 만들어주는 중요한 기능 중 하나입니다.

이러한 이유로 PyTorch는 많은 딥러닝 개발자들에게 선호되는 프레임워크가 되었습니다.
---
layout: post
title: "[파이썬] xgboost 트리 가중치와 분할 기준"
description: " "
date: 2023-09-07
tags: [python,xgboost]
comments: true
share: true
---

xgboost는 강력한 트리 기반 머신 러닝 알고리즘으로 알려져 있습니다. 이 알고리즘은 Gradient Boosting 프레임워크에서 많은 인기를 얻고 있으며, 대규모 데이터셋에서도 높은 예측 정확도를 제공합니다. xgboost는 다양한 하이퍼파라미터를 가지고 있지만, 이번에는 트리 가중치와 분할 기준에 대해 알아보겠습니다.

### 트리 가중치 (Tree Weight)

xgboost의 핵심 컨셉 중 하나는 각 트리에 대한 가중치를 설정하는 것입니다. 이 가중치는 각 트리의 중요도를 나타내며, 모델의 예측에 어떤 영향을 미치는지 결정합니다. 가중치는 트리가 반영하는 손실 값에 따라 할당됩니다. 손실 값이 높을수록 트리의 가중치가 높아지며, 모델에서 중요하게 취급됩니다.

가중치는 xgboost 모델 학습시 `weight` 매개변수를 통해 설정할 수 있습니다. 높은 가중치를 할당하면 해당 트리에 큰 영향을 줄 수 있습니다. 또한, `scale_pos_weight` 매개변수를 사용하여 양성 클래스에 대한 가중치를 조정할 수도 있습니다.

### 분할 기준 (Split Criteria)

트리 기반 알고리즘에서는 데이터를 분할하기 위한 기준이 필요합니다. xgboost는 분할 기준으로 다양한 메트릭을 제공합니다.

기본적으로 xgboost는 이진 분류를 위해 지니 계수(Gini coefficient)를 사용하고, 회귀를 위해 평균 제곱 오차(Mean Squared Error)를 사용합니다. 그러나 분류 문제에서는 메트릭을 변경할 수도 있습니다. 예를 들어, `logistic`으로 설정하면 로지스틱 손실을 사용할 수 있습니다.

메트릭은 xgboost 모델 학습시 `objective` 매개변수로 지정됩니다. 적절한 메트릭 선택은 모델의 성능에 큰 영향을 미칠 수 있습니다.

### 예제 코드

아래는 xgboost에서 트리 가중치와 분할 기준을 설정하는 예제 코드입니다.

```python
import xgboost as xgb

# 데이터 로드
X, y = load_data()

# DMatrix 객체 생성
dtrain = xgb.DMatrix(X, label=y)

# 모델 파라미터 설정
params = {
    'tree_weight': 0.5,  # 트리 가중치 설정
    'objective': 'binary:logistic',  # 분할 기준 설정
    'eval_metric': 'auc'  # 평가 메트릭 설정
}

# 모델 학습
model = xgb.train(params, dtrain)

# 예측
pred = model.predict(dtest)
```

위 예제에서는 `tree_weight` 매개변수를 0.5로 설정하여 트리 가중치를 조정하고, `objective` 매개변수를 `binary:logistic`으로 설정하여 로지스틱 손실을 사용합니다. 또한, `eval_metric` 매개변수를 `auc`로 설정하여 평가 메트릭으로 AUC를 사용하였습니다.

이제 xgboost에서 트리 가중치와 분할 기준을 사용하여 데이터를 학습하고 예측할 수 있습니다. 이러한 설정을 통해 모델의 예측 정확도를 높일 수 있습니다.

### 마무리

이번 글에서는 xgboost의 트리 가중치와 분할 기준에 대해 알아보았습니다. xgboost의 강력한 기능을 활용하여 데이터를 학습하고 예측할 때, 적절한 가중치와 분할 기준을 설정함으로써 모델의 성능을 향상시킬 수 있습니다. xgboost의 다른 하이퍼파라미터와 함께 트리 가중치와 분할 기준을 조정하여 최상의 결과를 얻을 수 있도록 노력해보세요.
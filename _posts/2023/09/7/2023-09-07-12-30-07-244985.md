---
layout: post
title: "[파이썬] lightgbm 성능 최적화 및 튜닝 전략"
description: " "
date: 2023-09-07
tags: [lightgbm]
comments: true
share: true
---

LightGBM은 경량화된 Gradient Boosting 프레임워크로, 대용량 데이터셋에서도 빠른 학습과 예측 속도를 제공합니다. 이 글에서는 LightGBM의 성능을 최적화하기 위한 몇 가지 전략과 튜닝 방법에 대해 알아보겠습니다.

## 1. 데이터 전처리 및 피처 엔지니어링

- **데이터 정제**: 결측치나 이상치를 처리하여 데이터의 품질을 향상시킵니다.
- **피처 선택**: 불필요한 피처를 제거하거나 유용한 피처를 추가하여 모델의 성능을 향상시킵니다.
- **피처 스케일링**: 피처 간의 스케일 차이가 큰 경우, 스케일링을 통해 모델의 성능을 개선합니다.

## 2. Hyperparameter 튜닝

LightGBM 모델의 성능을 최적화하기 위해 튜닝할 수 있는 주요 Hyperparameter들이 있습니다. 이들 중 일부는 다음과 같습니다.

- **num_leaves**: 트리가 가질 수 있는 최대 리프 수입니다. 이 값을 높일수록 모델의 복잡성이 증가하지만, 더 많은 학습이 가능합니다.
- **max_depth**: 트리의 최대 깊이를 설정합니다. 높은 값은 모델의 복잡성을 증가시킬 수 있으나, 과적합의 위험이 있습니다.
- **min_data_in_leaf**: 리프 노드에 필요한 최소 데이터 수입니다. 작은 값은 모델의 오버피팅 가능성을 높일 수 있습니다.
- **learning_rate**: 각 트리에서의 업데이트 가중치로, 너무 크면 수렴이 어렵고 너무 작으면 학습 시간이 늘어날 수 있습니다.

## 3. Early Stopping

Early Stopping은 지정한 metric이 개선되지 않을 때 학습을 조기 중단하는 기법입니다. LightGBM에서는 validation set을 사용하여 Early Stopping을 구현할 수 있습니다. 이를 통해 불필요한 학습을 줄이고, 과적합을 방지할 수 있습니다.

``` python
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# 데이터셋 분리
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

# LightGBM 데이터셋 생성
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid)

# 모델 파라미터 설정
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'early_stopping_rounds': 100,
    'num_boost_round': 1000
}

# 학습
model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=100)
```

## 4. Feature Importance 확인

피처의 상대적인 중요도를 확인하여 모델의 향상을 위한 인사이트를 얻을 수 있습니다. LightGBM은 `plot_importance` 함수를 통해 피처의 중요도를 시각화할 수 있습니다.

``` python
import lightgbm as lgb

# 학습 데이터셋으로 모델 훈련
model = lgb.train(params, train_data)

# 피처 중요도 시각화
lgb.plot_importance(model, figsize=(10, 6))
```

## 5. Cross Validation을 통한 평가

Cross Validation은 모델의 일반화 성능을 추정하기 위해 사용되는 방법 중 하나입니다. LightGBM에서는 `cv` 함수를 통해 Cross Validation을 수행할 수 있습니다.

``` python
import lightgbm as lgb
from sklearn.model_selection import cross_val_score

# LightGBM 데이터셋 생성
data = lgb.Dataset(X, label=y)

# 모델 파라미터 설정
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_boost_round': 1000
}

# Cross Validation 수행
cv_results = lgb.cv(params, data, num_boost_round=1000, early_stopping_rounds=100, nfold=5, shuffle=True, stratified=True, verbose_eval=100)

# CV 결과 출력
print('best n_estimators:', len(cv_results['binary_logloss-mean']))
print('best CV score:', cv_results['binary_logloss-mean'][-1])
```

위에서는 5-fold Cross Validation을 수행하고, 모델의 성능을 `binary_logloss` 메트릭을 기준으로 확인합니다.

LightGBM을 사용하여 모델을 구축하고 튜닝하는 과정에서는 데이터 전처리, Hyperparameter 튜닝, Early Stopping, Feature Importance 확인, Cross Validation 등의 기법을 사용할 수 있습니다. 이러한 전략들은 더 강력하고 정확한 모델을 만들기 위해 반복적으로 조정하고 업데이트할 수 있습니다.
---
layout: post
title: "[파이썬] Latent Dirichlet Allocation (LDA) 소개"
description: " "
date: 2023-09-04
tags: [python]
comments: true
share: true
---

![LDA](https://www.gstatic.com/edu/cs0065/lda.jpg)

## 소개

**Latent Dirichlet Allocation (LDA)**는 토픽 모델링(Topic Modeling) 기법 중 하나로, 문서 집합에서 숨겨진 토픽을 추출하는 확률적 그래픽 모델입니다. LDA는 주어진 문서 내에서 공통적인 주제를 찾아내어 토픽의 분포를 추론하는 작업을 수행합니다.

LDA의 핵심 아이디어는 각 문서가 토픽들의 혼합으로 구성되며, 각 토픽은 단어들의 분포를 가지고 있다는 가정입니다. 따라서 LDA는 문서를 작성하는 과정에 포함된 토픽들을 찾아내는 것을 목표로 합니다.

## LDA의 동작 원리

LDA의 동작 원리를 간단히 설명하겠습니다. LDA는 문서 집합에 대해 다음과 같은 가정을 합니다.

1. 각 문서는 여러 개의 토픽으로 구성되어 있습니다.
2. 각 토픽은 다양한 단어들의 분포를 가지고 있습니다.
3. 각 문서에서 사용되는 단어들은 해당 문서의 토픽들로부터 생성되었습니다.

LDA는 주어진 문서 집합을 입력으로 받아, 알고리즘이 자동으로 토픽의 수와 각 토픽의 단어 분포를 추정합니다. 이를 위해 LDA는 **Gibbs Sampling**이라는 알고리즘을 사용합니다.

## LDA를 사용한 토픽 모델링 예시

```python
import numpy as np
import lda

# 문서 집합 (문서 수 x 단어 수)의 형태로 데이터를 준비합니다.
doc_word_matrix = np.array([[1, 0, 2, 1, 0],
                            [0, 1, 1, 0, 1],
                            [2, 0, 3, 0, 1],
                            [0, 2, 0, 1, 0]])

# 토픽의 수를 지정합니다.
num_topics = 2

# LDA 모델을 생성하고 학습합니다.
model = lda.LDA(n_topics=num_topics, n_iter=1000, random_state=1)
model.fit(doc_word_matrix)

# 학습된 모델을 사용하여 문서 집합의 토픽들을 추론합니다.
topic_matrix = model.transform(doc_word_matrix)
```

위 예시 코드에서는 LDA 모델을 사용하여 4개의 문서로 구성된 문서 집합에서 2개의 토픽을 추론합니다. 추론 결과는 `topic_matrix`에 저장되어 반환됩니다.

## 요약

Latent Dirichlet Allocation (LDA)는 토픽 모델링을 수행하는 확률적 그래픽 모델로, 문서 집합 내에서 숨겨진 토픽들을 추출하는 기법입니다. LDA는 각 문서의 토픽 분포와 토픽의 단어 분포를 추정하여 토픽 모델링 작업을 수행합니다.
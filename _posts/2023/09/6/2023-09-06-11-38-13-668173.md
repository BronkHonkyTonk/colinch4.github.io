---
layout: post
title: "[파이썬] Beautiful Soup 4 다양한 유저 에이전트 사용하기"
description: " "
date: 2023-09-06
tags: [beautiful soup]
comments: true
share: true
---

Beautiful Soup 4은 Python의 웹 크롤링 및 스크레이핑 작업을 쉽게 수행할 수 있도록 도와주는 강력하고 유용한 도구입니다. 이번 포스트에서는 Beautiful Soup 4과 함께 다양한 유저 에이전트(User Agent)를 사용하는 방법에 대해 알아보겠습니다. 

## 유저 에이전트(User Agent)란?

유저 에이전트(User Agent)는 웹 브라우저가 웹 서버에게 요청을 보낼 때 식별하는 값으로, 웹 서버는 이 값을 통해 요청을 보낸 클라이언트를 구분합니다. 일반적으로 유저 에이전트는 웹 브라우저의 종류와 버전을 나타내며, 웹 서버는 다양한 유저 에이전트를 통해 웹 사이트에 접속하는 클라이언트를 파악할 수 있습니다.

## Beautiful Soup 4에서 유저 에이전트(User Agent) 사용하기

Beautiful Soup 4은 기본적으로 Python의 `requests` 라이브러리를 사용하여 웹 페이지를 가져옵니다. `requests` 라이브러리는 기본적으로 기본 유저 에이전트를 사용하며, 이는 크롤러라는 것을 웹 서버에 알리는 역할을 합니다. 

하지만 때로는 웹 사이트가 크롤러를 차단하거나 특정 유저 에이전트에 대해 서비스를 제한하는 경우가 있습니다. 이럴 때는 Beautiful Soup 4에서 다양한 유저 에이전트를 설정하여 서비스 제한을 우회할 수 있습니다.

다음은 Beautiful Soup 4에서 유저 에이전트를 설정하는 방법의 예시 코드입니다:

```python
import requests
from bs4 import BeautifulSoup

url = "http://www.example.com"

# 유저 에이전트 설정하기
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

# 웹 페이지 가져오기
response = requests.get(url, headers=headers)

# BeautifulSoup 객체 생성하기
soup = BeautifulSoup(response.text, 'html.parser')

# 원하는 작업 수행하기
# ...
```

위의 예시 코드에서는 `requests` 라이브러리를 사용하여 `http://www.example.com` 웹 페이지를 가져오고, `headers` 변수를 통해 유저 에이전트를 설정합니다. 이후 `BeautifulSoup` 객체를 생성하여 웹 페이지를 파싱하여 작업을 수행할 수 있습니다.

다양한 유저 에이전트를 사용하면 웹 사이트에 접속시 특정한 상황에서 유용하게 사용될 수 있습니다. 하지만 항상 유저 에이전트 사용에 대한 웹 사이트의 정책을 준수하는 것이 중요합니다.
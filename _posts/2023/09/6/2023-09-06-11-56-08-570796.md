---
layout: post
title: "[파이썬] Beautiful Soup 4 웹 스크레이핑 도구와의 통합: CrawlSpider"
description: " "
date: 2023-09-06
tags: [beautiful soup]
comments: true
share: true
---

웹 스크레이핑은 웹 페이지에서 정보를 추출하는 데 사용되는 강력한 도구입니다. 파이썬에서 웹 스크레이핑을 위해 사용되는 많은 도구 중에서 **Beautiful Soup 4**(이하 **BS4**)는 인기있는 선택입니다. BS4는 웹 페이지의 HTML 및 XML 구문을 파싱하고 데이터를 추출하는 간단하고 직관적인 API를 제공합니다.

하지만 크롤링에는 단순히 웹 페이지의 내용을 분석하는 것 이상의 요소들이 있습니다. 링크를 따라가며 여러 페이지를 탐색하거나 웹 사이트의 구조를 파악하는 등의 작업을 수행해야 할 때 **CrawlSpider**(이하 **CrawlSpider**)를 사용해 보는 것이 좋습니다. CrawlSpider는 **Scrapy**, 파이썬 기반 웹 크롤링 및 스크레이핑 프레임워크의 일부로 제공됩니다.

이 블로그 포스트에서는 BS4와 CrawlSpider를 통합하여 파이썬으로 웹 스크레이핑과 크롤링을 수행하는 방법을 알아보겠습니다.

## CrawlSpider란 무엇인가요?

**CrawlSpider**는 Scrapy 프레임워크에서 제공하는 기능 중 하나입니다. CrawlSpider는 웹 사이트를 크롤링하고 데이터를 추출하는 데 사용되는 강력한 도구입니다. CrawlSpider를 사용하면 다음과 같은 작업을 수행할 수 있습니다.

- 시작 URL을 지정하고 링크를 자동으로 따라가며 여러 페이지를 탐색합니다.
- 웹 사이트의 구조를 파악하여 적절한 URL 및 크롤링 규칙을 설정합니다.
- HTML 문서를 분석하여 원하는 데이터를 추출합니다.
- 추출한 데이터를 원하는 방식으로 가공, 저장 또는 출력합니다. 

CrawlSpider는 웹 사이트 크롤링을 간편하게 만들어주는 기능들을 제공합니다. 그렇다면 이제 BS4와 CrawlSpider를 함께 사용하여 웹 스크레이핑과 크롤링을 어떻게 수행하는지 살펴보겠습니다.

## BS4 및 CrawlSpider를 사용한 웹 스크레이핑과 크롤링 예시

다음 예시에서는 CrawlSpider와 BS4를 사용하여 인기 블로그 **Tech House**의 게시물 제목과 링크를 추출합니다. 먼저 필요한 패키지를 설치하고 임포트합니다.

```python
import scrapy
from scrapy import Request
from scrapy.linkextractors import LinkExtractor
from bs4 import BeautifulSoup
```

다음으로 CrawlSpider를 정의하고 시작 URL을 설정합니다.

```python
class TechHouseSpider(scrapy.spiders.CrawlSpider):
    name = "tech_house"
    allowed_domains = ["techhouse.com"]
    start_urls = ["https://www.techhouse.com"]
```

게시물의 링크를 추출하기 위해 Rule 및 callback 함수를 설정합니다.

```python
    rules = (
        Rule(LinkExtractor(allow=r"/post/"), callback="parse_post", follow=True),
    )
```

링크를 따라가며 웹 페이지를 탐색하고 게시물 제목과 링크를 추출하는 콜백 함수를 정의합니다.

```python
    def parse_post(self, response):
        soup = BeautifulSoup(response.body, "html.parser")
        title = soup.find("h1").text
        link = response.url

        yield {
            "title": title,
            "link": link,
        }
```

마지막으로 **scrapy** 명령을 사용하여 크롤러를 실행합니다.

```shell
scrapy crawl tech_house -o output.json
```

위의 예시 코드를 실행하면 해당 블로그에서 게시물의 제목과 링크가 추출되며, **output.json** 파일에 결과가 저장됩니다.

이 예시를 통해 BS4와 CrawlSpider를 함께 사용하여 강력한 웹 스크레이핑 및 크롤링 도구를 구축할 수 있다는 것을 확인하였습니다.

## 결론

BS4는 파이썬에서 웹 스크레이핑을 쉽게 수행할 수 있는 강력한 도구입니다. 그리고 CrawlSpider는 Scrapy 프레임워크에서 제공하는 웹 크롤링 기능 중 하나로, 다양한 작업을 자동화하고 웹 사이트의 구조를 파악하는 데 도움을 줍니다.

BS4와 CrawlSpider를 함께 사용하여 웹 스크레이핑과 크롤링 작업을 효율적으로 수행할 수 있습니다. 그러므로 웹 데이터를 탐색하고 필요한 정보를 추출하는 작업을 수행할 때 이러한 도구들을 활용해 보세요.

Happy scraping and crawling!
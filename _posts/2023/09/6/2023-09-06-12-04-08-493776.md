---
layout: post
title: "[파이썬] Scrapy와 AWS S3 통합"
description: " "
date: 2023-09-06
tags: [python,Scrapy]
comments: true
share: true
---

Scrapy는 웹 스크래핑 및 크롤링을 위한 파이썬 프레임워크입니다. AWS S3는 클라우드 스토리지 서비스로, 스크래퍼에서 수집한 데이터를 저장하고 관리하기에 이상적인 선택입니다.

이 블로그 포스트에서는 Scrapy와 AWS S3를 통합하여 수집한 데이터를 자동으로 S3 버킷에 업로드하는 방법을 알아보겠습니다.

## Step 1: 프로젝트 설정

먼저, Scrapy 프로젝트를 생성하고 `settings.py` 파일을 편집해야 합니다.

```python
# settings.py

ITEM_PIPELINES = {
   'myproject.pipelines.MyProjectPipeline': 300,
}

AWS_ACCESS_KEY_ID = 'your_aws_access_key'
AWS_SECRET_ACCESS_KEY = 'your_aws_secret_access_key'
AWS_REGION_NAME = 'your_aws_region_name'
AWS_S3_BUCKET_NAME = 'your_aws_s3_bucket_name'
```

여기서 `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION_NAME`, `AWS_S3_BUCKET_NAME`는 각각 AWS 계정의 액세스 키, 비밀 액세스 키, 지역 이름 및 S3 버킷 이름으로 대체되어야 합니다.

## Step 2: 데이터 업로드를 위한 Item Pipeline 작성

다음으로, 데이터를 S3에 업로드하기 위한 Scrapy Item Pipeline을 작성해야 합니다. `pipelines.py` 파일을 생성하고 다음 코드를 추가하세요.

```python
# pipelines.py

import boto3

class MyProjectPipeline(object):
    
    def open_spider(self, spider):
        self.s3 = boto3.client('s3',
                               aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
                               aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,
                               region_name=settings.AWS_REGION_NAME)

    def close_spider(self, spider):
        pass

    def process_item(self, item, spider):
        # 데이터를 S3에 업로드하기
        self.s3.put_object(Bucket=settings.AWS_S3_BUCKET_NAME,
                           Key=item['file_name'],
                           Body=item['file_content'])
        return item
```

위의 코드에서는 `boto3` 라이브러리를 사용하여 AWS S3에 데이터를 업로드하고 있습니다. `open_spider` 함수에서 AWS 인증 정보를 사용하여 S3 클라이언트를 초기화하고, `process_item` 함수에서 각 아이템을 S3에 업로드합니다.

## Step 3: 아이템 생성 및 업로드

`spiders` 폴더 아래에 원하는 크롤러를 작성한 후, 추출한 데이터를 Scrapy Item으로 변환하고 `file_name` 및 `file_content` 필드를 설정합니다.

```python
# spiders/myspider.py

import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['http://example.com']

    def parse(self, response):
        item = MyItem()
        item['file_name'] = 'example.txt'
        item['file_content'] = response.body
        yield item
```

위의 코드에서는 `start_urls`을 example.com으로 설정하고, 해당 웹 페이지의 HTML을 `file_content`로 설정하고 있습니다. `file_name`은 S3 버킷에 저장될 파일의 이름을 정의합니다.

## Step 4: 크롤링 및 데이터 업로드

마지막 단계로 크롤링 및 데이터 업로드를 실행해야 합니다. 터미널에서 다음 명령을 실행하세요.

```bash
$ scrapy crawl myspider
```

위 명령은 `myspider` 크롤러를 실행하고, 추출한 데이터를 S3에 업로드합니다.

이제 Scrapy와 AWS S3를 통합하여 데이터를 저장하고 관리하는 방법을 알게 되었습니다. 이 방법을 사용하면 스크래퍼에서 수집한 데이터를 안전하게 보관할 수 있으며, 필요할 때마다 쉽게 접근할 수 있습니다.
---
layout: post
title: "[파이썬] Scrapy WebSocket을 통한 데이터 스크레이핑"
description: " "
date: 2023-09-06
tags: [python,Scrapy]
comments: true
share: true
---

[Scrapy](https://scrapy.org/)는 Python으로 작성된 오픈소스 웹 스크레이핑 프레임워크로, 웹 페이지에서 데이터를 추출하는 데 매우 강력하고 유연한 도구입니다. Scrapy는 기본적으로 HTTP를 통해 웹 페이지에 액세스하여 데이터를 수집합니다. 그러나 때로는 HTTP 요청과 응답을 넘어서 실시간으로 업데이트되는 데이터를 스크레이핑해야 할 때가 있습니다. 이러한 경우에 Scrapy의 WebSocket을 활용할 수 있습니다.

WebSocket은 서버 및 클라이언트 간의 양방향 통신 채널을 제공하는 프로토콜입니다. 이를 사용하면 실시간으로 업데이트되는 데이터를 효과적으로 스크레이핑할 수 있습니다. 이제 Scrapy의 WebSocket을 사용하여 데이터 스크레이핑하는 방법을 알아보겠습니다.

## 1. 필수 패키지 설치

Scrapy WebSocket을 사용하기 위해 몇 가지 패키지를 설치해야 합니다. 터미널에서 다음 명령을 실행하여 패키지를 설치합니다.

```shell
pip install scrapy scrapy-websocket
```

## 2. 프로젝트 생성 및 설정

다음으로 Scrapy 프로젝트를 생성해야 합니다. 터미널에서 다음 명령을 실행하여 Scrapy 프로젝트를 생성합니다.

```shell
scrapy startproject myproject
```

생성된 프로젝트 폴더로 이동한 다음, `settings.py` 파일을 엽니다. 다음과 같이 설정을 추가하여 WebSocket을 사용할 수 있도록 활성화합니다.

```python
# settings.py

DOWNLOADER_MIDDLEWARES = {
    'scrapy_websocket.middleware.WebsocketMiddleware': 610,
}
```

## 3. WebSocket 스파이더 작성

이제 데이터 스크레이핑을 위해 WebSocket을 사용하는 스파이더를 작성해보겠습니다. 프로젝트 폴더에서 `spiders` 폴더를 생성한 다음, `webscraper.py`라는 파일을 생성합니다. 다음은 간단한 예시입니다.

```python
# webscraper.py

import scrapy
from scrapy_websocket.websocket_spiders import WebsocketSpider


class MySpider(WebsocketSpider):
    name = 'myspider'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.data = []

    def start_requests(self):
        url = 'wss://example.com/ws'  # WebSocket URL
        yield scrapy.Request(url, self.parse_websocket)

    def parse_websocket(self, response):
        message = 'Hello, WebSocket!'
        self.send_message(message)  # Send a message to the WebSocket server

    def on_message(self, message):
        self.data.append(message)  # Process the received message
        self.log(f'Received message: {message}')

    def closed(self, reason):
        self.log('WebSocket connection closed')

        # Process collected data
        self.log(f'Data collected: {self.data}')
```

위의 코드에서 `MySpider` 클래스는 `WebsocketSpider`를 상속받아 구현하고 있습니다. `start_requests` 메서드에서 웹소켓을 연결한 다음, `parse_websocket` 메서드가 웹소켓에 연결되면 데이터를 처리하는 로직을 작성할 수 있습니다. 받은 메세지는 `on_message` 메서드를 통해 처리하며, 연결이 종료되면 `closed` 메서드가 호출됩니다.

## 4. 스파이더 실행

이제 작성한 스파이더를 실행하여 실제로 데이터를 스크레이핑해봅시다. 프로젝트 폴더에서 다음 명령을 실행합니다.

```shell
scrapy crawl myspider
```

WebSocket 서버와의 연결이 성공하면 `on_message` 메서드가 호출되고, 받은 데이터를 `data` 리스트에 저장합니다. 연결이 종료되면 `closed` 메서드가 호출되고, 최종적으로 `data` 리스트에 저장된 데이터가 출력됩니다.

Scrapy WebSocket을 통해 데이터 스크레이핑을 하는 방법을 알아보았습니다. 이를 통해 웹소켓을 사용하는 실시간 데이터를 효과적으로 스크레이핑하는 데 도움이 될 것입니다.
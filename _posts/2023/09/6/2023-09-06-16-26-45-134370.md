---
layout: post
title: "[파이썬] Gensim에서의 Sequence-to-Sequence 모델"
description: " "
date: 2023-09-06
tags: [Gensim]
comments: true
share: true
---

![sequence-to-sequence](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/seq2seq_model-1536x864.png)

[Gensim](https://radimrehurek.com/gensim/), a popular Python library for natural language processing (NLP), provides a powerful toolkit for implementing various machine learning techniques. In this blog post, we will explore how to use Gensim to build a Sequence-to-Sequence (Seq2Seq) model.

The Sequence-to-Sequence model is widely used in various NLP tasks such as machine translation, text summarization, and conversational AI. It consists of two main components: an encoder and a decoder. The encoder processes the input sequence and generates a fixed-length representation called the context vector. The decoder then takes the context vector as input and predicts the output sequence.

Let's dive into implementation details using Gensim.

## Installing Gensim

Before we begin, make sure Gensim is installed on your machine. You can install it using pip:

```
pip install gensim
```

## Preparing the Data

For this example, let's assume we are building a machine translation model. We will need a parallel corpus containing pairs of sentences in the source language and their translations in the target language. Gensim's Seq2Seq module expects the data to be in a specific format: each sentence should be a list of words.

```python
import gensim
from gensim.models import Word2Vec

# Define the source and target sentences
source_sentences = [['I', 'like', 'cats'], ['He', 'hates', 'dogs']]
target_sentences = [['Je', 'aime', 'les', 'chats'], ['Il', 'déteste', 'les', 'chiens']]

# Train word embeddings
source_model = Word2Vec(sentences=source_sentences, size=100, min_count=1)
target_model = Word2Vec(sentences=target_sentences, size=100, min_count=1)

# Pad sentences to a fixed length
source_sentences = gensim.models.word2vec.EpochSaver(source_model, save_vocab=None, save_hyperparams=None)pad_sequences(source_sentences)
target_sentences = gensim.models.word2vec.EpochSaver(target_model, save_vocab=None, save_hyperparams=None)pad_sequences(target_sentences)
```

## Building the Seq2Seq Model

Once we have the preprocessed data, we can start building the Seq2Seq model using Gensim.

```python
from gensim.models import Seq2Seq

# Define the Seq2Seq model
seq2seq_model = Seq2Seq(
    source_model, target_model,
    vector_size=100, cell_type='gru',
    optimizer='adam'
)

# Train the model
seq2seq_model.train(source_sentences, target_sentences, epochs=10)
```

## Generating Predictions

Finally, we can use the trained Seq2Seq model to generate predictions.

```python
# Generate predictions for new source sentences
new_source_sentences = [['We', 'love', 'Gensim'], ['How', 'are', 'you']]
predicted_target_sentences = seq2seq_model.predict(new_source_sentences)

# Print the predicted target sentences
for sentence in predicted_target_sentences:
    print(' '.join(sentence))
```

Congratulations! You have successfully built a Seq2Seq model using Gensim. This powerful tool can be extended to various NLP tasks beyond machine translation. Feel free to experiment with different hyperparameters, data preprocessing techniques, and other Gensim functionalities to further improve the model's performance.

This is just an introduction to Seq2Seq modeling with Gensim. Keep exploring and experimenting with different use cases to enhance your NLP skills. Happy coding!
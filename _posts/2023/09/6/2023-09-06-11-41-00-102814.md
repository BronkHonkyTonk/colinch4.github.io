---
layout: post
title: "[파이썬] Scrapy 다양한 웹 페이지 탐색"
description: " "
date: 2023-09-06
tags: [python,Scrapy]
comments: true
share: true
---

Scrapy는 Python 기반의 웹 크롤링 및 스크래핑 프레임워크입니다. 이 강력한 도구는 웹 페이지의 데이터를 수집하고 처리하는 데 사용됩니다. 이 글에서는 Scrapy를 사용하여 다양한 웹 페이지를 탐색하는 방법을 알아보겠습니다.

## 설치와 설정

Scrapy를 사용하기 위해서는 먼저 설치해야 합니다. 아래의 명령어로 Scrapy를 설치할 수 있습니다.

```bash
pip install scrapy
```

설치가 완료되면 scrapy 명령어를 사용할 수 있습니다. 하지만 먼저 프로젝트를 생성해야합니다. 프로젝트를 생성하려면 다음 명령어를 입력하세요.

```bash
scrapy startproject myproject
```

프로젝트 디렉토리가 생성되면 다음으로 크롤러를 생성해야합니다. 크롤러는 웹 페이지를 탐색하고 데이터를 수집하는 데 사용됩니다. 다음의 명령어를 사용하여 크롤러를 생성하세요.

```bash
cd myproject
scrapy genspider myspider example.com
```

위의 명령어를 사용하면 `example.com`을 크롤링하는 데 사용되는 기본 크롤러가 생성됩니다.

## 웹 페이지 탐색

이제 크롤러를 생성했으니 웹 페이지를 탐색해보겠습니다. Scrapy는 시작 URL을 설정하고 해당 URL에서부터 탐색을 시작합니다. 기본적으로 스파이더의 `start_requests` 메서드가 시작 URL을 지정하는 데 사용됩니다.

```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['http://www.example.com']

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(url, self.parse)

    def parse(self, response):
        # 웹 페이지의 내용을 처리하는 코드 작성
        pass
```

위의 예제에서 `start_urls`에 원하는 웹 사이트의 URL을 설정합니다. 스파이더의 `start_requests` 메서드에서는 해당 URL을 요청하여 `parse` 메서드로 전달됩니다. `parse` 메서드에서는 웹 페이지의 내용을 처리하는 로직을 작성할 수 있습니다.

## 데이터 추출

Scrapy를 사용하여 웹 페이지를 탐색한 후에는 원하는 데이터를 추출해야 합니다. 이를 위해 Scrapy는 CSS 선택자 또는 XPath를 사용하여 원하는 요소를 선택할 수 있습니다. 아래는 Scrapy에서 데이터를 추출하는 간단한 예입니다.

```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['http://www.example.com']

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(url, self.parse)

    def parse(self, response):
        title = response.css('h1::text').get()
        yield {
            'title': title
        }
```

위의 예제에서 `response.css('h1::text').get()`는 웹 페이지에서 `<h1>` 태그 내의 텍스트를 추출하는 코드입니다. 추출한 데이터는 Python 사전 형태로 반환됩니다.

## 결과 저장

마지막으로 추출한 데이터를 저장하는 방법에 대해 알아보겠습니다. Scrapy는 다양한 저장 방식을 지원합니다. 가장 일반적인 방법은 CSV, JSON 또는 데이터베이스에 저장하는 것입니다.

아래는 데이터를 CSV 파일에 저장하는 예제입니다.

```python
import scrapy
from scrapy.exporters import CsvItemExporter

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['http://www.example.com']

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(url, self.parse)

    def parse(self, response):
        title = response.css('h1::text').get()
        yield {
            'title': title
        }

    def closed(self, reason):
        items = list(self.parse(response))  # 추출한 데이터 리스트
        with open('data.csv', 'wb') as f:  # 파일명 설정
            exporter = CsvItemExporter(f)
            exporter.start_exporting()
            for item in items:
                exporter.export_item(item)
            exporter.finish_exporting()
```

위의 예제에서는 `pars
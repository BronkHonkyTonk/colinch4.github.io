---
layout: post
title: "[파이썬] Scrapy Cookies와 세션 관리"
description: " "
date: 2023-09-06
tags: [Scrapy]
comments: true
share: true
---

Scrapy는 파이썬 기반의 웹 스크래핑 프레임워크로, 웹페이지에서 데이터를 추출하는 데 사용됩니다. 그러나 일부 웹 사이트는 사용자 인증을 위해 쿠키와 세션을 사용하므로, 스크래핑 작업에서 쿠키와 세션을 효과적으로 관리해야 합니다. 이번 포스트에서는 Scrapy에서 쿠키와 세션을 관리하는 방법에 대해 알아보겠습니다.

## 1. 쿠키와 세션의 개념

우선, 쿠키와 세션의 개념에 대해 간략히 설명하겠습니다. 

- **쿠키(Cookie)**: 클라이언트 측에 저장되는 작은 텍스트 파일로, 웹사이트의 상태 정보를 유지하기 위해 사용됩니다. 보안적으로는 취약할 수 있으므로 주의가 필요합니다.

- **세션(Session)**: 서버 측에 정보를 저장하는 방법으로, 쿠키를 기반으로 동작합니다. 사용자의 상태를 유지하고 인증 등의 기능을 처리하기 위해 사용됩니다.

## 2. Scrapy에서 쿠키 사용하기

Scrapy에서 쿠키를 사용하기 위해서는 `CookiesMiddleware`를 활성화해야 합니다. `settings.py` 파일에서 다음과 같이 설정할 수 있습니다:

```python
COOKIES_ENABLED = True
```

이제 스크래피에서 HTTP 요청을 보낼 때, 응답 헤더에 포함된 쿠키를 자동으로 추출하여 자동으로 저장됩니다. 저장된 쿠키는 후속 요청에 자동으로 포함되어 서버에 보내집니다. 

쿠키를 수동으로 설정하고 싶은 경우, 아래와 같이 Request 객체의 `cookies` 속성을 설정할 수 있습니다:

```python
yield Request(url, cookies={"cookie_name": "cookie_value"})
```

## 3. Scrapy에서 세션 사용하기

Scrapy에서 세션을 사용하기 위해서는 추가적인 라이브러리인 `requests`와 `Session` 클래스를 활용할 수 있습니다. 먼저, `requests` 라이브러리를 설치해주세요:

```
pip install requests
```

그리고 아래와 같이 세션을 사용하는 예제 코드를 살펴보겠습니다:

```python
import requests
from scrapy import Spider

class MySpider(Spider):
    name = 'myspider'
    
    def start_requests(self):
        session = requests.Session()
        
        # 세션 설정
        session.headers.update({'User-Agent': 'Mozilla/5.0'})
        session.auth = ('username', 'password')
        
        # 요청 보내기 (세션 사용)
        yield session.get('https://example.com')
```

위의 예제에서는 `requests.Session()`을 사용하여 세션을 생성하고, `session.headers`와 `session.auth`를 사용하여 세션을 설정한 후, `session.get()`을 사용하여 요청을 보냅니다.

세션을 사용하면 쿠키를 직접 설정할 필요없이, 세션 객체를 통해 자동으로 관리됩니다.

## 4. 로그인 및 세션 유지

많은 웹사이트는 로그인과 세션 유지를 통해 사용자를 인증합니다. Scrapy에서 사용자 인증을 처리하기 위해서는 `FormRequest`를 사용하면 됩니다. 로그인 페이지에 POST 요청을 보내고, 로그인에 필요한 정보를 `FormRequest` 객체에 전달하여 인증을 처리할 수 있습니다.

```python
from scrapy import Spider, FormRequest

class LoginSpider(Spider):
    name = 'loginspider'
    login_url = 'https://example.com/login'
    
    def start_requests(self):
        yield FormRequest(
            self.login_url,
            formdata={'username': 'your_username', 'password': 'your_password'},
            callback=self.after_login
        )
    
    def after_login(self, response):
        # 로그인 이후에 실행할 작업
        if response.xpath('//div[@class="login-error"]'):
            self.logger.error('Login failed')
            return
        else:
            # 세션 유지를 위해 추가 작업 수행
            yield Request(url='https://example.com/my_account', callback=self.parse_account)
    
    def parse_account(self, response):
        # 세션 유지 작업 수행
        # ...
        pass
```

위의 예제에서는 먼저 `start_requests` 메소드를 사용하여 로그인 페이지에 POST 요청을 보냅니다. 로그인 폼의 필드명을 적절히 지정하여 `formdata`에 사용자명과 비밀번호를 전달해야 합니다. 

로그인에 성공한 후에는 `after_login` 메소드가 호출되며, 이 메소드에서는 로그인에 성공했는지 확인하고, 세션을 유지하는 추가 작업을 수행합니다. 이후에는 상황에 맞게 `parse_account`나 다른 콜백 메소드를 사용하여 원하는 작업을 처리할 수 있습니다.

## 마무리

이번 포스트에서는 Scrapy에서 쿠키와 세션을 관리하는 방법에 대해 알아보았습니다. 쿠키와 세션을 효과적으로 사용하여 웹 스크래핑 작업을 수행하면, 웹 사이트의 인증 및 세션 유지 등의 작업을 쉽게 처리할 수 있습니다. 
Happy scraping!
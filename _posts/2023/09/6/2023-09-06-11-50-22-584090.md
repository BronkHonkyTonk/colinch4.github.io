---
layout: post
title: "[파이썬] Beautiful Soup 4 웹 스크레이핑 결과의 데이터베이스 연동"
description: " "
date: 2023-09-06
tags: [python,Beautiful Soup]
comments: true
share: true
---

웹 스크레이핑은 웹페이지에서 데이터를 추출하는 과정을 말합니다. Beautiful Soup 4는 파이썬에서 많이 사용되는 웹 스크레이핑 라이브러리 중 하나입니다. 이번 글에서는 Beautiful Soup 4를 사용하여 스크레이핑한 결과를 데이터베이스에 연동하는 방법을 알아보겠습니다.

## 데이터베이스 설정

먼저, Beautiful Soup 4로 스크레이핑한 데이터를 저장할 데이터베이스를 설정해야 합니다. Python에서 데이터베이스에 접속하고 데이터를 조작하기 위해 `sqlite3` 모듈을 사용할 수 있습니다. 예를 들어, SQLite 데이터베이스를 만들어보겠습니다.

```python
import sqlite3

# 데이터베이스와 연결
conn = sqlite3.connect('scraped_data.db')

# 커서 생성
c = conn.cursor()

# 테이블 생성
c.execute('''
    CREATE TABLE IF NOT EXISTS scraped_data (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT,
        content TEXT
    )
''')

# 데이터베이스에 변경사항 반영
conn.commit()

# 연결 종료
conn.close()
```

위 코드는 `scraped_data.db`라는 SQLite 데이터베이스를 생성하고, `scraped_data`라는 테이블을 생성합니다. 테이블에는 `id`, `title`, `content`라는 칼럼이 있습니다. 데이터베이스에 대한 설정은 실제 환경에 따라 다를 수 있으니 참고 부탁드립니다.

## 웹 스크레이핑 및 데이터베이스 연동

이제 Beautiful Soup 4를 사용해 웹 스크레이핑을 하고, 스크레이핑한 결과를 데이터베이스에 연동하는 과정을 알아보겠습니다.

```python
import requests
from bs4 import BeautifulSoup
import sqlite3

# 웹 페이지에 접속하여 HTML 가져오기
url = "https://example.com"
response = requests.get(url)
html_content = response.text

# Beautiful Soup 객체 생성
soup = BeautifulSoup(html_content, 'html.parser')

# 데이터 추출
title = soup.find('h1').text
content = soup.find('div', class_='content').text

# 데이터베이스 연동
conn = sqlite3.connect('scraped_data.db')
c = conn.cursor()

# 데이터베이스에 데이터 삽입
c.execute('''
    INSERT INTO scraped_data (title, content)
    VALUES (?, ?)
''', (title, content))

# 데이터베이스에 변경사항 반영
conn.commit()

# 연결 종료
conn.close()
```

위 코드에서 `requests` 모듈을 사용하여 웹 페이지에 접속하고, `BeautifulSoup` 객체를 생성하여 HTML을 파싱합니다. 그리고 `find` 메서드를 사용하여 원하는 데이터를 추출합니다. 스크레이핑한 결과는 `title`과 `content` 변수에 저장됩니다.

그 다음, `sqlite3` 모듈을 사용하여 데이터베이스에 연결하고, `execute` 메서드를 사용하여 데이터를 삽입합니다. 마지막으로, 데이터베이스에 변경사항을 반영하고 연결을 종료합니다.

## 마무리

이제 Beautiful Soup 4를 사용하여 웹 스크레이핑한 결과를 데이터베이스에 연동하는 방법을 알아보았습니다. 웹 스크레이핑과 데이터베이스 연동을 함께 사용하면, 웹에서 필요한 데이터를 추출하여 보다 편리하게 활용할 수 있습니다.
---
layout: post
title: "[파이썬] Scrapy 데이터 정제 및 전처리"
description: " "
date: 2023-09-06
tags: [Scrapy]
comments: true
share: true
---

Scrapy는 파이썬 기반의 웹 크롤링 프레임워크로, 웹페이지의 데이터를 수집하는 데 사용됩니다. 하지만 수집한 데이터는 종종 정제 및 전처리가 필요한 경우가 많습니다. 이번 포스트에서는 Scrapy를 사용하여 수집한 데이터를 정제하고 전처리하는 방법에 대해 알아보겠습니다.

## 1. 데이터 수집

Scrapy를 사용하여 웹페이지의 데이터를 수집하는 방법에 대해서는 다른 포스트에서 다루었습니다. 여기서는 이미 데이터를 수집했다고 가정하고, 해당 데이터를 어떻게 정제하고 전처리할 수 있는지 알아보겠습니다.

## 2. 데이터 정제

수집한 데이터를 분석하기 전에 정제해야 합니다. 이 단계에서는 데이터에서 불필요한 정보를 제거하고, 포맷을 통일하고, 중복되거나 불완전한 데이터를 처리하는 작업이 이루어집니다.

### 2.1 HTML 태그 제거

Scrapy로 데이터를 수집할 때, 주로 웹페이지의 HTML 태그를 포함한 형태로 저장됩니다. 이 HTML 태그를 제거하기 위해 BeautifulSoup과 같은 라이브러리를 사용할 수 있습니다.

```python
from bs4 import BeautifulSoup

def remove_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()
```

### 2.2 특수 문자 제거

데이터에는 종종 특수 문자나 기호가 포함되어 있을 수 있습니다. 이러한 특수 문자를 제거하기 위해 정규 표현식을 사용할 수 있습니다.

```python
import re

def remove_special_characters(text):
    pattern = r'[^a-zA-Z0-9\s]'
    return re.sub(pattern, '', text)
```

### 2.3 공백 제거

문자열에서 공백을 제거하는 것은 데이터를 전처리할 때 자주 사용되는 작업입니다. 파이썬의 문자열 메서드인 `strip()`을 사용하여 양쪽 공백을 제거할 수 있습니다.

```python
def remove_whitespace(text):
    return text.strip()
```

## 3. 데이터 전처리

데이터 정제 후에는 실제 분석에 사용할 수 있는 형태로 데이터를 전처리해야 합니다. 전처리 작업은 데이터를 토큰화하거나 어근 추출, 불용어 제거 등으로 이루어질 수 있습니다. 아래는 몇 가지 예시입니다.

### 3.1 토큰화

토큰화는 텍스트를 작은 단위로 나누는 작업입니다. 예를 들어, 문장을 단어로 나누거나 문단을 문장으로 나눌 수 있습니다. 파이썬의 `nltk` 라이브러리를 사용하여 토큰화를 수행할 수 있습니다.

```python
from nltk.tokenize import word_tokenize

def tokenize(text):
    return word_tokenize(text)
```

### 3.2 어근 추출

어근 추출은 단어의 어근을 찾는 작업입니다. 예를 들어, "running", "ran", "runs" 등은 모두 "run"의 어근입니다. `nltk` 라이브러리에서 제공하는 `PorterStemmer`나 `WordNetLemmatizer`를 사용하여 어근 추출을 수행할 수 있습니다.

```python
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

def stem(text):
    stemmer = PorterStemmer()
    lemmatizer = WordNetLemmatizer()

    # stemming
    stemmed_text = [stemmer.stem(word) for word in text]
    
    # lemmatizing
    lemmatized_text = [lemmatizer.lemmatize(word) for word in text]
    
    return stemmed_text, lemmatized_text
```

### 3.3 불용어 제거

불용어(Stopwords)는 분석에서 무시할 수 있는 단어를 말합니다. 예를 들어, "a", "an", "the"와 같은 단어는 대부분의 경우 의미가 없는 단어입니다. `nltk` 라이브러리에서는 기본적인 불용어 리스트를 제공하고 있으며, 이를 사용하여 처리할 수 있습니다.

```python
from nltk.corpus import stopwords

def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    filtered_text = [word for word in text if not word in stop_words]
    return filtered_text
```

위에서 소개한 예시는 데이터를 정제하고 전처리하는 몇 가지 가장 기본적인 작업을 다루었습니다. 실제로는 데이터의 특성과 목적에 따라 더 다양한 전처리 작업이 수행될 수 있습니다. Scrapy와 함께 이러한 전처리 작업을 통해 수집한 데이터를 효율적으로 활용할 수 있습니다.
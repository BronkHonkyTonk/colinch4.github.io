---
layout: post
title: "[파이썬] TensorFlow에서 활성화 함수"
description: " "
date: 2023-09-06
tags: [statsmodels]
comments: true
share: true
---

활성화 함수(Activation Function)는 인공 신경망(ANN)에서 입력 신호의 총합을 출력 신호로 변환하는 함수입니다. TensorFlow는 다양한 종류의 활성화 함수를 제공하여 신경망 모델에서 비선형성을 추가하고 원하는 출력을 얻을 수 있게 도와줍니다.

## 시그모이드 함수 (Sigmoid Function)

시그모이드 함수는 입력 범위를 0과 1 사이로 제한하여 출력 값을 확률로 해석할 수 있도록 합니다. TensorFlow에서는 `tf.nn.sigmoid()` 함수를 사용하여 시그모이드 함수를 적용할 수 있습니다.

```python
import tensorflow as tf

x = tf.constant([-5.0, 0.0, 5.0]) 
y = tf.nn.sigmoid(x)

print(y.numpy()) # [0.0066929  0.5       0.9933071]
```

## 렐루 함수 (ReLU Function)

렐루 함수는 입력이 음수인 경우 0으로, 양수인 경우 입력 값을 그대로 출력하는 함수입니다. TensorFlow에서는 `tf.nn.relu()` 함수를 사용하여 렐루 함수를 적용할 수 있습니다.

```python
import tensorflow as tf

x = tf.constant([-5.0, 0.0, 5.0]) 
y = tf.nn.relu(x)

print(y.numpy()) # [0. 0. 5.]
```

## 소프트맥스 함수 (Softmax Function)

소프트맥스 함수는 다중 클래스 분류 문제에서 확률 값을 생성하기 위해 사용됩니다. 각 클래스에 대한 출력 값을 0과 1 사이의 값으로 변환하여 전체 값의 합이 1이 되도록 조정합니다. TensorFlow에서는 `tf.nn.softmax()` 함수를 사용하여 소프트맥스 함수를 적용할 수 있습니다.

```python
import tensorflow as tf

x = tf.constant([1.0, 2.0, 3.0]) 
y = tf.nn.softmax(x)

print(y.numpy()) # [0.09003057 0.24472848 0.66524094]
```

## 하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent Function)

하이퍼볼릭 탄젠트 함수는 시그모이드 함수와 유사하지만, 출력 범위가 -1과 1 사이로 제한됩니다. TensorFlow에서는 `tf.nn.tanh()` 함수를 사용하여 하이퍼볼릭 탄젠트 함수를 적용할 수 있습니다.

```python
import tensorflow as tf

x = tf.constant([-5.0, 0.0, 5.0]) 
y = tf.nn.tanh(x)

print(y.numpy()) # [-0.9999092  0.         0.9999092]
```

활성화 함수는 인공 신경망의 핵심 요소 중 하나입니다. TensorFlow의 다양한 활성화 함수를 적절하게 적용하여 모델의 성능을 향상시킬 수 있습니다.
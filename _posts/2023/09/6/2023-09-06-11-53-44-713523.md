---
layout: post
title: "[파이썬] Scrapy 데이터베이스 연동"
description: " "
date: 2023-09-06
tags: [Scrapy]
comments: true
share: true
---

Scrapy는 파이썬 기반의 웹 크롤링 프레임워크입니다. 이 블로그 포스트에서는 Scrapy를 사용하여 데이터를 웹에서 수집한 후, 데이터베이스에 연동하는 방법을 알아보겠습니다.

## 데이터베이스 연동을 위한 설정

Scrapy에서 데이터베이스에 연결하기 위해서는 `settings.py` 파일에서 몇 가지 설정을 해주어야 합니다. 

```python
# settings.py

# 데이터베이스 연결 설정
DATABASE = {
    'drivername': 'postgres',
    'host': 'localhost',
    'port': '5432',
    'username': 'your-username',
    'password': 'your-password',
    'database': 'your-database'
}

# Scrapy pipeline 활성화
ITEM_PIPELINES = {
    'yourproject.pipelines.DatabasePipeline': 300,
}
```

위 코드에서 `DATABASE` 변수에 데이터베이스 연결 정보를 입력합니다. `drivername`은 사용하는 데이터베이스의 드라이버 이름을, `host`는 데이터베이스 서버 주소를, `port`는 포트 번호를, `username`과 `password`는 데이터베이스 로그인에 필요한 정보를, 그리고 `database`는 연결할 데이터베이스의 이름을 각각 입력합니다.

또한 `ITEM_PIPELINES` 변수를 설정하여 데이터베이스에 저장하고자 하는 아이템을 처리하기 위한 파이프라인을 활성화합니다.

## 데이터베이스에 저장하는 파이프라인 구현

데이터를 수집하고 이를 데이터베이스에 저장하기 위해 Scrapy 파이프라인을 구현해야 합니다.

```python
# pipelines.py

from sqlalchemy.orm import sessionmaker
from yourproject.models import db_connect, create_table, YourItem

class DatabasePipeline(object):
    def __init__(self):
        engine = db_connect()
        create_table(engine)
        self.Session = sessionmaker(bind=engine)
    
    def process_item(self, item, spider):
        session = self.Session()
        your_item = YourItem(**item)
        
        try:
            session.add(your_item)
            session.commit()
        except:
            session.rollback()
            raise
        finally:
            session.close()

        return item
```

위 코드는 `yourproject.pipelines` 모듈에서 데이터베이스에 저장하는 파이프라인을 구현한 예시입니다. 

첫번째로, `db_connect()` 함수를 사용하여 `settings.py`에서 설정한 데이터베이스 연결 정보로 데이터베이스에 연결합니다. 그리고 `create_table()` 함수로 데이터베이스에 테이블을 생성합니다.

`DatabasePipeline` 클래스는 Scrapy의 `process_item()` 메서드를 오버라이드하여 커스텀 로직을 구현합니다. 데이터베이스 세션을 열고, 수집한 아이템을 데이터베이스에 저장하는 작업을 수행합니다.

마지막으로, `YourItem` 클래스는 모델에서 정의한 데이터베이스 테이블의 구조에 맞게 아이템을 생성합니다.

## 데이터 수집 및 데이터베이스 연동 실행

마지막으로, Scrapy 스파이더를 실행하여 데이터를 수집하고 데이터베이스에 연동하는 작업을 실행합니다.

```bash
$ scrapy crawl yourspidername
```

위 명령어를 실행하면, `yourspidername`에 해당하는 스파이더가 작업을 실행하고 데이터를 수집합니다. 그리고 파이프라인에서 구현한 로직에 따라 데이터베이스에 저장됩니다.

이렇게 데이터베이스와 Scrapy를 연동하여 데이터의 수집과 저장을 한 번에 처리할 수 있습니다.

Scrapy의 강력한 크롤링 기능과 데이터베이스의 편리한 데이터 관리 기능을 함께 사용하여 웹 데이터를 효과적으로 수집하는 작업을 수행할 수 있습니다.
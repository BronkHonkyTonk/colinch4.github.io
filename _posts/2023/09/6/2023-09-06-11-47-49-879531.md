---
layout: post
title: "[파이썬] Scrapy 스케줄링과 중단점"
description: " "
date: 2023-09-06
tags: [python,Scrapy]
comments: true
share: true
---

Scrapy는 파이썬으로 작성된 웹 스크래핑 프레임워크로, 웹 페이지에서 구조화된 데이터를 추출하는 데 사용됩니다. Scrapy는 스크래핑 프로세스를 관리하기 위해 스케줄링 기능을 제공합니다. 또한, 중단된 스크래핑 작업을 재개하기 위한 중단점 기능도 제공됩니다.

## 스크래핑 스케줄링

스크래핑 작업을 계획하고 관리하기 위해 Scrapy는 스케줄링 기능을 제공합니다. 스케줄링은 각각의 요청(request)을 언제 보낼지, 각 요청에 대한 응답(response)이 언제 도착할지를 결정합니다. 스크래핑 작업을 효율적으로 실행하기 위해 다양한 스케줄링 알고리즘을 사용할 수 있습니다.

Scrapy에서는 기본적으로 우선순위 큐(Priority Queue) 기반의 스케줄러를 사용합니다. 요청은 우선순위에 따라 큐에 저장되고, 스케줄러는 우선순위가 가장 높은 요청을 선택하여 처리합니다. 이를 통해 중요한 페이지를 우선순위 있게 다운로드할 수 있고, 웹 서버에 과도한 부하를 주지 않고 스크래핑 작업을 분산시킬 수 있습니다.

스크래핑 스케줄링을 구현하려면 Scrapy에서 제공하는 `scrapy.Request` 객체를 사용하여 요청(request)을 생성하고, 해당 요청에 대한 응답(response)이 도착했을 때 처리하는 콜백(callback) 함수를 작성해야 합니다. 이러한 콜백 함수는 `scrapy.Spider` 클래스의 메소드로 작성됩니다. 콜백 함수에서는 응답을 처리하거나 결괏값을 추출하여 저장하는 작업을 수행할 수 있습니다.

```python
import scrapy

class MySpider(scrapy.Spider):
    name = "my_spider"
    start_urls = [
        "http://www.example.com/page1",
        "http://www.example.com/page2",
        "http://www.example.com/page3",
    ]

    def parse(self, response):
        # 응답 처리 작업 수행
        pass
```

위의 예제 코드에서는 `start_urls`에 지정된 페이지들을 순차적으로 방문하여 응답을 처리하기 위해 `parse` 메소드를 사용합니다. `parse` 메소드는 모든 응답에 대해 자동으로 호출되며, 각 응답을 처리하는 로직을 작성하면 됩니다.

## 중단점

스크래핑 작업을 수행하는 동안에는 다양한 이유로 중단되거나 종료될 수 있습니다. 이러한 경우에 Scrapy는 중단점 기능을 제공하여 중단된 스크래핑 작업을 재개할 수 있도록 합니다. 중단점은 현재까지 수행된 작업 상태를 저장하고, 재개할 때 저장된 상태부터 작업을 다시 시작할 수 있게 해줍니다.

Scrapy에서는 중단점을 저장하고 관리하는 기능을 제공하는 스토리지(storage) 클래스를 사용합니다. 스토리지는 작업 상태를 디스크에 저장하고, 중단된 작업을 재개하는 데 사용됩니다. Scrapy는 기본적으로 `scrapy.extensions.closespider.CloseSpider` 클래스를 사용하여 중단점을 관리합니다.

중단점을 사용하려면 `CLOSESPIDER_*` 설정 값을 수정해야 합니다. 예를 들어, `CLOSESPIDER_ITEMCOUNT` 설정 값을 변경하면 추출한 데이터 개수가 지정된 개수에 도달하면 스크래핑 작업이 중단됩니다. 중단된 작업을 재개하려면 `scrapy crawl` 명령어에 `-s JOBDIR=<job_directory>` 옵션을 추가하여 중단점이 저장된 디렉터리를 지정해야 합니다.

```bash
$ scrapy crawl my_spider -s JOBDIR=job_directory
```

위의 예제에서 `my_spider`는 스파이더 이름이고, `job_directory`는 중단점이 저장될 디렉터리입니다.

## 결론

Scrapy는 스크래핑 작업을 계획하고 관리하기 위한 스케줄링 기능과 중단된 작업을 재개하기 위한 중단점 기능을 제공합니다. 이러한 기능을 통해 효율적인 웹 스크래핑 작업을 수행할 수 있습니다.
---
layout: post
title: "[파이썬] TensorFlow에서 VAE (Variational Autoencoders)"
description: " "
date: 2023-09-06
tags: [statsmodels]
comments: true
share: true
---
========================================

Variational Autoencoders (VAEs) are powerful generative models that learn to reconstruct the input data and generate new samples from the learned latent space. In this blog post, we will explore how to implement VAEs using TensorFlow in Python.

What are Variational Autoencoders?
-----------------------------------

Autoencoders are neural networks that aim to learn a compressed representation of the input data. They consist of two main components - an **encoder** that learns to encode the input data into a lower-dimensional latent space, and a **decoder** that reconstructs the original data from the latent representation.

Variational Autoencoders (VAEs) enhance the basic autoencoder by introducing a probabilistic model in the latent space. Instead of mapping the input directly to a fixed point in the latent space, VAEs learn a **probability distribution** over the latent variables.

The encoder network parameterizes a **multivariate Gaussian distribution** representing the latent space, with mean and variance computed from the input data. The decoder network then generates samples from this distribution to reconstruct the input.

Implementation of VAE in TensorFlow
-----------------------------------

Let's now dive into the implementation of VAE using TensorFlow. Here's a step-by-step guide on how to set up the VAE model:

1. Import the required libraries:

```python
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.losses import mse
```

2. Define the input shape and latent dimension:

```python
input_shape = (784,)  # MNIST images are of shape (28, 28), flattened to (784,)
latent_dim = 2  # Number of dimensions in the latent space
```

3. Define the encoder network:

```python
def build_encoder(input_shape, latent_dim):
    inputs = Input(shape=input_shape)
    x = Dense(256, activation='relu')(inputs)
    x = Dense(128, activation='relu')(x)
    z_mean = Dense(latent_dim)(x)
    z_log_var = Dense(latent_dim)(x)
    return Model(inputs, [z_mean, z_log_var], name='encoder')
```

4. Define the sampling function:

```python
def sampling(args):
    z_mean, z_log_var = args
    batch_size = K.shape(z_mean)[0]
    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)
    return z_mean + K.exp(0.5 * z_log_var) * epsilon
```

5. Define the decoder network:

```python
def build_decoder(latent_dim):
    inputs = Input(shape=(latent_dim,))
    x = Dense(128, activation='relu')(inputs)
    x = Dense(256, activation='relu')(x)
    outputs = Dense(784, activation='sigmoid')(x)
    return Model(inputs, outputs, name='decoder')
```

6. Build the VAE model:

```python
def build_vae(encoder, decoder):
    inputs = Input(shape=input_shape)
    z_mean, z_log_var = encoder(inputs)
    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])
    reconstructed = decoder(z)
    return Model(inputs, reconstructed, name='vae')
```

7. Define the VAE loss function:

```python
def vae_loss(inputs, reconstructed):
    reconstruction_loss = mse(inputs, reconstructed)
    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    return K.mean(reconstruction_loss + kl_loss)
```

8. Compile and train the VAE:

```python
vae = build_vae(build_encoder(input_shape, latent_dim), build_decoder(latent_dim))
vae.compile(optimizer='adam', loss=vae_loss)
vae.fit(X_train, X_train, epochs=10, batch_size=128)
```

Conclusion
----------

In this blog post, we have learnt how to implement Variational Autoencoders (VAEs) in TensorFlow using Python. VAEs are powerful generative models that can learn to reconstruct the input data and generate new samples from the learned latent space.
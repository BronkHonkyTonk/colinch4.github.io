---
layout: post
title: "[파이썬] nltk 정보 검색 (Information Retrieval) 기초"
description: " "
date: 2023-09-06
tags: [python,nltk]
comments: true
share: true
---

## 소개

정보 검색은 컴퓨터 시스템에서 사용자가 원하는 정보를 효율적으로 찾아주는 과정입니다. 이를 위해 많은 기술과 알고리즘이 사용되는데, 자연어 처리 라이브러리인 NLTK(natural language toolkit)를 사용하여 정보 검색을 수행할 수 있습니다. 이 글에서는 NLTK를 사용한 정보 검색의 기초를 소개하고, 예제 코드를 제공하겠습니다.

## NLTK 설치

NLTK를 설치하기 위해서는 Python 패키지 관리자 `pip`를 사용합니다. 다음 명령을 터미널에서 실행하여 NLTK를 설치하세요:

```bash
pip install nltk
```

## 문서 전처리

정보 검색에서는 효율적인 검색을 위해 문서를 전처리하는 과정이 필요합니다. NLTK를 사용하여 문서를 토큰화(tokenization), 정제(cleaning), 정규화(normalization)하는 등의 과정을 수행할 수 있습니다.

```python
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def preprocess_document(document):
    # 문서를 소문자로 변환
    document = document.lower()
    
    # 토큰화
    tokens = word_tokenize(document)
    
    # 불용어 제거
    stopwords_list = stopwords.words("english")
    tokens = [token for token in tokens if token not in stopwords_list]
    
    # 표제어 추출
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    return tokens

# 예제 문서
document = "NLTK is a powerful Python library for natural language processing. It provides easy-to-use interfaces for tasks such as tokenization, lemmatization, and text classification."

# 문서 전처리 수행
preprocessed_doc = preprocess_document(document)
print(preprocessed_doc)
```

위 예제에서는 `preprocess_document()` 함수를 사용하여 문서를 전처리합니다. 이 함수는 문서를 소문자로 변환한 후, 토큰화, 불용어 제거, 표제어 추출 과정을 거칩니다. 결과로 토큰화된 단어들이 출력됩니다.

## 검색 쿼리 처리

사용자가 검색할 때 입력한 쿼리를 처리하는 것도 중요한 단계입니다. 검색 쿼리도 문서와 마찬가지로 전처리하여 효과적인 검색을 수행할 수 있습니다. 다음은 검색 쿼리를 전처리하는 예제 코드입니다.

```python
def preprocess_query(query):
    # 문서 전처리와 동일한 과정 수행
    query = query.lower()
    query_tokens = word_tokenize(query)
    query_tokens = [token for token in query_tokens if token not in stopwords_list]
    query_tokens = [lemmatizer.lemmatize(token) for token in query_tokens]
    
    return query_tokens

# 예제 검색 쿼리
query = "Python library for natural language processing"

# 검색 쿼리 전처리 수행
preprocessed_query = preprocess_query(query)
print(preprocessed_query)
```

위 예제에서는 `preprocess_query()` 함수를 사용하여 검색 쿼리를 전처리합니다. 검색 쿼리는 문서와 동일한 과정을 거쳐 소문자 변환, 토큰화, 불용어 제거, 표제어 추출을 수행합니다.

## 단어 빈도 계산

검색 쿼리와 전처리된 문서를 비교하여 단어 빈도를 계산하는 것도 정보 검색의 중요한 단계입니다. NLTK는 `FreqDist` 클래스를 사용하여 단어 빈도를 계산할 수 있습니다. 다음은 단어 빈도를 계산하는 예제 코드입니다.

```python
from nltk import FreqDist

def calculate_word_frequencies(document):
    # 단어 빈도 계산
    word_freq = FreqDist(document)
    
    return word_freq

# 전처리된 문서와 검색 쿼리를 함께 사용하여 단어 빈도 계산
word_freq = calculate_word_frequencies(preprocessed_doc + preprocessed_query)
print(word_freq.most_common(5))
```

위 예제에서는 `calculate_word_frequencies()` 함수를 사용하여 단어 빈도를 계산합니다. 전처리된 문서와 검색 쿼리를 함께 사용하여 단어 빈도 계산을 수행하고, 가장 빈도가 높은 상위 5개의 단어를 출력합니다.

## 결론

NLTK를 사용하여 정보 검색의 기초를 소개하고, 예제 코드를 제공했습니다. 이를 통해 문서 전처리, 검색 쿼리 처리, 단어 빈도 계산 등의 과정을 수행하는 방법을 배웠습니다. 이러한 기초를 바탕으로 더 복잡하고 정교한 정보 검색 시스템을 구축할 수 있습니다. NLTK를 적극적으로 활용하여 다양한 정보 검색 프로젝트를 시작해보세요!
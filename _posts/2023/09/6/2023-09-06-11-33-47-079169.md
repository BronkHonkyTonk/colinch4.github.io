---
layout: post
title: "[파이썬] Beautiful Soup 4 스크레이핑한 데이터의 데이터베이스 저장"
description: " "
date: 2023-09-06
tags: [Beautiful Soup]
comments: true
share: true
---

이제는 웹 스크레이핑을 통해 얻은 데이터를 어떻게 저장할지 알아보겠습니다. 스크레이핑한 데이터를 데이터베이스에 저장하는 것은 매우 일반적인 작업이며, 파이썬에서는 Beautiful Soup 4를 사용하여 쉽게 구현할 수 있습니다.

## 데이터베이스 선택

데이터를 저장하기 전에 어떤 종류의 데이터베이스를 사용할지 결정해야 합니다. 파이썬에서 가장 일반적으로 사용되는 데이터베이스 중 하나는 SQLite입니다. SQLite는 경량 데이터베이스로, 파일 시스템에 저장되며 다른 데이터베이스 서버나 구성 요소를 필요로하지 않습니다. SQLite를 사용하면 데이터를 간단하게 저장하고 검색할 수 있습니다. 

만약 SQLite 외에 다른 데이터베이스를 사용하고 싶다면, 앞으로 설명하는 코드에서 SQLAlchemy를 사용해서 다른 데이터베이스에 연결할 수 있습니다. SQLAlchemy는 패턴 및 ORM(Object-Relational Mapping)을 제공하여 다양한 데이터베이스 백엔드와 상호 작용할 수 있도록 도와줍니다.

## SQLite로 데이터 저장

우리가 스크레이핑한 데이터를 저장하기 위해 SQLite를 사용할 때, 먼저 `sqlite3` 모듈을 import 해야 합니다. 그러나 이 모듈은 기본적으로 파이썬에 내장되어 있어 추가 설치가 필요하지 않습니다.

```python
import sqlite3
```

SQLite 데이터베이스에 연결하려면, `sqlite3.connect()` 함수를 사용합니다. 이 함수는 데이터베이스 파일이 존재하지 않는 경우 파일을 만들어 연결을 돕습니다. 이 예제에서는 `test.db`라는 파일을 생성하고 데이터베이스에 연결합니다.

```python
conn = sqlite3.connect("test.db")
```

연결에 성공하면 `conn` 변수에 데이터베이스 연결 객체가 할당됩니다. 이제 데이터베이스와 상호 작용을 할 수 있습니다.

먼저, 데이터베이스에 테이블을 생성해야 합니다. 테이블은 데이터를 저장하는 데 사용되는 구조입니다. 테이블을 생성하기 위해 SQL `CREATE TABLE` 문을 사용합니다. 이 예제에서는 "books"라는 테이블을 생성합니다.

```python
c = conn.cursor()
c.execute('''CREATE TABLE books
             (title TEXT, author TEXT, year INTEGER)''')
```

이제 데이터를 스크레이핑하고, 데이터베이스로 저장하는 예제를 살펴보겠습니다. 예를 들어서 책의 제목, 작가 및 출판 연도를 스크레이핑했다고 가정해 봅시다. 이 데이터를 데이터베이스에 저장하기 위해 다음 코드를 사용할 수 있습니다.

```python
# 스크레이핑한 데이터
book_title = "Python Web Scraping"
book_author = "John Smith"
book_year = 2022

# 데이터베이스에 데이터 추가
c.execute("INSERT INTO books VALUES (?, ?, ?)", (book_title, book_author, book_year))
```

데이터를 추가한 후에는 `commit()`을 호출하여 변경 사항을 확정합니다.

```python
conn.commit()
```

마지막으로, 데이터베이스 연결을 닫기 위해 `close()` 메서드를 호출합니다.

```python
conn.close()
```

이제 스크레이핑한 데이터가 SQLite 데이터베이스에 저장되었습니다.

## SQLAlchemy로 데이터 저장

SQLite 외의 다른 데이터베이스에 데이터를 저장하려면, SQLAlchemy를 사용할 수 있습니다. SQLAlchemy는 다른 데이터베이스 백엔드와 작업하기 위한 통합 도구이며, 간단한 API를 제공합니다.

먼저 SQLAlchemy를 설치해야 합니다.

```shell
$ pip install sqlalchemy
```

SQLite 데이터베이스에 데이터 저장을 위해서는 다음 코드를 사용할 수 있습니다.

```python
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# SQLAlchemy 엔진 생성
engine = create_engine("sqlite:///test.db", echo=True)

# BaseModel 생성
Base = declarative_base()

# 데이터베이스 테이블 모델 선언
class Book(Base):
    __tablename__ = "books"

    id = Column(Integer, primary_key=True)
    title = Column(String(50))
    author = Column(String(50))
    year = Column(Integer)

# 데이터베이스 테이블 생성
Base.metadata.create_all(engine)

# 데이터 추가하기
Session = sessionmaker(bind=engine)
session = Session()

book = Book(title="Python Web Scraping", author="John Smith", year=2022)
session.add(book)
session.commit()

# 세션 닫기
session.close()
```

위의 코드는 SQLAlchemy를 사용하여 `test.db`에 `books` 테이블을 생성하고, 데이터를 추가하는 예제입니다. SQLAlchemy는 ORM을 사용하여 객체를 데이터베이스 테이블에 매핑하므로, 데이터를 쉽게 추가할 수 있습니다.

## 마치며

이 글에서는 Beautiful Soup 4를 사용하여 스크레이핑한 데이터를 데이터베이스에 저장하는 방법을 알아보았습니다. 기본적으로 SQLite를 사용하여 데이터를 저장할 수 있으며, SQLAlchemy를 사용하여 다른 데이터베이스 백엔드와 상호 작용할 수도 있습니다. 데이터베이스를 활용하면 스크레이핑한 데이터를 효율적으로 관리하고 다양한 분석 및 처리 작업을 수행할 수 있습니다.
---
layout: post
title: "[파이썬] Scrapy CrawlSpider 사용하기"
description: " "
date: 2023-09-06
tags: [Scrapy]
comments: true
share: true
---

Scrapy는 웹 데이터를 수집하고 스크래핑하는 데 사용되는 파이썬 기반의 라이브러리입니다. CrawlSpider는 Scrapy의 하위 클래스로, 다양한 웹 페이지를 자동으로 크롤링하고 파싱하는 기능을 제공합니다. 이번 포스트에서는 Scrapy CrawlSpider를 사용하는 방법을 살펴보겠습니다.

## CrawlSpider란?

CrawlSpider는 Scrapy에서 제공하는 스파이더(Spider) 중 하나입니다. 기본적으로 여러 웹 페이지를 크롤링 함에 있어 자동으로 다음 페이지로 이동하고 링크를 따라가는 기능을 지원합니다. 이는 웹 사이트의 여러 페이지를 자동으로 탐색하고 데이터를 수집할 때 큰 편의성을 제공합니다.

## 세팅하기

먼저, 프로젝트 디렉토리를 만들고 가상 환경을 설정합니다:

```bash
$ mkdir myproject
$ cd myproject
$ python -m venv venv
$ source venv/bin/activate
```

다음으로 Scrapy를 설치합니다:

```bash
$ pip install scrapy
```

## 스파이더 생성하기

새로운 스파이더를 생성하기 위해 다음 명령어를 실행합니다:

```bash
$ scrapy genspider myspider example.com
```

위 명령어는 `myspider`라는 이름의 새로운 스파이더를 생성하며, `example.com`을 크롤링 대상으로 설정합니다. 이렇게 생성된 스파이더는 `myproject/spiders` 디렉토리에 저장됩니다.

## CrawlSpider 상속받기

생성된 스파이더의 코드를 열고, `CrawlSpider` 클래스를 상속받도록 수정합니다. 다음과 같이 코드가 변경됩니다:

```python
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'myspider'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = [
        Rule(LinkExtractor(allow=()), callback='parse_item', follow=True),
    ]

    def parse_item(self, response):
        # 데이터 파싱 로직 작성
        pass
```

`allowed_domains`는 스파이더가 크롤링할 도메인을 지정하고, `start_urls`는 시작 URL을 설정합니다. `rules`는 크롤링 규칙을 정의하는데, `LinkExtractor`를 사용하여 다음 페이지로 이동할 링크를 찾고, `parse_item` 메서드를 호출하여 데이터를 파싱합니다.

## 데이터 파싱

`parse_item` 메서드를 수정하여 원하는 데이터를 파싱합니다. 데이터를 추출하기 위해 `XPath`나 `CSS Selector` 등을 사용할 수 있습니다. 다음은 간단한 예시입니다:

```python
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'myspider'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = [
        Rule(LinkExtractor(allow=()), callback='parse_item', follow=True),
    ]

    def parse_item(self, response):
        item = {}
        item['title'] = response.css('h1::text').get()
        item['content'] = response.css('p::text').getall()
        yield item
```

위 예시에서는 `h1` 태그와 `p` 태그에 포함된 텍스트를 추출하여 `item` 객체에 저장하고 반환합니다. 추출한 데이터는 스크래피 프레임워크에 의해 자동으로 처리됩니다.

## 크롤링 시작하기

마지막으로, 다음 명령어를 실행하여 스파이더를 실행합니다:

```bash
$ scrapy crawl myspider
```

이 명령어는 `myspider`라는 이름의 스파이더를 실행합니다. 해당 스파이더가 `start_urls`에서 시작하여 규칙에 따라 다음 페이지로 이동하면서 데이터를 수집합니다.

Scrapy CrawlSpider를 사용하면 웹 크롤링과 스크래핑을 더욱 효율적으로 수행할 수 있습니다. 다양한 웹 페이지를 자동으로 탐색하고 데이터를 추출하는 기능을 통해 시간과 노력을 절약할 수 있습니다.
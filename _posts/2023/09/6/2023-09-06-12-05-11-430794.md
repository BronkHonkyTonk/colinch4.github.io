---
layout: post
title: "[파이썬] Scrapy와 AWS EC2 통합"
description: " "
date: 2023-09-06
tags: [python,Scrapy]
comments: true
share: true
---

![Scrapy Logo](https://scrapy.org/img/scrapylogo.png)

**Scrapy**는 파이썬으로 작성된 오픈소스 웹 크롤링 프레임워크입니다. 이는 웹사이트에서 데이터를 추출하기 위한 강력한 도구로서, 크롤링 프로세스를 자동화하고 정교한 스크래핑을 수행하는데 사용됩니다.

**AWS EC2**(Amazon Elastic Compute Cloud)는 클라우드에서 가상 서버를 제공하는 웹 서비스입니다. EC2를 사용하면 서버를 프로비저닝하고, 관리하고, 확장하는데 소요되는 시간과 비용을 절감할 수 있습니다.

이 블로그 포스트에서는 Scrapy와 AWS EC2를 통합하는 방법을 알아보겠습니다. 이로써 당신은 크롤러를 AWS 클라우드 내에서 실행하여 크롤링 프로세스를 효율적으로 관리할 수 있습니다.

## 1. EC2 인스턴스 설정

첫째로, AWS Management Console에 로그인하여 EC2 인스턴스를 생성해야 합니다.

1. EC2 대시보드로 이동합니다.
2. "인스턴스 시작" 버튼을 클릭합니다.
3. 원하는 AMI(Amazon Machine Image)를 선택합니다. 예를 들어, "Amazon Linux 2"를 선택할 수 있습니다.
4. 인스턴스 유형을 선택합니다. 예를 들어, "t2.micro"를 선택할 수 있습니다.
5. 구성 세부 정보를 설정합니다.
6. 스토리지를 구성합니다.
7. 보안 그룹을 구성합니다.
8. 키페어를 선택하고, 인스턴스를 시작합니다.

## 2. SSH를 통한 EC2 인스턴스에 연결

인스턴스를 실행한 후, 이제 SSH를 통해 인스턴스에 연결해야 합니다.

1. AWS Management Console에서 인스턴스에 연결하려는 키 페어를 선택하고, 창에 나타나는 연결 명령을 복사합니다.

예시 명령:
```
ssh -i "your-key-pair.pem" ec2-user@ec2-xxx-xxx-xxx-xxx.compute-1.amazonaws.com
```

2. 명령 프롬프트(CMD) 또는 터미널을 열고, 위의 명령을 붙여 넣고 Enter 키를 누릅니다.

## 3. 필수 소프트웨어 설치

EC2 인스턴스에는 기본적으로 필요한 소프트웨어가 설치되어 있지 않을 수 있습니다. 따라서 필요한 패키지를 설치해야 합니다.

1. 명령 프롬프트 또는 터미널에서 다음 명령을 실행하여 필수 패키지를 설치합니다.

```bash
sudo yum update -y
sudo yum install python3 python3-devel gcc -y
```

2. Scrapy를 설치합니다.

```bash
pip3 install scrapy
```

## 4. Scrapy 프로젝트 생성 및 실행

이제 Scrapy 프로젝트를 생성하고 실행해 보겠습니다.

1. 터미널에서 프로젝트를 생성할 디렉토리로 이동합니다.

```bash
cd /path/to/project/directory
```

2. Scrapy 프로젝트를 생성합니다.

```bash
scrapy startproject myproject
```

3. 생성된 프로젝트로 이동합니다.

```bash
cd myproject
```

4. Scrapy 스파이더를 생성합니다.

```bash
scrapy genspider myspider example.com
```

5. 스파이더 코드를 편집합니다.

```bash
vi myspider.py
```

6. 스파이더를 실행합니다.

```bash
scrapy crawl myspider
```

이제 Scrapy를 사용하여 웹사이트에서 데이터를 크롤링하는 프로세스가 EC2 인스턴스에서 실행되고 있습니다.

## 5. 크롤링 데이터 저장

크롤링된 데이터를 로컬 환경이 아닌 AWS S3 등의 스토리지에 저장하는 것이 좋습니다. 이렇게 하면 데이터 손실의 위험을 감소시킬 수 있습니다.

Scrapy 프로젝트 설정 파일(`settings.py`)에 다음과 같은 코드를 추가하여 AWS S3에 데이터를 저장합니다.

```python
FEED_URI = 's3://my-bucket/data.json'
FEED_FORMAT = 'jsonlines'
```

위 코드에서 `my-bucket`은 AWS S3 버킷 이름, `data.json`은 저장할 파일 이름입니다. 필요에 따라 변경할 수 있습니다.

## 6. 스케줄링 및 자동화

크롤링 프로세스를 AWS EC2 인스턴스에서 설정된 동안만 실행하고 싶다면, AWS CloudWatch Events나 AWS Lambda를 사용하여 스케줄링 및 자동화할 수 있습니다. 이를 통해 주기적으로 크롤링 작업을 실행할 수 있고, EC2 인스턴스 비용을 절감할 수 있습니다.

AWS CloudWatch Events 및 AWS Lambda 사용 방법은 이 블로그 포스트의 범위를 벗어나므로 자세한 내용은 AWS 문서를 참조하시기 바랍니다.

## 결론

Scrapy와 AWS EC2를 통합하여 크롤링 프로세스를 실행할 수 있습니다. 이를 통해 높은 성능과 확장성을 가진 웹 크롤링 솔루션을 구축할 수 있으며, 데이터 저장과 스케줄링 자동화도 간편하게 수행할 수 있습니다. 자세한 내용은 Scrapy와 AWS 문서를 참조하시기 바랍니다. Happy Crawling!
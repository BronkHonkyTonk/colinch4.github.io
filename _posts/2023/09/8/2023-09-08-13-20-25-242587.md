---
layout: post
title: "[파이썬] regex 문자열 토큰화"
description: " "
date: 2023-09-08
tags: [python,regex]
comments: true
share: true
---

Regular expressions (regex) are a powerful tool for pattern matching and text processing. In Python, the `re` module provides functions to work with regex. One common use case is tokenizing a string based on regex patterns.

Tokenization is the process of breaking down a string into smaller units, typically words or tokens. It is a crucial step in many natural language processing tasks, such as text classification or sentiment analysis. With regex, we can define patterns to identify different types of tokens and split the string accordingly.

Let's see how to tokenize a string using regex patterns in Python:

```python
import re

text = "Hello, regex tokenization!"
pattern = r'\W+'  # Match one or more non-word characters

tokens = re.split(pattern, text)
print(tokens)
```

Output:
```
['Hello', 'regex', 'tokenization', '']
```

In the above code, we import the `re` module and define our string `text`. The regex pattern `r'\W+'` matches one or more non-word characters, which includes any character that is not a letter, digit, or underscore. This pattern will split the string at these non-word characters and return the tokens.

The `re.split()` function is used to split the string based on the regex pattern. It returns a list of tokens.

In the example output, we can see that the string has been tokenized into individual words: "Hello", "regex", and "tokenization". The empty string at the end is the result of splitting at the exclamation mark.

We can customize the regex pattern based on our specific tokenization requirements. For example, we may want to include hyphens and apostrophes as valid characters within tokens. In that case, we can modify the pattern to `r"[\w'-]+"`.

By using regex to tokenize strings, we can easily break them down into meaningful units for further analysis or processing. This technique offers flexibility and precision in handling text data.

Remember to add appropriate documentation, error handling, and consider performance considerations when implementing regex tokenization in real-world scenarios.
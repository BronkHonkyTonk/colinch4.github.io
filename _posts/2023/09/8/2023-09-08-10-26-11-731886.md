---
layout: post
title: "[파이썬] Google Cloud Composer 데이터 워크플로 오케스트레이션"
description: " "
date: 2023-09-08
tags: [python,google cloud python]
comments: true
share: true
---

![Google Cloud Composer](https://cloud.google.com/images/products/composer/composer-logo.png)

Google Cloud Composer는 Google Cloud Platform에서 호스팅되는 전문적인 데이터 워크플로 관리 서비스입니다. 이러한 서비스는 데이터 파이프라인의 작업을 자동화하고 간편하게 관리할 수 있도록 도와줍니다. 

이 블로그 게시물에서는 Python을 사용하여 Google Cloud Composer를 사용하는 방법에 대해 알아보겠습니다. 이를통해 데이터 파이프라인을 구축하고 관리하는 방법을 실습해보세요.

## Google Cloud Composer 설정

1. Google Cloud Console에 로그인합니다.
2. "Compose" 검색 상자에 "Cloud Composer"를 입력하고 선택합니다.
3. "서비스 계정 만들기(Create service account)" 버튼을 클릭하고 액세스할 프로젝트를 선택합니다.
4. "역할(Role)"을 선택하고 "Cloud Composer 작업자(Composer Worker)"로 설정합니다.
5. "키 생성(Create key)" 버튼을 클릭하고 "JSON" 형식으로 키를 저장합니다.

## Python 코드 작성

Google Cloud Composer 데이터 워크플로를 만들기 위해 Python 코드를 작성해보겠습니다. 파일 이름을 `workflow.py`로 지정합니다.

```python
from airflow import DAG
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from datetime import datetime

# DAG(Directed Acyclic Graph) 객체 생성
dag = DAG(
    'my_workflow',
    start_date=datetime(2022, 1, 1),
    schedule_interval='@daily'
)

# BigQueryOperator를 사용하여 작업 정의
bq_task = BigQueryOperator(
    task_id='my_bq_task',
    sql='SELECT * FROM `project_id.dataset.table`',
    destination_dataset_table='project_id.output_dataset.output_table',
    dag=dag
)

# 워크플로 간에 작업 순서를 정의
bq_task
```

위의 코드에서는 `airflow` 모듈을 사용하여 DAG 객체를 생성하고, `BigQueryOperator`를 사용하여 BigQuery 작업을 정의합니다. DAG의 시작 일자와 스케줄 간격을 설정하고, BigQuery 작업의 SQL 쿼리 및 결과 테이블을 지정합니다.

## Google Cloud Composer에 워크플로 업로드

1. Google Cloud Console에 로그인합니다.
2. "Compose" 검색 상자에 "Cloud Composer"를 입력하고 선택합니다.
3. "Environments" 탭을 선택하고 원하는 Composer 환경을 클릭합니다.
4. "DAGs" 탭을 선택하고 "DAG 넣기(Upload DAG)" 버튼을 클릭합니다.
5. "workflow.py" 파일을 선택하고 "업로드(Upload)" 버튼을 클릭합니다.

## 워크플로 실행

Google Cloud Composer에 워크플로를 업로드하면 자동으로 실행됩니다. 워크플로의 진행 상태와 결과를 확인하려면 다음 단계를 따릅니다.

1. Google Cloud Console에 로그인합니다.
2. "Compose" 검색 상자에 "Cloud Composer"를 입력하고 선택합니다.
3. "Environments" 탭을 선택하고 원하는 Composer 환경을 클릭합니다.
4. "DAGs" 탭을 선택하고 실행 중인 워크플로를 확인합니다.
5. 필요에 따라 각 작업의 로그 및 출력을 확인하고 분석합니다.

## 결론

Python을 사용하여 Google Cloud Composer를 이용하여 데이터 워크플로를 관리하는 방법에 대해 알아보았습니다. Google Cloud Composer를 사용하면 간편하게 데이터 파이프라인을 관리하고 실행할 수 있습니다. 이를 통해 데이터 엔지니어링 작업을 보다 효율적으로 수행할 수 있습니다.

Google Cloud Composer 공식 문서에서 더 자세한 내용을 찾아보세요.
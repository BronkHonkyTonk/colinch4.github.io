---
layout: post
title: "[파이썬] Airflow와 Hadoop HDFS 연동"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Airflow is an open-source platform that allows you to programmatically author, schedule, and monitor workflows. It provides a way to orchestrate complex data pipelines and manage task dependencies.

Hadoop HDFS (Hadoop Distributed File System) is a distributed file system designed to store and process large data sets across multiple machines. It is a core component of the Apache Hadoop ecosystem.

In this blog post, we will explore how to integrate Airflow with Hadoop HDFS using Python. We will demonstrate how to perform common operations such as reading and writing files in Hadoop HDFS from Airflow tasks.

## Prerequisites

Before we start, make sure you have the following installed:

- Airflow: Follow the official documentation to install and configure Airflow on your system.
- Hadoop: Install a Hadoop distribution or set up HDFS on a remote cluster. Make sure you have the necessary access permissions and network connectivity.

## Connecting to Hadoop HDFS

To connect to Hadoop HDFS from Airflow, we can use the `hdfs` module from the `pywebhdfs` library. This library provides a Python interface to interact with Hadoop HDFS using its REST API.

Install the library by running the following command:

```python
pip install pywebhdfs
```

Once installed, you can import the `PyWebHdfsClient` class and create a client object to connect to HDFS. Here's an example:

```python
from pywebhdfs.webhdfs import PyWebHdfsClient

# Create a client object
hdfs_client = PyWebHdfsClient(host='your_hdfs_host', port='your_hdfs_port', user_name='your_username')
```

Replace `your_hdfs_host`, `your_hdfs_port`, and `your_username` with the appropriate values for your Hadoop HDFS setup.

## Reading Files from HDFS

To read a file from HDFS in an Airflow task, you can use the `read_file` method of the `PyWebHdfsClient` class. Here's an example:

```python
# Read a file from HDFS
hdfs_file_path = '/path/to/your/file.txt'
data = hdfs_client.read_file(hdfs_file_path)
```

The `read_file` method returns the content of the file as a string.

## Writing Files to HDFS

To write a file to HDFS from an Airflow task, you can use the `create_file` method of the `PyWebHdfsClient` class. Here's an example:

```python
# Write a file to HDFS
hdfs_file_path = '/path/to/your/new_file.txt'
data = 'This is the content of the file.'
hdfs_client.create_file(hdfs_file_path, data, overwrite=True)
```

The `create_file` method takes the file path, content, and an optional `overwrite` parameter to determine whether to overwrite the file if it already exists.

## Conclusion

Integrating Airflow with Hadoop HDFS allows you to seamlessly incorporate Hadoop data processing into your workflow pipelines. By using the `pywebhdfs` library, you can easily read and write files in HDFS from Airflow tasks.

In this blog post, we covered the basic steps to connect to Hadoop HDFS and perform read and write operations. There are many more features and functionalities available, so make sure to explore the documentation of both Airflow and Hadoop HDFS for more advanced use cases.

Happy data orchestration!
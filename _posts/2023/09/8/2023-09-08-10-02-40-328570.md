---
layout: post
title: "[파이썬] Airflow Task Dependency 설정"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow is a powerful workflow management system that allows you to programmatically author, schedule, and monitor your workflows as directed acyclic graphs (DAGs). In Airflow, tasks represent a unit of work that needs to be executed, and dependencies between tasks help define the order in which tasks should be executed. In this blog post, we will explore how to set up task dependencies in Airflow using Python.

## Defining Tasks in Airflow

Before we dive into task dependencies, let's first understand how to define tasks in Airflow. In Airflow, tasks are defined as instances of the `BaseOperator` class or its subclasses. The `BaseOperator` class provides the basic functionality for executing tasks and can be extended to define custom operators.

To create a task, you need to define a Python function that represents the logic to be executed. This function will be wrapped inside an operator class and will be attached to a DAG. Here's an example:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def my_task():
    # code to be executed

dag = DAG('my_dag', start_date=datetime(2022, 1, 1))

task = PythonOperator(
    task_id='my_task',
    python_callable=my_task,
    dag=dag
)
```

In the above example, we define a function `my_task` as the logic to be executed. We then create a `DAG` object named `'my_dag'` with a start date. Finally, we create a `PythonOperator` called `task`, which represents our task, and attach it to the DAG. 

## Task Dependencies in Airflow

Task dependencies in Airflow are defined using `set_upstream` and `set_downstream` methods. `set_upstream` defines that the current task should run after the tasks passed as parameters, while `set_downstream` defines that the current task should run before the tasks passed as parameters.

Here's an example of how to define task dependencies:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def task_one():
    # code for task one

def task_two():
    # code for task two

def task_three():
    # code for task three

dag = DAG('my_dag', start_date=datetime(2022, 1, 1))

task_one = PythonOperator(
    task_id='task_one',
    python_callable=task_one,
    dag=dag
)

task_two = PythonOperator(
    task_id='task_two',
    python_callable=task_two,
    dag=dag
)

task_three = PythonOperator(
    task_id='task_three',
    python_callable=task_three,
    dag=dag
)

task_two.set_upstream(task_one)
task_three.set_upstream([task_one, task_two])
```

In the above example, we define three tasks: `task_one`, `task_two`, and `task_three`. We then use `set_upstream` to define the dependencies between the tasks. In this case, `task_one` should run before `task_two`, and both `task_one` and `task_two` should run before `task_three`.

## Conclusion

Task dependency management plays a crucial role in workflow orchestration, and Airflow provides a robust mechanism to define and manage task dependencies. By setting up task dependencies, you can control the flow and order of task execution in your workflows. This allows you to build complex workflows efficiently and ensure that your tasks are executed in the desired order.

In this blog post, we learned how to define tasks in Airflow using Python and how to set up task dependencies using `set_upstream` and `set_downstream` methods. Armed with this knowledge, you can now start building more structured and efficient workflows using Airflow. Happy task scheduling!
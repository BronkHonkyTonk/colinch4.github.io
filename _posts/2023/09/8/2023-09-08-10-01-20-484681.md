---
layout: post
title: "[파이썬] Airflow DAG(Directed Acyclic Graph) 작성"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow is an open-source platform for programmatically authoring, scheduling, and monitoring workflows. It allows you to define complex workflows as directed acyclic graphs (DAGs) and schedule them to run at specific intervals. In this blog post, we will explore how to write an Airflow DAG using Python.

## What is a DAG?

A Directed Acyclic Graph (DAG) is a concept used in computer science and mathematics to represent a collection of tasks or processes, where each task depends on one or more other tasks. The DAG ensures that there are no cycles or loops in the dependencies, meaning that the tasks can be executed in a specific order without encountering any circular dependencies.

## How to Write an Airflow DAG

To write an Airflow DAG, you need to define a Python script that imports the necessary modules and defines the tasks and their dependencies. Here is an example of how to write a simple Airflow DAG using the Python `datetime` module:

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

# Define the DAG with a start date and schedule interval
dag = DAG(
    'my_dag',
    start_date=datetime(2022, 1, 1),
    schedule_interval=timedelta(days=1)
)

# Define tasks as Python functions
def task1():
    print("Executing Task 1")

def task2():
    print("Executing Task 2")

# Define the tasks using PythonOperator
t1 = PythonOperator(
    task_id='task_1',
    python_callable=task1,
    dag=dag
)

t2 = PythonOperator(
    task_id='task_2',
    python_callable=task2,
    dag=dag
)

# Set task dependencies
t2.set_upstream(t1)
```

In this example, we first import the necessary modules: `datetime`, `timedelta`, `DAG`, and `PythonOperator` from the Airflow package. We then define the DAG by creating a new instance of the `DAG` class and providing it with a unique `dag_id`, a start date, and a schedule interval.

Next, we define the tasks as Python functions, `task1()` and `task2()`, which will be executed by the `PythonOperator`. The `PythonOperator` is responsible for executing the Python function as a task in the workflow.

Finally, we set the task dependencies using the `set_upstream()` method. In this example, we specify that `task2` depends on `task1`, meaning that `task1` will be executed first before `task2`.

## Conclusion

Writing an Airflow DAG using Python is a straightforward process. By defining tasks as Python functions and setting up the dependencies between them, you can create complex workflows that can be scheduled and monitored using the Airflow platform. With Airflow's robust scheduling capabilities and intuitive user interface, you can easily manage and automate your data pipelines or other workflow automation tasks.

So why not give it a try and start leveraging the power of Airflow DAGs for your workflow automation needs today?
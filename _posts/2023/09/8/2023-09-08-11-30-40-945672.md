---
layout: post
title: "[파이썬] csv 대용량 CSV 데이터의 효과적 관리"
description: " "
date: 2023-09-08
tags: [python,csv]
comments: true
share: true
---

CSV (Comma Separated Values) 형식은 데이터를 효과적으로 저장하고 전송하는 데 많이 사용되는 형식입니다. 그러나 대용량의 CSV 데이터를 처리하는 것은 도전적인 과제일 수 있습니다. 이 글에서는 Python을 사용하여 대용량 CSV 데이터를 효과적으로 관리하는 방법을 알아보겠습니다.

## 1. 메모리 효율적인 방법

대용량의 CSV 파일을 한 번에 메모리에 로드하는 것은 메모리 부족과 성능 저하를 일으킬 수 있습니다. 따라서, 메모리를 효율적으로 활용하기 위해 다음과 같은 방법을 고려해 볼 수 있습니다.

### 1.1. Chunk 단위로 처리하기

`pandas` 라이브러리를 사용하여 CSV 파일을 읽을 때, `chunksize` 매개변수를 지정하여 Chunk 단위로 데이터를 처리할 수 있습니다. 이렇게하면 한 번에 데이터를 로드하지 않고, 필요한 만큼만 로드하여 메모리를 절약할 수 있습니다.

```python
import pandas as pd

chunksize = 100000  # 한 번에 처리할 데이터의 개수

for chunk in pd.read_csv('large_file.csv', chunksize=chunksize):
    # 데이터 처리 로직을 작성하세요
    process_chunk(chunk)
```

### 1.2. 메모리 맵 파일 사용하기

`numpy`의 `memmap` 클래스를 사용하여 대용량의 CSV 파일을 메모리 맵 파일로 처리할 수도 있습니다. 이 방법은 파일을 메모리에 통째로 로드하지 않고 필요한 부분만 읽어와 메모리에 매핑하는 방식으로 동작합니다.

```python
import numpy as np

data = np.memmap('large_file.csv', dtype='object', mode='r')

# 데이터에 접근하여 필요한 부분만 처리
process_data(data[:100000])
```

## 2. 병렬 처리 방법

대용량 CSV 데이터를 처리할 때, 병렬 처리를 통해 성능을 향상시킬 수 있습니다. 다음은 병렬 처리를 위해 사용할 수 있는 두 가지 방법입니다.

### 2.1. `multiprocessing` 모듈 사용하기

Python의 `multiprocessing` 모듈을 사용하여 병렬 처리를 구현할 수 있습니다. 이 모듈은 프로세스를 생성하여 여러 작업을 동시에 처리할 수 있는 기능을 제공합니다.

```python
import multiprocessing

num_processes = multiprocessing.cpu_count()  # CPU 코어의 개수
chunksize = int(len(data) / num_processes)  # 데이터를 적절히 분할하기 위한 크기

# 병렬 처리 함수 정의
def process_chunk(chunk):
    # 데이터 처리 로직을 작성하세요

# 프로세스 풀 생성
pool = multiprocessing.Pool(processes=num_processes)

# 각 프로세스에 작업 할당
results = [pool.apply_async(process_chunk, args=(chunk,))
           for chunk in np.array_split(data, num_processes)]

# 결과 수집
output = [res.get() for res in results]
```

### 2.2. `dask` 라이브러리 사용하기

`dask`는 병렬 처리와 분산 컴퓨팅을 지원하는 패키지로, 대용량 데이터 처리에 많이 활용됩니다. `dask`는 `pandas`와 유사한 API를 제공하여 사용하기 쉽습니다.

```python
import dask.dataframe as dd

df = dd.read_csv('large_file.csv')

# 데이터 처리 로직을 작성하세요
result = df.map_partitions(process_chunk).compute()
```

## 3. 인덱싱 및 필터링

대용량의 CSV 데이터를 처리할 때, 특정 행이나 열에만 접근하여 처리해야하는 경우가 많습니다. 이러한 경우, 인덱싱 및 필터링 기능을 활용하여 필요한 데이터에만 접근하여 처리할 수 있습니다.

### 3.1. 열 필터링하기

`pandas`의 `read_csv` 함수를 사용할 때, `usecols` 매개변수를 사용하여 필요한 열만 선택하여 데이터를 읽을 수 있습니다.

```python
import pandas as pd

df = pd.read_csv('large_file.csv', usecols=['column1', 'column2'])
```

### 3.2. 조건에 맞는 행 필터링하기

특정 조건에 맞는 행만 필터링하고 싶은 경우, `pandas`의 불린 인덱싱을 활용할 수 있습니다. 이를 통해 조건에 맞는 행만 선택하여 처리할 수 있습니다.

```python
import pandas as pd

df = pd.read_csv('large_file.csv')

# 조건에 맞는 행 필터링
filtered_df = df[df['column1'] > 100]

# 필터링된 행 처리
process_data(filtered_df)
```

## 결론

Python을 사용하여 대용량 CSV 데이터를 효과적으로 관리하는 방법에 대해 알아보았습니다. 메모리 효율적인 방법으로 데이터를 처리하고, 병렬 처리를 통해 성능을 향상시킬 수 있으며, 인덱싱 및 필터링 기능을 통해 필요한 데이터에만 접근하여 처리할 수 있습니다. 이러한 기법들을 활용하여 대용량 CSV 데이터 처리에 대한 도전을 극복해 보세요.
---
layout: post
title: "[파이썬] Airflow 설치 및 설정"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Airflow는 데이터 플로우를 조정하고 스케줄링하는 오픈 소스 플랫폼으로, 대규모 데이터 파이프라인을 쉽고 효율적으로 구축할 수 있습니다. 이번 블로그 포스트에서는 Airflow를 설치하고 기본 설정하는 방법에 대해 알아보겠습니다.

## 1. Airflow 설치

Airflow는 파이썬으로 개발되어 있는데, 따라서 파이썬 패키지 매니저인 pip를 사용하여 설치할 수 있습니다. 다음 명령을 터미널에서 실행하여 Airflow를 설치합니다.

```python
$ pip install apache-airflow
```

## 2. Airflow 초기화

Airflow를 설치한 후에는 초기화 작업이 필요합니다. 터미널에서 다음 명령을 실행하여 초기화를 수행합니다.

```python
$ airflow initdb
```

위 명령은 Airflow의 메타데이터를 저장할 SQLite 데이터베이스를 생성하고 초기 테이블을 설정합니다.

## 3. Airflow 설정

Airflow의 기본 설정 파일은 `airflow.cfg`입니다. 이 파일을 수정하여 사용자 지정 설정을 구성할 수 있습니다. 일부 중요한 설정 옵션은 다음과 같습니다.

- **dags_folder**: Airflow에서 실행할 DAG(Directed Acyclic Graph) 파일들이 위치하는 폴더를 설정합니다.
- **executor**: Airflow 작업을 실행할 executor 종류를 설정합니다. 기본값은 `SequentialExecutor`인데, 단일 노드 환경에서 사용하기에 적합합니다. 대규모 클러스터 환경에서는 `CeleryExecutor` 등을 고려해야 합니다.
- **sql_alchemy_conn**: Airflow 메타데이터를 저장할 데이터베이스를 설정합니다. 기본값은 SQLite이며, PostgreSQL 또는 MySQL과 같은 다른 데이터베이스를 사용할 수도 있습니다.

`airflow.cfg` 파일은 텍스트 편집기로 열어서 설정을 수정할 수 있습니다.

## 4. Airflow 웹 서버 시작

Airflow를 사용하기 위해 웹 서버를 시작해야 합니다. 터미널에서 다음 명령을 실행하여 웹 서버를 시작합니다.

```python
$ airflow webserver
```

Airflow 웹 서버가 시작되면 웹 브라우저에서 http://localhost:8080에 접속하여 Airflow 웹 UI를 확인할 수 있습니다.

## 5. Airflow 스케줄러 시작

Airflow의 스케줄러는 DAG를 실행하고 작업들을 스케줄링하는 역할을 합니다. 스케줄러를 시작하려면 새로운 터미널 창을 열고 다음 명령을 실행합니다.

```python
$ airflow scheduler
```

스케줄러가 작동하면서 설정된 주기에 따라 DAG를 실행하고 작업들을 스케줄링합니다.

## 6. DAG 작성 및 실행

Airflow에서는 데이터 파이프라인을 구성하기 위해 DAG 파일을 작성해야 합니다. DAG 파일은 Python 스크립트로 작성되며, Airflow에서 실행할 작업들을 정의하는 코드가 포함됩니다. 작성한 DAG 파일을 `dags_folder`에 복사하고, Airflow 웹 UI에서 DAG를 활성화하여 실행할 수 있습니다.

예를 들어, `example_dag.py`라는 파일을 작성하고 DAG를 정의해보겠습니다.

```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime

default_args = {
    'start_date': datetime(2022, 1, 1)
}

with DAG('example_dag', default_args=default_args, schedule_interval='@daily') as dag:
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date'
    )
```

위 DAG 파일은 매일 실행되며, `print_date`라는 이름의 작업을 실행하여 현재 날짜를 출력합니다.

## 결론

이제 Airflow를 설치하고 설정하는 방법을 알아보았습니다. Airflow를 사용하여 데이터 파이프라인을 구축하고 스케줄링하기 위해 DAG 파일을 작성하는 방법에 대해서도 알아보았습니다. Airflow는 대규모 데이터 처리 작업을 간편하게 관리할 수 있는 강력한 도구이므로, 데이터 엔지니어나 데이터 사이언티스트에게 유용한 플랫폼입니다.
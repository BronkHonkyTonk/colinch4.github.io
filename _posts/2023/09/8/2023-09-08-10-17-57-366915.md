---
layout: post
title: "[파이썬] Airflow와 Prometheus 연동"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

![Airflow and Prometheus](https://images.unsplash.com/photo-1600675244624-31a6c4020b9e)

Airflow is a popular open-source platform for programmatically authoring, scheduling, and monitoring workflows. It allows you to create complex workflows and manage dependencies between tasks. Prometheus, on the other hand, is a powerful monitoring and alerting toolkit. It helps you gather metrics from your applications and services, store them, and visualize them in real-time.

Integrating Airflow with Prometheus can give you better insights into the performance and health of your workflows. In this blog post, we will discuss how to set up the integration between Airflow and Prometheus using Python.

## Prerequisites

Before we start, make sure you have the following:

1. Airflow installed and running
2. Prometheus installed and running

## Installing Prometheus client library

To get started, you need to install the Prometheus client library for Python. You can install it using pip:

```
$ pip install prometheus-client
```

## Instrumenting Airflow with Prometheus

Airflow exposes several metrics that can be scraped by Prometheus. We can use the Prometheus client library to instrument our Airflow environment and expose these metrics to Prometheus.

Create a new Python file called `prometheus_airflow.py` and add the following code:

```python
import time
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

def collect_metrics():
    registry = CollectorRegistry()
    g = Gauge('airflow_task_count', 'Number of tasks', registry=registry)
    g.set(len(tasks))
    push_to_gateway('localhost:9091', job='airflow', registry=registry)

if __name__ == '__main__':
    while True:
        collect_metrics()
        time.sleep(60)
```

In the above code, we define a function `collect_metrics()` that creates a **Gauge** metric named `airflow_task_count`. We then push this metric to the Prometheus Pushgateway. The `time.sleep(60)` code makes sure that the metrics are collected and pushed every 60 seconds.

## Configuring Airflow to expose metrics

To enable Airflow to expose metrics, you need to make some changes to the Airflow configuration. Open the `airflow.cfg` file and make the following changes:

```ini
[metrics]
statsd_on=True
statsd_host=localhost
statsd_port=8125
statsd_prefix=airflow
```

The above configuration enables Airflow to emit metrics to StatsD, a network daemon that collects and aggregates statistics. We configure it to use `localhost` as the host and `8125` as the port. We also set the `statsd_prefix` to `airflow`.

## Scraping Airflow metrics with Prometheus

To scrape the Airflow metrics, you need to configure Prometheus to collect data from the StatsD exporter. Open the `prometheus.yml` file and add the following configuration:

```yaml
scrape_configs:
  - job_name: 'airflow'
    metrics_path: '/metrics'
    static_configs:
      - targets: ['localhost:9102']
```

In the above configuration, we define a job named `airflow` and configure it to scrape the `/metrics` endpoint from `localhost` on port `9102`. Adjust the configuration if your Airflow and Prometheus setups use different ports or hosts.

## Visualizing Airflow metrics

Once you have set up the integration, Prometheus will start scraping the Airflow metrics and storing them. You can now use Prometheus' built-in query language, PromQL, to retrieve and visualize the metrics.

You can access the Prometheus UI by navigating to `http://localhost:9090` (adjust the port if necessary). From there, you can explore the collected Airflow metrics, create custom queries, and build dashboards.

## Conclusion

Integrating Airflow with Prometheus provides valuable insights into the performance and health of your workflows. By instrumenting Airflow with Prometheus, you can monitor task counts, durations, and other relevant metrics. This integration is a powerful addition to your monitoring stack and can help you troubleshoot and optimize your workflows effectively.
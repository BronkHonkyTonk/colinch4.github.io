---
layout: post
title: "[파이썬] csv 모듈의 성능 최적화"
description: " "
date: 2023-09-08
tags: [python,csv]
comments: true
share: true
---

CSV 파일은 데이터를 저장하고 공유하기 위한 매우 일반적인 형식입니다. Python은 CSV 파일을 처리하기 위한 강력한 내장 모듈인 `csv`를 제공합니다. 그러나 대용량의 CSV 파일을 처리할 때 성능 문제가 발생할 수 있습니다. 이번 글에서는 Python의 `csv` 모듈의 성능을 최적화하는 몇 가지 방법을 살펴보겠습니다.

### 1. DictReader나 DictWriter 사용

`csv` 모듈은 `csv.reader`와 `csv.writer`를 통해 CSV 파일을 읽고 쓸 수 있는 기능을 제공합니다. 그러나 대량의 데이터를 다룰 때 `csv.DictReader`와 `csv.DictWriter`를 사용하는 것이 더 효율적입니다.

`csv.DictReader`는 파일의 각 행을 딕셔너리로 읽어옵니다. 이를 통해 각 필드에 직접 접근할 수 있습니다. 마찬가지로 `csv.DictWriter`는 딕셔너리를 CSV 파일에 쓸 수 있게 해줍니다. 딕셔너리를 사용하면 필드의 이름을 키로 사용하여 필드에 액세스하는 것이 보다 직관적이고 빠릅니다.

### 예시:

``` python
import csv

# CSV 파일 읽기
with open('data.csv', 'r') as file:
    reader = csv.DictReader(file)
    for row in reader:
        # 필드에 직접 접근
        print(row['name'], row['email'])

# CSV 파일 쓰기
with open('data.csv', 'w') as file:
    fieldnames = ['name', 'email']
    writer = csv.DictWriter(file, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerow({'name': 'John Doe', 'email': 'johndoe@example.com'})
```

### 2. 중간 처리 없이 직접 읽고 쓰기

`csv` 모듈은 파일을 읽거나 쓸 때 중간 처리 과정이 필요합니다. 이를 통해 필드를 파싱하고 형식을 변환하는 등의 작업이 이루어집니다. 하지만 정확한 필드의 유형을 알고 있다면, 중간 처리 없이 직접 데이터를 읽고 쓰는 것이 성능을 향상시킬 수 있습니다.

### 예시:

``` python
import csv

# CSV 파일 직접 읽기
with open('data.csv', 'r') as file:
    reader = csv.reader(file)
    for row in reader:
        # 필요한 작업 수행
        name = row[0]
        email = row[1]
        # 처리된 데이터 사용

# CSV 파일 직접 쓰기
with open('data.csv', 'w') as file:
    writer = csv.writer(file)
    writer.writerow(['John Doe', 'johndoe@example.com'])
```

### 3. 청크(chunk) 단위로 읽기

대용량의 CSV 파일을 한 번에 통째로 읽는 것은 메모리 문제를 발생시킬 수 있습니다. 이를 해결하기 위해 `csv` 모듈의 `reader` 객체에 `chunksize` 매개변수를 사용하여 청크 단위로 파일을 조각조각 읽을 수 있습니다. 이를 통해 메모리 사용량을 제한하고 읽기 성능을 향상시킬 수 있습니다.

### 예시:

``` python
import csv

chunksize = 1000

# 청크 단위로 파일 읽기
with open('data.csv', 'r') as file:
    reader = csv.reader(file)
    while True:
        rows = [next(reader) for _ in range(chunksize)]
        if not rows:
            break
        # 필요한 작업 수행
        for row in rows:
            name = row[0]
            email = row[1]
            # 처리된 데이터 사용
```

### 4. 병렬 처리 사용

CSV 파일을 처리하는 데 많은 시간이 걸린다면, 병렬 처리를 사용해야 할 수도 있습니다. Python의 `concurrent.futures` 모듈을 사용하여 간단하게 병렬 처리할 수 있습니다. 파일을 청크 단위로 나누고 병렬로 처리하여 전체 작업 속도를 향상시킬 수 있습니다.

### 예시:

``` python
import csv
from concurrent.futures import ThreadPoolExecutor

chunksize = 1000

# 병렬 처리 함수
def process_chunk(chunk):
    # 필요한 작업 수행
    for row in chunk:
        name = row[0]
        email = row[1]
        # 처리된 데이터 사용

# CSV 파일 병렬 처리
with open('data.csv', 'r') as file:
    reader = csv.reader(file)
    chunks = [chunk for chunk in iter(lambda: list(islice(reader, chunksize)), [])]
    with ThreadPoolExecutor() as executor:
        executor.map(process_chunk, chunks)
```

이러한 성능 최적화 기법을 사용하여 Python의 `csv` 모듈의 성능을 높일 수 있습니다. 단순한 CSV 파일 처리 작업에서도 이러한 최적화는 많은 성능 향상을 가져올 수 있습니다. 비록 파일 크기, 하드웨어 및 작업 유형에 따라 결과가 달라질 수 있지만, 확실히 시도해볼 가치가 있습니다.

프로젝트에 적용하기 전에 각 기법의 성능 특징을 평가하고, 작업에 가장 적합한 방법을 선택하세요.
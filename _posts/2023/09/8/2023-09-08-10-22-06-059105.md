---
layout: post
title: "[파이썬] Airflow Fault Tolerance"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Airflow is an open-source platform to programmatically author, schedule, and monitor workflows. It allows you to define complex data pipelines as directed acyclic graphs (DAGs). With its robust features and extensibility, Airflow has gained popularity in the data engineering world.

One of the key aspects of any data pipeline is **fault tolerance**. Fault tolerance refers to the ability of a system to continue functioning properly even in the presence of failures. In the context of Airflow, fault tolerance ensures that your workflows can handle and recover from failures gracefully.

## Task-level Fault Tolerance

By default, Airflow retries a failed task a few times before giving up. This behavior is controlled by the `retries` parameter, which you can set in each task definition in your DAG.

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def my_task():
    # Task logic here

dag = DAG(
    'my_dag',
    schedule_interval='*/5 * * * *',
    start_date=datetime(2022, 1, 1)
)

task1 = PythonOperator(
    task_id='task1',
    python_callable=my_task,
    retries=3,  # Set the number of retries
    dag=dag
)
```

In the above example, `retries` is set to 3, which means Airflow will retry the task three times before considering it a failure. You can adjust this value based on your specific use case.

Additionally, you can set the `retry_delay` parameter to specify the delay between retries in seconds:

```python
task1 = PythonOperator(
    task_id='task1',
    python_callable=my_task,
    retries=3,
    retry_delay=timedelta(minutes=5),  # Set the delay between retries
    dag=dag
)
```

In the above example, there will be a 5-minute delay between each retry of the task.

## DAG-level Fault Tolerance

Airflow provides a built-in mechanism to handle failures at the DAG level using the `on_failure_callback` parameter. You can specify a function to be called whenever a task in the DAG fails.

```python
def my_failure_callback(context):
    # Handle failure logic here

dag = DAG(
    'my_dag',
    schedule_interval='*/5 * * * *',
    start_date=datetime(2022, 1, 1),
    on_failure_callback=my_failure_callback  # Set the failure callback function
)
```

The `my_failure_callback` function will receive a `context` parameter that contains information about the failed task. You can utilize this information to implement custom error handling, send alerts, or perform any other desired actions.

## External Dependencies Fault Tolerance

Airflow allows you to define **external dependencies** for your tasks. These dependencies are conditions that need to be met before a task can execute. Airflow provides various types of sensors to handle external dependencies, such as `TimeSensor`, `FileSensor`, `HttpSensor`, etc.

By utilizing sensors, you can make your workflows more fault-tolerant by ensuring that the required resources or conditions are available before executing a task. For example, you can use a `HttpSensor` to check the availability of an API endpoint before triggering a task that relies on it.

```python
from airflow.contrib.sensors.http_sensor import HttpSensor

task2 = HttpSensor(
    task_id='task2',
    http_conn_id='my_http_connection',
    endpoint='/api/health',
    method='HEAD',
    response_check=lambda response: True if response.status_code == 200 else False,
    retries=5,
    retry_delay=timedelta(minutes=1),
    dag=dag
)
```

In the above example, the `HttpSensor` will make a `HEAD` request to the specified endpoint and check if the response status code is `200`. If the check fails, it will retry the request 5 times with a 1-minute delay between each retry.

## Conclusion

Fault tolerance is a crucial aspect of any data pipeline. Airflow provides several mechanisms to handle failures at both the task and DAG levels, as well as to handle external dependencies. By leveraging these features, you can ensure that your workflows continue to execute reliably even in the face of failures.
---
layout: post
title: "[파이썬] Airflow와 BigQuery 연동"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow is an open-source platform used for programmatically authoring, scheduling, and monitoring workflows. It allows developers to define complex workflows as directed acyclic graphs (DAGs) and execute them on a schedule or manually. One common use case is to integrate Airflow with Google BigQuery, a fully-managed, serverless data warehouse solution.

In this blog post, we will explore how to connect Airflow with BigQuery and perform various data operations using Python.

## Installing Airflow and Dependencies

First, let's install Airflow and the necessary dependencies:

```python
pip install apache-airflow
pip install apache-airflow[google]
```

The `apache-airflow[google]` package includes the necessary components for integrating Airflow with Google services like BigQuery.

## Creating a Connection

To connect Airflow with BigQuery, we need to set up a connection in the Airflow configuration. This configuration allows Airflow to authenticate and communicate with BigQuery.

1. Open the Airflow configuration file (`airflow.cfg`) and locate the `[core]` section.
2. Add the following line to enable the BigQuery hook:

```ini
load_examples=False
```

3. Locate the `[google_cloud_platform]` section and add the necessary connection settings:

```ini
[google_cloud_platform]
project_id=<YOUR_PROJECT_ID>
key_path=<PATH_TO_YOUR_SERVICE_ACCOUNT_KEY_FILE>
```

Replace `<YOUR_PROJECT_ID>` with your Google Cloud project ID, and `<PATH_TO_YOUR_SERVICE_ACCOUNT_KEY_FILE>` with the path to your service account key file.

## Using the BigQuery Hook

Now that we have set up the connection, we can use the BigQuery hook to interact with BigQuery.

Here is an example of how to execute a query in BigQuery using the BigQuery hook:

```python
from airflow.contrib.hooks import BigQueryHook

def execute_bigquery_query():
    hook = BigQueryHook()
    sql = "SELECT * FROM dataset.table"
    results = hook.get_pandas_df(sql)
    print(results)
```

We import the `BigQueryHook` from the `contrib.hooks` module and create an instance of it. Then, we can pass any BigQuery SQL query to the `get_pandas_df()` method to retrieve the query results as a Pandas DataFrame.

## Running a BigQuery Job in Airflow

To run a BigQuery job in Airflow, we can use the `BigQueryOperator` provided by the `google.cloud.bigquery` library.

Here is an example of how to create and execute a BigQuery job using the `BigQueryOperator`:

```python
from airflow.contrib.operators.bigquery_operator import BigQueryOperator

dag = DAG(
    'bigquery_job',
    schedule_interval='@daily',
    default_args={
        'start_date': datetime(2022, 1, 1),
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    },
)

bq_operator = BigQueryOperator(
    task_id='bigquery_task',
    sql='SELECT * FROM dataset.table',
    destination_dataset_table='project_id.dataset.table',
    dag=dag,
)
```

In this example, we define a `DAG` with a daily schedule interval and set default arguments for the job. We create a `BigQueryOperator` instance with the SQL query and destination dataset table. We pass the operator to the DAG for execution.

## Conclusion

Integrating Airflow with BigQuery allows us to leverage the power of Airflow's workflow management capabilities with BigQuery's data processing capabilities. We can easily schedule and monitor data pipelines, execute queries, and perform various operations on BigQuery data using Python.

In this blog post, we explored how to connect Airflow with BigQuery, execute queries using the BigQuery hook, and run BigQuery jobs in Airflow. This is just the beginning of what can be accomplished with this powerful combination.

Now, go ahead and try it out yourself, and see how you can automate and streamline your data workflows with Airflow and BigQuery!
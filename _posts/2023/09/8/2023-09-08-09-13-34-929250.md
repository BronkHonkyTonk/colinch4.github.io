---
layout: post
title: "[파이썬] FastAPI API 캐싱 전략"
description: " "
date: 2023-09-08
tags: [python,FastAPI]
comments: true
share: true
---

## Introduction

Caching is an effective method to improve the performance and reduce the load on your API. It allows you to store the response of an API call and serve it directly from cache when the same request is made again. FastAPI, a modern and high-performance web framework for building APIs with Python, provides various options to implement caching strategies.

In this blog post, we will explore different caching strategies that can be implemented in FastAPI using Python.

## Caching Strategies in FastAPI

### 1. In-Memory Caching

One of the simplest caching strategies is to use in-memory caching. This involves storing the API response in memory and serving it directly from cache without making the actual API call again.

Here's an example of how you can implement in-memory caching in FastAPI using the `cachetools` library:

```python
import cachetools

cache = cachetools.Cache(maxsize=100)
cache_get = cache.get

@app.get("/api/data")
def get_data():
    data = cache_get("data")
    if data is None:
        data = make_expensive_api_call()
        cache["data"] = data
    return data
```

In this example, we create an instance of the `Cache` class from the `cachetools` library and define a route `/api/data` that serves the cached data if available, or makes an expensive API call and caches the response if not.

### 2. Redis Caching

Redis is an in-memory data structure store that can be used as a more scalable and persistent caching solution. FastAPI provides built-in support for Redis caching using the `aioredis` library.

To use Redis caching in FastAPI, you need to set up a Redis server and install the `aioredis` library:

```python
import aioredis

redis = aioredis.from_url("redis://localhost")

@app.get("/api/data")
async def get_data():
    data = await redis.get("data")
    if data is None:
        data = make_expensive_api_call()
        await redis.set("data", data, expire=3600)  # Cache data for 1 hour
    return data
```

In this example, we create a connection to the Redis server using `aioredis.from_url()` and define a route `/api/data` that serves the cached data if available, or makes an expensive API call and caches the response in Redis if not.

### 3. Response-Based Caching

Response-based caching allows you to cache the response based on the request parameters and headers. FastAPI provides a decorator `@cache()` that can be used to implement response-based caching.

Here's an example of how you can implement response-based caching in FastAPI:

```python
from fastapi_cache import caches, close_caches, FastAPICache

cache = FastAPICache()

@app.on_event("startup")
async def startup():
    await caches.set("data", expire=3600)  # Cache data for 1 hour

@app.get("/api/data")
@cache("data")
def get_data():
    return make_api_call()
```

In this example, we define a route `/api/data` that is decorated with `@cache("data")`. The decorator `@cache()` internally uses the cache instance created from `FastAPICache` to cache the response based on the request parameters.

## Conclusion

Implementing caching strategies can greatly improve the performance of your FastAPI API. In this blog post, we explored different caching strategies, including in-memory caching, Redis caching, and response-based caching.

You can choose the caching strategy that best suits your application's requirements and scalability needs. Remember to carefully consider the trade-offs in terms of performance, memory usage, and cache invalidation strategies.

By incorporating caching techniques into your FastAPI applications, you can provide faster and more efficient responses to your users while reducing the load on your backend resources.
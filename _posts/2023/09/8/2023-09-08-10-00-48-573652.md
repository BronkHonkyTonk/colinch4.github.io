---
layout: post
title: "[파이썬] Airflow Apache Airflow 기본 개념"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Airflow Apache는 Python으로 작성된 오픈 소스 워크플로우 관리 도구입니다. 이번 블로그 게시물에서는 Apache Airflow의 기본 개념을 살펴보겠습니다. 

## 1. Airflow의 구성요소

### DAGs (Directed Acyclic Graphs)
Airflow에서 작업의 흐름을 정의하는 DAGs가 기본 구성 요소입니다. DAG는 작업단위를 정의하고 코드 및 의존성을 관리하는데 사용됩니다. 각각의 작업은 Airflow에서 Task로 나타내며 이러한 작업들이 DAG 내에서 정의된 의존성을 따라 실행됩니다.

### Scheduler
Airflow의 스케줄러는 정의된 DAG에 따라 작업을 예약하고 실행 시간을 관리합니다. 스케줄러는 DAG의 의존성 그래프를 분석하여 실행 시간과 실행 간격을 계산하고 작업의 실행을 스케줄합니다.

### Executor
Airflow의 Executor는 작업을 실행할 방법을 결정합니다. 기본 Executor는 SequentialExecutor로, 작업을 하나씩 순차적으로 실행합니다. 다른 Executor도 사용할 수 있으며, 이러한 Executor는 동시에 여러 작업을 실행하여 병렬 처리를 가능하게 합니다.

### Web Server
Airflow의 웹 서버는 Airflow UI를 제공합니다. 웹 서버는 DAGs, 작업 실행 상태 및 로그를 모니터링하고 관리하는데 사용됩니다. 웹 서버를 통해 DAGs를 편집하고 작업을 수동으로 실행할 수도 있습니다.

### Database
Airflow는 작업, DAG, 작업 실행 정보 및 상태 등을 저장하기 위해 데이터베이스를 사용합니다. SQLite, PostgreSQL, MySQL 등 다양한 데이터베이스를 지원합니다.

## 2. DAG 정의하기

Airflow에서 DAG를 정의할 때는 Python 스크립트를 사용합니다. DAG 정의는 DAG 클래스의 인스턴스화를 통해 이루어지며, DAG의 이름, 기본 설정 및 작업의 의존성을 정의할 수 있습니다. 작업은 Python 함수로 정의되며, Operator 클래스를 사용하여 구성됩니다.

```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator

default_args = {
    'start_date': datetime(2022, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'my_first_dag',
    default_args=default_args,
    description='My first Airflow DAG',
    schedule_interval='0 0 * * *',
)

task1 = BashOperator(
    task_id='task1',
    bash_command='echo Task 1',
    dag=dag,
)

task2 = BashOperator(
    task_id='task2',
    bash_command='echo Task 2',
    dag=dag,
)

task3 = BashOperator(
    task_id='task3',
    bash_command='echo Task 3',
    dag=dag,
)

task1 >> task2 >> task3
```

위 예시에서는 세 개의 작업(task1, task2, task3)이 정의되고, 이러한 작업들이 DAG의 의존성을 따라 실행됩니다. 작업은 BashOperator를 사용하여 쉘 명령을 실행하는 단순한 예제입니다.

## 3. DAG 실행하기

DAG를 실행하려면 Airflow 스케줄러에게 명령을 내려야 합니다. 이렇게 하면 스케줄러가 작업을 예약하고 실행을 처리합니다. `airflow dags trigger` 명령어를 사용하여 DAG를 수동으로 실행할 수도 있습니다.

```bash
airflow scheduler
```

위 명령어를 통해 스케줄러가 예약된 작업을 주기적으로 실행합니다. 실행된 작업의 상태 및 로그는 Airflow UI를 통해 확인할 수 있습니다.

## 4. 확장성과 유연성

Airflow는 매우 확장 가능하며, 다양한 플러그인 및 확장 기능을 통해 기능을 추가할 수 있습니다. Airflow를 사용하여 데이터 파이프라인, 작업 스케줄링, ML 모델 훈련 등 다양한 작업을 자동화할 수 있습니다.

## 결론

이상으로 Apache Airflow의 기본 개념을 살펴보았습니다. Airflow를 사용하면 복잡한 워크플로우를 쉽게 관리하고 자동화할 수 있습니다. 추가적인 자세한 내용은 [공식 Airflow 문서](https://airflow.apache.org/docs/)를 참조해주세요.
---
layout: post
title: "[파이썬] Airflow Executor 종류 및 설정"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow is an open-source platform used for creating, scheduling, and monitoring workflows. One of the key components of Airflow is the executor, which determines how tasks within a workflow are executed. 

In this blog post, we will explore the types of executors available in Airflow and how to configure them in Python.

## Types of Executors

Airflow offers three types of executors to choose from:

1. **Sequential Executor**: This is the default executor and runs tasks sequentially on a single worker. It is suitable for small-scale deployments or debugging purposes where parallelism is not a requirement.

2. **Local Executor**: The local executor leverages parallelism by spinning up multiple worker processes on the same machine. Each worker can execute tasks independently, which can help speed up the workflow execution time. However, this executor is limited by the resources available on a single machine.

3. **Celery Executor**: The Celery executor integrates with [Celery](http://www.celeryproject.org/) - a distributed task queue framework. It allows you to distribute tasks across multiple worker nodes, enabling horizontal scalability. The Celery executor requires additional setup and configuration but is suitable for large-scale deployments with high parallelism requirements.

## Configuring Executors in Python

To configure executors in Airflow, you need to modify the `airflow.cfg` configuration file. The location of this file depends on your installation method.

For the Sequential Executor, no extra configuration is needed as it is the default executor. However, if you want to explicitly set it, you can add the following line to your `airflow.cfg` file:

```python
executor = SequentialExecutor
```

For the Local Executor, you need to specify the number of workers to spin up. Add the following lines to your `airflow.cfg` file:

```python
executor = LocalExecutor
parallelism = 16  # Number of parallel workers to run
```

Replace `16` with the desired number of workers you want to run simultaneously.

Finally, for the Celery Executor, you will need to configure both Airflow and Celery. Here are the steps:

1. Install Celery using `pip`:

```bash
pip install celery
```

2. Configure your Airflow `airflow.cfg` file:

```python
executor = CeleryExecutor
```

3. Create a separate `celery_config.py` file with the following contents:

```python
from datetime import timedelta

broker_url = 'your_broker_url'  # e.g., redis://localhost:6379/0
result_backend = 'your_backend_url'  # e.g., redis://localhost:6379/0
task_default_queue = 'your_queue_name'  # e.g., default
worker_prefetch_multiplier = 1
result_expires = timedelta(days=1)
```

Replace `your_broker_url`, `your_backend_url`, and `your_queue_name` with your actual Celery broker, backend, and queue names respectively.

## Conclusion

Understanding the different types of executors available in Airflow and knowing how to configure them is essential for optimizing your workflow execution. Whether you need sequential, local, or distributed task execution, Airflow has the right executor for your needs.
---
layout: post
title: "[파이썬] Airflow와 AWS S3 연동"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Airflow는 Apache Software Foundation에서 개발한 오픈 소스 플랫폼으로, **워크플로우 관리, 스케줄링 및 모니터링**을 할 수 있습니다. AWS S3는 Amazon Web Services에서 제공하는 **객체 스토리지 서비스**로, 비용 효율적이고 확장 가능한 데이터 저장을 제공합니다.

이 블로그 포스트에서는 Airflow와 AWS S3를 연동하는 방법을 알아보겠습니다.

## 1. Boto3 설치

Boto3는 AWS 서비스와 상호 작용하는 데 사용되는 Python 라이브러리입니다. 아래의 명령을 사용하여 Boto3를 설치합니다.

```python
pip install boto3
```

## 2. AWS S3 연결 설정

AWS S3에 연결하기 위해 AWS 계정에 액세스 키와 비밀 액세스 키가 필요합니다. 다음과 같이 환경 변수에 액세스 키를 설정합니다.

```python
import os
os.environ['AWS_ACCESS_KEY_ID'] = 'your_access_key'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret_access_key'
```

## 3. Airflow DAG에 S3 연동 추가

DAG(Directed Acyclic Graph)는 Airflow에서 작업의 순서 및 의존성을 정의하는데 사용됩니다. DAG에 S3 연동을 추가하기 위해 `PythonOperator`를 사용할 수 있습니다.

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2022, 1, 1),
    'retries': 1,
}

def upload_to_s3():
    import boto3
    s3 = boto3.client('s3')
    s3.upload_file('local_file.txt', 'bucket_name', 'remote_file.txt')

with DAG('s3_dag', default_args=default_args, schedule_interval='@once') as dag:
    task = PythonOperator(
        task_id='upload_to_s3',
        python_callable=upload_to_s3,
    )

    task
```

위의 예시에서는 `PythonOperator`를 사용하여 `upload_to_s3` 함수가 S3로 파일을 업로드하도록 구성했습니다. `boto3.client`로 S3 클라이언트를 생성하고, `s3.upload_file`로 로컬 파일을 S3에 업로드합니다. `bucket_name`과 `remote_file.txt`는 S3 버킷 이름과 업로드할 파일 이름을 의미합니다.

## 4. Airflow 실행

위의 DAG를 실행하기 전에, Airflow 환경에서 DAG를 등록해야 합니다. 다음 명령을 사용하여 DAG를 등록합니다.

```bash
airflow dags unpause s3_dag
```

그리고 Airflow 웹 인터페이스를 통해 DAG를 실행할 수 있습니다.

## 결론

이번 블로그 포스트에서는 Airflow와 AWS S3를 연동하는 방법에 대해 알아보았습니다. AWS S3를 활용하여 Airflow에서 데이터를 손쉽게 저장 및 관리할 수 있습니다. 이를 통해 복잡한 데이터 워크플로우를 효율적으로 관리할 수 있습니다.
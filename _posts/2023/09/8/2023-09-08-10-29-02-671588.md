---
layout: post
title: "[파이썬] Airflow와 Web Scraping"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

**Airflow** is an open-source platform used for orchestrating and scheduling workflows. It allows you to define and monitor complex workflows as directed acyclic graphs (DAGs). One of the powerful use cases of Airflow is combining it with **web scraping** to automate the process of data extraction from websites.

In this blog post, we will explore how to use Airflow and Python for web scraping. We will see how to set up a DAG in Airflow to scrape a website periodically and store the data in a database.

## Getting Started with Airflow

Before diving into web scraping, let's first install and set up Airflow on our system. To install Airflow, run the following command:

```bash
pip install apache-airflow
```

Once installed, initialize the Airflow database by running the following command:

```bash
airflow db init
```

To start the Airflow web server and scheduler, run:

```bash
airflow webserver -p 8080
airflow scheduler
```

You can access the Airflow web interface by opening http://localhost:8080 in your browser.

## Setting up the Web Scraping DAG

To create a DAG for web scraping, we need to define a Python script containing the DAG configuration. Let's create a file called `web_scraping_dag.py` and define the DAG as follows:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2022, 1, 1),
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'web_scraping_dag',
    default_args=default_args,
    schedule_interval=timedelta(days=1),
)

def scrape_website():
    # Web scraping code goes here
    # ...

scrape_task = PythonOperator(
    task_id='scrape_task',
    python_callable=scrape_website,
    dag=dag,
)

scrape_task
```

In the above code, we define a DAG named `web_scraping_dag`. The DAG is scheduled to run daily (`schedule_interval=timedelta(days=1)`). We also define a `scrape_task` that executes the `scrape_website` function using the `PythonOperator`.

## Web Scraping with Python

Now that we have set up the DAG, let's implement the web scraping logic inside the `scrape_website` function. We'll use the popular `BeautifulSoup` library for web scraping.

```python
from bs4 import BeautifulSoup
import requests

def scrape_website():
    url = 'https://example.com'  # Replace with the URL of the website you want to scrape
    response = requests.get(url)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # Use BeautifulSoup to extract data from the website
        # ...
    else:
        raise Exception('Failed to scrape website')
```

In the above code, we fetch the web page using the `requests` library and initialize a `BeautifulSoup` object with the HTML content. We can then use various methods provided by `BeautifulSoup` to extract the required data from the website.

## Storing Scraped Data

After scraping the website, we can store the extracted data in a database for further analysis. Let's use **SQLite** as an example database for simplicity. 

```python
import sqlite3

def scrape_website():
    # Web scraping code goes here

    conn = sqlite3.connect('data.db')
    cursor = conn.cursor()

    # Create a table if it doesn't exist
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS scraped_data (
            id INTEGER PRIMARY KEY,
            data TEXT
        )
    ''')

    # Insert the scraped data into the table
    cursor.execute('INSERT INTO scraped_data (data) VALUES (?)', (scraped_data,))

    conn.commit()
    conn.close()
```

In the above code, we create a database connection using `sqlite3` and create a table named `scraped_data` to store our scraped data. We then insert the data into the table using a prepared statement.

## Conclusion

In this blog post, we explored how to use Airflow and Python for web scraping. We learned how to set up a DAG in Airflow to schedule and automate the web scraping process. We also saw how to use the BeautifulSoup library for extracting data from websites. Finally, we stored the scraped data in an SQLite database for further analysis.

By combining Airflow's workflow management capabilities with web scraping, you can automate the extraction of data from websites and integrate it into your data pipelines for analysis and decision-making.

Remember to respect the website's policies and terms of service when scraping data and always follow ethical practices.
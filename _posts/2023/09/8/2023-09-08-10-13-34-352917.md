---
layout: post
title: "[파이썬] Airflow와 Apache Hive 연동"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow and Apache Hive are two powerful tools in the world of big data analytics. Airflow provides a platform for scheduling and orchestrating data workflows, while Hive is a data warehouse infrastructure built on top of Apache Hadoop.

In this blog post, we will explore how to integrate Airflow with Hive using python. To get started, make sure you have Airflow and Hive installed on your system.

## Setting up the Connection

The first step is to set up a connection between Airflow and Hive. Open your Airflow configuration file and locate the **airflow.cfg** file. Add the following lines to the file:

```python
[hive]
hive_cli_conn_id = hive_default
```

Next, open the **connections.py** file in your Airflow directory and add the following code:

```python
from airflow import settings
from airflow.models import Connection

def create_hive_connection():
    hive_conn = Connection(
        conn_id='hive_default',
        conn_type='hive_cli',
        host='localhost',
        port=10000,
        login='hiveuser',
        password='hivepassword'
    )
    session = settings.Session()
    session.add(hive_conn)
    session.commit()

create_hive_connection()
```
Make sure to modify the connection details according to your Hive setup. This code will create a new connection named 'hive_default' with the required parameters.

## Defining a HiveOperator in Airflow

Once the connection is set up, you can use the HiveOperator in your Airflow DAGs to execute Hive queries. Here's an example of how to define a HiveOperator:

```python
from airflow import DAG
from airflow.operators.hive_operator import HiveOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2022, 1, 1),
    'retries': 1,
}

dag = DAG(
    'hive_example',
    default_args=default_args,
    schedule_interval='@daily',
)

hive_query = """
    CREATE TABLE IF NOT EXISTS example_table (
        id INT,
        name STRING
    )
"""

hive_operator = HiveOperator(
    task_id='execute_hive_query',
    hql=hive_query,
    hive_cli_conn_id='hive_default',
    dag=dag,
)

hive_operator
```

In this example, we create a new DAG named 'hive_example' with a daily schedule. The HiveOperator is used to execute a Hive query, which creates a table named 'example_table' with two columns.

Make sure to replace the `hive_query` with your actual Hive query.

## Running the Airflow DAG

To run the DAG, save the python file and place it in your Airflow DAG directory. Once you start your airflow scheduler, it will automatically pick up the new DAG and start executing it according to the specified schedule.

You can monitor the progress of the DAG and view the logs in the Airflow UI.

## Conclusion

Integrating Apache Airflow with Apache Hive allows you to schedule and manage your Hive queries with ease. This combination enables you to build efficient data workflows and perform complex analytics tasks.

In this blog post, we covered the steps to set up the connection between Airflow and Hive and how to use the HiveOperator to execute Hive queries. With this knowledge, you can leverage the power of Airflow and Hive in your data analytics pipelines.
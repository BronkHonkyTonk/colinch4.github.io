---
layout: post
title: "[파이썬] Airflow와 Google Cloud Storage 연동"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow는 작업 스케줄링, 워크플로우 관리 및 작업 모니터링을 위한 오픈 소스 플랫폼입니다. 이렇게 강력한 도구를 사용하여 데이터 처리 파이프라인을 구축할 때 Google Cloud Storage (GCS)와의 연동은 매우 유용합니다. GCS는 Google Cloud Platform의 객체 스토리지 서비스로서 대규모의 데이터를 안전하게 저장할 수 있습니다.

이 블로그 포스트에서는 Python을 사용하여 Airflow와 GCS를 연동하는 방법에 대해 알아보겠습니다.

## 1. Google Cloud SDK 설치

Airflow와 GCS를 연동하기 위해 Google Cloud SDK를 설치해야 합니다. 클라우드 SDK는 Google Cloud Platform과 상호 작용할 수 있는 명령 줄 도구입니다. 다음 명령을 사용하여 Google Cloud SDK를 설치합니다.

```
$ curl https://sdk.cloud.google.com | bash
$ exec -l $SHELL
$ gcloud init
```

## 2. Google Cloud Storage에 접근 자격 증명 생성

Google Cloud Storage와 연결하기 위해 인증 정보가 필요합니다. GCP 콘솔에서 서비스 계정을 생성하고 JSON 형식의 인증 키를 다운로드합니다. 이 인증 키는 나중에 Airflow에서 사용될 것입니다.

## 3. Airflow DAG 작성

이제 Airflow DAG를 작성하여 GCS 파일을 읽고 쓸 수 있습니다. 다음은 DAG 파일의 예입니다.

```python
from airflow import DAG
from airflow.contrib.operators.gcs_to_gcs import GoogleCloudStorageToGoogleCloudStorageOperator
from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator

from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2021, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG('gcs_dag', default_args=default_args, schedule_interval='@daily')

task1 = GoogleCloudStorageToBigQueryOperator(
    task_id='gcs_to_bq',
    bucket='my_gcs_bucket',
    source_objects=['file.csv'],
    destination_project_dataset_table='my_project.my_dataset.my_table',
    autodetect=True,
    dag=dag,
)

task2 = GoogleCloudStorageToGoogleCloudStorageOperator(
    task_id='gcs_to_gcs',
    source_bucket='my_source_bucket',
    source_object='file.csv',
    destination_bucket='my_destination_bucket',
    destination_object='file.csv',
    move_object=True,
    google_cloud_storage_conn_id='google_cloud_default',
    dag=dag,
)

task1 >> task2
```

위의 DAG 예제에서는 GoogleCloudStorageToBigQueryOperator와 GoogleCloudStorageToGoogleCloudStorageOperator를 사용하여 GCS에서 BigQuery로 데이터를 로드하고 GCS로 데이터를 이동시키는 작업을 정의했습니다.

## 4. Airflow 구성 파일 업데이트

Airflow와 GCS를 연동하려면 Airflow 구성 파일을 업데이트해야 합니다. `airflow.cfg` 파일을 열고 다음과 같이 GCS 관련 설정을 추가합니다.

```ini
[core]
...
dags_folder = /path/to/your/dags

[operators]
...
gcs_bucket = my_gcs_bucket
gcs_project_id = my_project_id
gcs_key_path = /path/to/your/gcs/key.json
```

위의 예에서 `gcs_bucket`은 GCS의 버킷 이름, `gcs_project_id`는 GCP 프로젝트 ID, `gcs_key_path`는 업로드 한 GCS 인증 키 파일의 경로입니다. 이 정보는 DAG에서 사용됩니다.

## 5. DAG 실행 및 모니터링

이제 DAG를 실행할 준비가 되었습니다. Airflow 웹 UI를 통해 DAG를 스케줄링하고 실행할 수 있습니다. 또한 웹 UI에서 실행된 작업의 상태와 로그를 모니터링할 수 있습니다. Airflow의 강력한 기능을 활용하여 데이터 파이프라인을 자유롭게 구성하고 모니터링할 수 있습니다.

이제 Airflow와 Google Cloud Storage를 연동하여 데이터를 처리하는 강력한 파이프라인을 구축할 수 있습니다. 이를 통해 데이터 처리 작업을 자동화하고 간단하게 관리할 수 있습니다.

Happy coding!
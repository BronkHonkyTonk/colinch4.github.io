---
layout: post
title: "[파이썬] Airflow와 ETL 파이프라인 구성"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow는 데이터 파이프라인을 구축하고 관리하기 위한 오픈 소스 플랫폼입니다. Airflow를 사용하여 ETL(Extract, Transform, Load) 작업을 자동화하고 관리하는 것은 매우 유용합니다. 이 블로그 포스트에서는 Airflow를 사용하여 ETL 파이프라인을 구성하는 방법에 대해 알아보겠습니다.

## Airflow 설치

Airflow를 사용하기 위해서는 먼저 설치해야 합니다. Python 패키지 관리자인 pip를 사용하여 Airflow를 설치할 수 있습니다. 다음 명령을 실행하여 Airflow를 설치합니다.

```shell
pip install apache-airflow
```

Airflow의 기본적인 설치가 완료되었으면, 이제 다음 단계로 넘어갈 수 있습니다.

## ETL 파이프라인 작성

Airflow를 사용하여 ETL 파이프라인을 작성하려면 DAG(Directed Acyclic Graph)라는 개념을 이해해야 합니다. DAG는 작업(task) 간에 종속성(dependency) 관계를 나타내는 방향성 그래프입니다. 파이프라인에서 각 작업은 한 단계의 ETL 처리를 나타내며, DAG는 전체 파이프라인 흐름을 제어합니다.

아래는 Airflow를 사용하여 간단한 ETL 파이프라인을 작성하는 예제입니다. 이 예제는 주어진 데이터 소스에서 데이터를 추출하고, 변환 작업을 수행한 후 최종적으로 로드하는 간단한 파이프라인입니다.

```python
from datetime import timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

# DAG 설정
dag = DAG(
    'simple_etl_pipeline',
    description='A simple ETL pipeline example',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2022, 1, 1),
)

# 추출 작업
def extract_data():
    # 데이터 추출 로직 작성
    pass

# 변환 작업
def transform_data():
    # 데이터 변환 로직 작성
    pass

# 로드 작업
def load_data():
    # 데이터 로드 로직 작성
    pass

# 추출 작업을 수행하는 Task 설정
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag,
)

# 변환 작업을 수행하는 Task 설정
transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

# 로드 작업을 수행하는 Task 설정
load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)

# 작업 간의 종속성 설정
extract_task >> transform_task >> load_task
```

위의 코드는 DAG 생성, 추출 작업(Task), 변환 작업(Task), 로드 작업(Task)을 정의합니다. 각 작업은 Python 함수로 분리되어 있으며, 작업들 간의 종속성을 설정하여 순차적으로 실행되도록 구성됩니다.

아래는 Airflow UI에서 DAG 그래프를 시각적으로 확인할 수 있는 예시입니다.

![Airflow DAG Example](airflow_dag_example.png)

## 실행 및 스케줄링

ETL 파이프라인을 실행하려면 Airflow 스케줄러를 설정해야 합니다. Airflow 스케줄러는 작업의 실행 시간을 관리하고 종속성을 확인하여 작업을 자동으로 실행합니다.

Airflow 스케줄러는 다음 명령을 실행하여 실행할 수 있습니다.

```shell
airflow scheduler
```

또한 Airflow UI를 통해 파이프라인 실행 및 모니터링을 할 수도 있습니다.

## 결론

Airflow와 Python을 사용하여 ETL 파이프라인을 구축하는 방법에 대해 알아보았습니다. Airflow는 파이프라인의 흐름을 제어하고 실행 스케줄링을 관리하는 강력한 도구입니다. ETL 작업의 자동화에 Airflow를 사용하여 생산성을 향상시키고 시간을 절약할 수 있습니다. 이러한 이점을 통해 데이터 처리 과정을 효율적으로 관리하고 분석 작업을 용이하게 할 수 있습니다.
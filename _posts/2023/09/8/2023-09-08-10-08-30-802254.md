---
layout: post
title: "[파이썬] Airflow SLA 설정"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

In Apache Airflow, **Service Level Agreements (SLAs)** are used to define the maximum allowed duration for a task or a workflow to complete. By setting SLAs, you can track and monitor the performance of your workflows and identify any tasks that are not meeting their expected deadlines. 

To configure SLAs in Airflow, you can use the `sla` parameter when defining your tasks. This parameter specifies the maximum duration in seconds for a task to complete before it is marked as failed.

Here is an example of how to set an SLA for a task in a Python Airflow script:

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def my_task():
    # Task logic goes here
    pass

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2022, 1, 1),
    'sla': timedelta(minutes=30)  # Set SLA to 30 minutes
}

with DAG('my_dag', default_args=default_args, schedule_interval='@daily') as dag:
    task1 = PythonOperator(
        task_id='my_task',
        python_callable=my_task
    )
```

In the above example, we have created a DAG named "my_dag" and set the `sla` parameter in the `default_args` dictionary to `timedelta(minutes=30)`, which means that any task in this DAG should complete within 30 minutes. If a task exceeds this duration, it will be marked as failed and trigger any downstream tasks accordingly.

It's important to note that SLAs are applicable at the DAG level, but you can also set individual task level SLAs if needed. Additionally, you can configure Airflow to send notifications or trigger alerts whenever an SLA is breached, allowing you to take action in case of any performance issues.

By setting SLAs in your Airflow workflows, you can ensure that your tasks are completing within the expected timeframes and maintain performance standards for your data pipelines.
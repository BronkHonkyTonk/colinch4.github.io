---
layout: post
title: "[파이썬] boto3 AWS DataSync 데이터 전송 작업 설정"
description: " "
date: 2023-09-08
tags: [python,boto3]
comments: true
share: true
---

AWS DataSync is a service that allows you to move large amounts of data between on-premises storage and AWS storage services. It provides a simple and efficient way to transfer data that can be used for migration, disaster recovery, or data processing workflows. In this blog post, we will explore how to configure and initiate a data transfer task using the boto3 SDK in Python.

## Prerequisites

Before getting started, you will need the following:

- An AWS account
- Python 3.x installed on your machine
- Boto3 library installed (`pip install boto3`)

## Setting Up the AWS DataSync Client

To interact with the DataSync service using Python, we need to configure the AWS credentials and create a DataSync client using the `boto3` library.

```python
import boto3

# Configure AWS credentials
aws_access_key_id = 'YOUR_AWS_ACCESS_KEY_ID'
aws_secret_access_key = 'YOUR_AWS_SECRET_ACCESS_KEY'
aws_region = 'YOUR_AWS_REGION'

# Create DataSync client
datasync_client = boto3.client(
    'datasync',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
```

Make sure to replace `'YOUR_AWS_ACCESS_KEY_ID'`, `'YOUR_AWS_SECRET_ACCESS_KEY'`, and `'YOUR_AWS_REGION'` with your own AWS credentials and region.

## Creating a DataSync Task

To create a DataSync task, we need to specify the source and destination locations, as well as any additional options for the transfer.

```python
# Specify source and destination locations
source_location_arn = 'arn:aws:datasync:region:account-id:location/source-location'
destination_location_arn = 'arn:aws:datasync:region:account-id:location/destination-location'

# Specify task name
task_name = 'my-datasync-task'

# Specify any additional options for the task
options = {
    'OverrideOptions': {
        'VerifyMode': 'POINT_IN_TIME_CONSISTENT'
    }
}

# Create the DataSync task
response = datasync_client.create_task(
    SourceLocationArn=source_location_arn,
    DestinationLocationArn=destination_location_arn,
    Name=task_name,
    Options=options
)
```

In the above code, replace `'arn:aws:datasync:region:account-id:location/source-location'` and `'arn:aws:datasync:region:account-id:location/destination-location'` with the ARNs of your source and destination locations respectively. You can also specify additional options for the transfer, such as verifying the data's point-in-time consistency.

## Initiating a Data Transfer Task

Once the task is created, we can initiate the data transfer by running the task.

```python
# Get the task ARN (returned in the response of the create_task API call)
task_arn = response['TaskArn']

# Start the DataSync task
start_response = datasync_client.start_task_execution(
    TaskArn=task_arn
)

# Get the task execution ARN (returned in the response of the start_task_execution API call)
task_execution_arn = start_response['TaskExecutionArn']
```

After running the `start_task_execution` API call, you will get the `task_execution_arn` which represents the execution of the task. This can be used to monitor the status of the task or retrieve detailed information about the transfer.

## Conclusion

In this blog post, we explored how to configure and initiate a data transfer task using the boto3 SDK in Python. By leveraging the power of AWS DataSync, you can easily transfer large amounts of data between on-premises storage and AWS storage services. Feel free to explore the `boto3` documentation for more available options and functionalities to further customize your data transfer tasks.
---
layout: post
title: "[파이썬] Airflow와 Google Cloud Composer 활용"
description: " "
date: 2023-09-08
tags: [python,Airflow]
comments: true
share: true
---

Apache Airflow는 데이터 파이프라인을 작성하고 실행하기 위한 오픈 소스 플랫폼입니다. 이를 통해 사용자는 복잡한 데이터 처리 작업을 스케줄링하고 모니터링할 수 있습니다. Google Cloud Composer는 Google Cloud에서 호스팅되는 완전 관리형 Airflow 서비스입니다. 이 문서에서는 Airflow와 Google Cloud Composer를 사용하여 데이터 파이프라인을 구축하는 방법을 안내합니다.

## 1. Google Cloud Composer 생성

Google Cloud Console에 로그인한 후 Google Cloud Composer 페이지로 이동하여 새로운 Composer 환경을 생성합니다. 필요한 설정을 구성하고 완료 버튼을 클릭하여 Composer 환경을 생성합니다.

## 2. DAG 작성하기

DAG(Directed Acyclic Graph)는 Airflow에서 작업 흐름을 정의하는 코드입니다. DAG는 Python으로 작성되며, 실행 빈도, 작업 간의 종속성, 작업 병렬화 등을 정의할 수 있습니다.

아래는 예제 DAG의 코드입니다:

```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2022, 1, 1),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'example_dag',
    default_args=default_args,
    description='An example DAG',
    schedule_interval=timedelta(days=1),
)

t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

t2 = BashOperator(
    task_id='sleep',
    bash_command='sleep 5',
    retries=3,
    dag=dag,
)

t1 >> t2
```

위의 코드에서는 DAG를 생성하고 두 개의 작업(`t1`과 `t2`)을 정의합니다. `t1`은 현재 날짜를 출력하는 작업이고, `t2`는 5초간 대기하는 작업입니다. `t1` 다음에 `t2`가 실행되도록 설정되어 있습니다.

## 3. DAG 배포하기

DAG 코드를 작성한 후 Google Cloud Composer에 배포해야 합니다. 배포하기 위해서는 Cloud Storage에 DAG 코드를 업로드해야 합니다. Google Cloud Console에서 생성한 Composer 환경의 "DAGs 폴더"로 이동한 뒤, DAG 코드를 업로드합니다.

## 4. DAG 실행하기

DAG 코드를 배포한 후 Google Cloud Composer 대시보드에서 DAG 목록을 확인할 수 있습니다. DAG를 실행하려면 해당 DAG를 선택한 뒤 "Trigger DAG" 버튼을 클릭하면 됩니다. Airflow는 설정된 스케줄링에 따라 작업들을 실행하고 결과를 표시합니다.

## 5. 추가 기능 활용하기

Airflow와 Google Cloud Composer는 여러 가지 추가 기능을 제공합니다. 예를 들어, 서드 파티 라이브러리를 사용하여 작업을 구현하거나, 시각화 도구를 사용하여 작업 흐름을 모니터링할 수 있습니다. 또한, 에어플로우 큐를 사용하여 작업을 병렬로 실행하거나, 작업 간의 결과를 공유할 수도 있습니다.

## 결론

Airflow와 Google Cloud Composer를 통해 데이터 파이프라인을 구축하고 관리하는 것은 매우 간편합니다. Airflow를 사용하여 복잡한 작업 흐름을 정의하고 Google Cloud Composer를 사용하여 이를 관리하면 데이터 처리 작업의 자동화와 모니터링이 용이해집니다.
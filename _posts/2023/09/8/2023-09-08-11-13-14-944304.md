---
layout: post
title: "[파이썬] json 큰 JSON 파일 처리 전략"
description: " "
date: 2023-09-08
tags: [python,json]
comments: true
share: true
---

이번 글에서는 Python에서 큰 JSON 파일을 처리하는 전략에 대해 알아보겠습니다. JSON 파일은 비정형 데이터를 저장하는 데 유용한 파일 형식 중 하나입니다. 하지만 매우 큰 크기의 JSON 파일을 처리하려면 추가적인 작업과 최적화가 필요합니다.

## 1. 메모리 사용 최적화

큰 JSON 파일을 한 번에 전체 로드하면 메모리 부족 문제가 발생할 수 있습니다. 대신, 파일을 조금씩 나눠서 처리하는 방법을 고려해야 합니다. 예를 들어, 파일을 작은 청크로 나누어 각 청크를 메모리에 로드하고 필요한 부분만 처리한 뒤 메모리에서 제거하는 방식입니다. 이를 위해 `json` 라이브러리의 `loads()` 함수 대신 `load()` 함수를 사용하고, `jsonlines` 라이브러리를 이용하여 청크 단위로 파일을 읽을 수 있습니다.

```python
import jsonlines

def process_chunk(chunk):
    # 청크 처리 로직 구현

with jsonlines.open('large.json') as reader:
    for chunk in reader.iter():
        process_chunk(chunk)
```

## 2. 병렬 처리

병렬 처리는 큰 JSON 파일의 처리 속도를 향상시키는 데 도움이 될 수 있습니다. `multiprocessing` 라이브러리를 사용하여 병렬로 여러 청크를 동시에 처리할 수 있습니다. 각 프로세스는 독립적인 메모리 공간을 가지므로 메모리 부족 문제도 완화될 수 있습니다.

```python
import jsonlines
import multiprocessing

def process_chunk(chunk):
    # 청크 처리 로직 구현

def process_json_file(file_path):
    with jsonlines.open(file_path) as reader:
        for chunk in reader.iter():
            process_chunk(chunk)

if __name__ == '__main__':
    file_paths = ['large1.json', 'large2.json', 'large3.json']
    pool = multiprocessing.Pool()
    pool.map(process_json_file, file_paths)
    pool.close()
    pool.join()
```

## 3. 스트리밍 처리

존재하는 모든 JSON 객체를 한 번에 읽지 않고, 스트리밍 방식으로 처리하는 것도 고려해야 합니다. `ijson` 라이브러리를 사용하면 JSON을 이터레이터로 스트리밍하면서 필요한 부분만 추출할 수 있습니다.

```python
import ijson

def process_json_stream(file_path):
    with open(file_path, 'r') as file:
        parser = ijson.parse(file)
        for prefix, event, value in parser:
            # 필요한 JSON 객체 추출 및 처리

file_path = 'large.json'
process_json_stream(file_path)
```

## 마무리

Python에서 큰 JSON 파일을 효율적으로 처리하기 위해 메모리 사용 최적화, 병렬 처리, 스트리밍 처리 등의 전략을 적용할 수 있습니다. 이러한 전략을 활용하면 큰 JSON 파일을 효과적으로 처리하고 성능을 향상시킬 수 있습니다.
---
layout: post
title: "[python] 파이썬 BeautifulSoup과 웹 사이트 반응성 개선"
description: " "
date: 2023-12-04
tags: [python]
comments: true
share: true
---

웹 크롤링은 파이썬을 사용하는 개발자들 사이에서 매우 인기 있는 작업입니다. 파이썬의 `BeautifulSoup` 라이브러리는 웹 사이트의 HTML 코드를 쉽게 파싱하고 조작할 수 있는 강력한 도구입니다. 그러나 때로는 크롤링 작업이 오랜 시간이 걸리는 경우가 있고, 이로 인해 웹 사이트의 반응성이 저하될 수 있습니다. 이러한 문제를 해결하기 위해 우리는 몇 가지 기술과 팁을 살펴볼 것입니다.

## 1. 비동기 요청 사용하기

`BeautifulSoup`은 크롤링 작업을 수행하는 동안 웹 사이트의 응답을 기다려야하는데, 이는 작업이 오랜 시간이 걸리는 웹 페이지에서 문제가 될 수 있습니다. 비동기 요청을 사용하면 여러 작업을 동시에 처리할 수 있으므로 웹 사이트의 반응성을 개선할 수 있습니다. `aiohttp`와 같은 라이브러리를 사용하여 비동기 HTTP 요청을 보내고 `BeautifulSoup`와 함께 사용할 수 있습니다.

```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup

async def fetch_html(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

async def get_data(url):
    html = await fetch_html(url)
    soup = BeautifulSoup(html, 'html.parser')
    # 데이터를 추출하는 로직 작성
    # ...

async def main():
    urls = ['https://www.example.com', 'https://www.example2.com']
    tasks = [get_data(url) for url in urls]
    await asyncio.gather(*tasks)

if __name__ == '__main__':
    asyncio.run(main())
```

위의 예시에서, `fetch_html` 함수는 비동기적으로 HTML을 가져오고, `get_data` 함수는 `BeautifulSoup`를 사용하여 데이터를 추출합니다. `main` 함수에서는 여러 URL에 대해 비동기로 데이터를 추출하는 작업을 만들고 실행합니다.

## 2. 캐싱 사용하기

파이썬에서 웹 사이트를 크롤링할 때, 동일한 페이지를 여러번 요청하는 경우가 많습니다. 이러한 경우에는 캐싱을 사용하여 중복된 요청을 방지하고 성능을 향상시킬 수 있습니다. `requests-cache`와 같은 라이브러리를 사용하여 requests 요청을 캐싱할 수 있습니다.

```python
import requests_cache
from bs4 import BeautifulSoup

requests_cache.install_cache()

def get_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    # 데이터를 추출하는 로직 작성
    # ...
```

위의 예시에서, `requests_cache`를 사용하여 requests 요청을 캐싱합니다. 이렇게 하면 동일한 페이지에 대한 요청이 중복되지 않고 캐시된 응답을 사용하여 처리됩니다.

## 3. 멀티스레딩 사용하기

`BeautifulSoup`은 단일 스레드에서 동작하지만, 웹 크롤링 작업은 CPU를 많이 사용하지 않습니다. 따라서 멀티스레딩을 사용하여 작업을 동시에 처리할 수 있습니다. `concurrent.futures` 라이브러리를 사용하여 멀티스레딩을 구현할 수 있습니다.

```python
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

def get_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    # 데이터를 추출하는 로직 작성
    # ...

def main():
    urls = ['https://www.example.com', 'https://www.example2.com']
    with ThreadPoolExecutor() as executor:
        executor.map(get_data, urls)

if __name__ == '__main__':
    main()
```

위의 예시에서, `ThreadPoolExecutor`를 사용하여 멀티스레딩 작업을 생성하고 실행합니다. `get_data` 함수는 각각의 URL에 대해 병렬로 실행됩니다.

## 결론

`BeautifulSoup`과 함께 웹 크롤링 작업을 수행할 때 반응성을 개선하기 위해 위의 기술과 팁을 활용할 수 있습니다. 비동기 요청, 캐싱, 멀티스레딩을 사용하여 웹 사이트의 크롤링 작업을 더욱 효율적으로 처리할 수 있습니다. 이를 통해 웹 사이트의 사용자 경험을 향상시키고, 작업 시간을 단축시킬 수 있습니다.

참고: 
- [BeautifulSoup 공식 문서](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [aiohttp 공식 문서](https://docs.aiohttp.org/)
- [requests-cache 공식 문서](https://requests-cache.readthedocs.io/)
- [concurrent.futures 공식 문서](https://docs.python.org/3/library/concurrent.futures.html)
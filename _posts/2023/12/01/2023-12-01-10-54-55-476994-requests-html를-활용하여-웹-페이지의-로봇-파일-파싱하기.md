---
layout: post
title: "[python] Requests-HTML를 활용하여 웹 페이지의 로봇 파일 파싱하기"
description: " "
date: 2023-12-01
tags: [python]
comments: true
share: true
---

로봇 파일(Robots.txt)은 웹 사이트의 크롤러에게 어떤 페이지를 크롤링할 수 있는지 제어하는 텍스트 파일입니다. 이 파일은 대부분 웹 사이트의 루트 디렉토리에 위치하며, 크롤러가 해당 파일을 읽고 따라야 할 규칙을 확인합니다.

이번에는 Python의 Requests-HTML 라이브러리를 사용하여 웹 페이지의 로봇 파일을 파싱하는 방법을 알아보겠습니다.

## 1. Requests-HTML 설치하기

먼저, Requests-HTML 라이브러리를 설치해야 합니다. 아래의 명령을 사용하여 설치합니다.

```
pip install requests-html
```

## 2. 로봇 파일 가져오기

Requests-HTML를 사용하여 로봇 파일을 가져오는 방법은 간단합니다. 다음 예제 코드를 참고하세요.

```python
from requests_html import HTMLSession

# 웹 페이지의 URL
url = "http://www.example.com/robots.txt"

# HTMLSession 객체 생성
session = HTMLSession()

# 로봇 파일 가져오기
response = session.get(url)

# 가져온 로봇 파일 출력하기
print(response.text)
```

위 코드에서는 HTMLSession 객체를 사용하여 웹 페이지의 HTML을 가져옵니다. `get()` 메서드를 사용하여 웹 페이지의 URL을 전달하고, `response` 변수에 응답을 저장합니다.

## 3. 로봇 파일 파싱하기

로봇 파일을 가져오면, 이제 파싱하여 원하는 정보를 추출할 수 있습니다. 다음 예제 코드를 참고하세요.

```python
from urllib.parse import urlparse

# 웹 페이지의 URL
url = "http://www.example.com/robots.txt"

# URL에서 도메인 추출하기
domain = urlparse(url).netloc

# 로봇 파일 가져오기
response = session.get(url)

# 로봇 파일을 줄 단위로 나누기
lines = response.text.split("\n")

# 로봇 파일 파싱하기
for line in lines:
    if line.startswith("Disallow:"):
        path = line.replace("Disallow:", "").strip()
        print(f"{domain}{path}")
```

위 코드에서는 가져온 로봇 파일을 줄 단위로 나누고, `startwith()` 메서드를 사용하여 `Disallow:`로 시작하는 줄을 찾습니다. 해당 줄에서 `Disallow:`를 제거하고 앞뒤의 공백을 제거한 후, 도메인과 경로를 조합하여 출력합니다.

## 마무리

Python의 Requests-HTML 라이브러리를 사용하여 웹 페이지의 로봇 파일을 파싱하는 방법에 대해 알아보았습니다. 이를 통해 크롤러 개발시 로봇 파일을 해석하여 웹 사이트의 크롤링 규칙을 따르는 것이 가능합니다.

추가적인 자세한 내용은 다음 참고 자료를 참고하시기 바랍니다.

- [Requests-HTML 문서](https://requests-html.kennethreitz.org/)
- [Robots.txt 스펙 문서](https://www.robotstxt.org/robotstxt.html)
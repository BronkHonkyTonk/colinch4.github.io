---
layout: post
title: "[python] scikit-learn을 사용한 선형 판별 분석 (LDA)"
description: " "
date: 2023-12-01
tags: [python]
comments: true
share: true
---

scikit-learn은 파이썬에서 머신러닝과 데이터 분석을 위한 강력한 도구입니다. 이번 포스트에서는 scikit-learn을 사용하여 선형 판별 분석 (Linear Discriminant Analysis, LDA)을 수행하는 방법에 대해 알아보겠습니다.

## LDA란?

LDA는 특징 추출 및 차원 축소 기법 중 하나로, 분류 문제에서 유용하게 사용됩니다. LDA는 클래스 간의 분리를 극대화하며, 클래스 내의 분산을 최소화하는 방식으로 데이터를 변환합니다.

일반적으로 LDA는 다음과 같은 세 가지 단계로 이루어집니다:

1. 클래스 간의 분산과 클래스 내의 분산을 계산합니다.
2. 고유값 분해를 통해 계산된 공분산 행렬의 고유벡터를 구합니다.
3. 고유값에 따라 고유벡터를 정렬하고, 분류에 가장 중요한 주요 축을 선택합니다.

## scikit-learn을 사용한 LDA 구현

scikit-learn에서는 `sklearn.discriminant_analysis` 모듈을 통해 LDA를 구현할 수 있습니다. 다음은 간단한 예제 코드입니다.

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# 데이터 로드
X, y = load_data()

# LDA 객체 생성
lda = LinearDiscriminantAnalysis(n_components=2)

# LDA 수행
X_lda = lda.fit_transform(X, y)
```

위 코드에서 `load_data()` 함수는 사용자가 직접 정의해야 합니다. 데이터를 불러오고, 클래스 레이블을 가져오는 작업을 수행해야 합니다. 그 다음, `LinearDiscriminantAnalysis` 클래스 객체를 생성하고, `n_components` 매개변수를 통해 변환된 데이터의 차원을 설정합니다. 마지막으로 `fit_transform()` 함수를 호출하여 LDA를 수행하고 변환된 데이터를 얻을 수 있습니다.

## 예제

앞서 언급한 예제 코드를 이용하여 LDA를 사용하여 붓꽃 데이터셋을 분류해보겠습니다.

```python
from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import matplotlib.pyplot as plt

# 데이터 로드
iris = load_iris()
X = iris.data
y = iris.target

# LDA 객체 생성
lda = LinearDiscriminantAnalysis(n_components=2)

# LDA 수행
X_lda = lda.fit_transform(X, y)

# 시각화
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y)
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('LDA Visualization')
plt.show()
```

위 코드를 실행하면 LDA를 사용하여 붓꽃 데이터셋을 2차원으로 표현한 그래프가 나타납니다. 

## 결론

이번 포스트에서는 scikit-learn을 사용하여 선형 판별 분석 (LDA)를 수행하는 방법에 대해 알아보았습니다. LDA는 클래스 간의 분리를 극대화하고, 차원 축소를 수행하는데 유용한 기법입니다. scikit-learn을 사용하면 간편하게 LDA를 구현할 수 있으며, 다양한 분류 문제에 적용할 수 있습니다.

더 자세한 내용은 [scikit-learn의 LDA 문서](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)를 참조하십시오.
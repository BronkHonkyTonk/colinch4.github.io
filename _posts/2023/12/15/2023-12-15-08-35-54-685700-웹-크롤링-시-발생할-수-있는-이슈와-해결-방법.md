---
layout: post
title: "[python] 웹 크롤링 시 발생할 수 있는 이슈와 해결 방법"
description: " "
date: 2023-12-15
tags: [python]
comments: true
share: true
---

웹 크롤링은 다양한 목적으로 사용되는 기술로, 데이터를 수집하거나 분석하기 위해 웹 페이지를 자동으로 탐색하는 것을 말합니다. 하지만 웹 크롤링을 수행할 때 발생하는 다양한 이슈들이 있습니다. 이번 글에서는 웹 크롤링을 할 때 발생할 수 있는 이슈와 그에 대한 해결 방법에 대해 살펴보겠습니다.

## 1. 로봇 배제 표준 (Robots Exclusion Standard)
로봇 배제 표준은 웹 크롤러가 특정 웹 페이지를 방문하지 않기를 원하는 경우를 정의한 표준입니다. 웹 사이트들은 이를 이용하여 자신들의 사이트에 대한 접근을 제어할 수 있습니다. **로봇 배제 표준을 준수하지 않는 경우, 웹 사이트 소유자로부터 접근 차단을 당할 수 있습니다.** 이를 해결하기 위해서는 `robots.txt` 파일을 확인하고, 크롤링 대상 페이지의 권한을 확인해야 합니다.

## 2. IP 차단
웹 사이트에서는 여러 요청을 동시에 받아들이기 어렵기 때문에 과도한 요청을 보낸 IP 주소를 차단하는 경우가 있습니다. **웹 크롤링 시 과도한 요청을 보내는 경우 IP 차단을 당할 수 있습니다.** 이를 해결하기 위해서는 프록시 서버를 사용하거나, 요청 간겨를 조절하는 등의 방법을 사용할 수 있습니다.

## 3. 콘텐츠 구조 변경
웹 페이지의 구조가 변경되면 기존에 작성한 크롤링 스크립트는 동작하지 않을 수 있습니다. **웹 페이지의 구조 변경으로 크롤링이 실패하는 경우, 스크립트를 업데이트해야 합니다.** 정기적으로 스크립트를 검토하여 구조 변화에 대응할 수 있도록 유지 보수하는 것이 중요합니다.

## 4. 데이터 형식 변화
크롤링한 데이터의 형식이 변경될 수 있습니다. 웹 사이트가 업데이트되거나 새로운 데이터베이스가 도입될 경우 데이터 형식이 변할 수 있습니다. **크롤링한 데이터 형식이 변경된 경우, 스크립트를 수정하여 데이터를 정확히 파싱할 수 있도록 업데이트해야 합니다.**

## 마무리
웹 크롤링을 수행할 때 발생할 수 있는 다양한 이슈들을 해결하기 위해서는 문제를 미리 예측하고, 유연한 대처가 필요합니다. 또한, **웹 사이트의 이용 약관을 준수**하고, **규제를 준수하는 것이 매우 중요합니다.** 정당한 수집 목적과 합법적인 수집 방법을 사용하여 웹 크롤링을 수행해야 합니다.

이상으로 웹 크롤링 시 발생할 수 있는 이슈와 그에 대한 해결 방법에 대해 알아보았습니다.

## 참고 자료
- [robots.txt - Wikipedia](https://en.wikipedia.org/wiki/Robots_exclusion_standard)
- [웹 크롤링으로 인한 문제점과 대책 - velog.io](https://velog.io/@jeff0720/웹-크롤링으로-인한-문제점과-대책)
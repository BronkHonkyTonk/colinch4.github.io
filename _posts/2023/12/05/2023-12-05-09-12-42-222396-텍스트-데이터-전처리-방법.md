---
layout: post
title: "[python] 텍스트 데이터 전처리 방법"
description: " "
date: 2023-12-05
tags: [python]
comments: true
share: true
---

텍스트 데이터 전처리는 자연어 처리 작업에서 매우 중요한 단계입니다. 텍스트 데이터를 정제하고 구조화하여 모델에 입력으로 제공하는 것이 목표입니다. 이번 블로그 포스트에서는 Python을 사용하여 텍스트 데이터를 전처리하는 몇 가지 기본적인 방법을 알아보겠습니다.

## 1. 토큰화 (Tokenization)

토큰화는 텍스트 데이터를 작은 단위로 나누는 과정입니다. 예를 들어, 문장을 단어로 나누거나 단어를 문자로 나눌 수 있습니다. Python에서는 `nltk` 라이브러리를 통해 토큰화를 수행할 수 있습니다.

```python
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize, sent_tokenize

# 문장 토큰화
text = "토큰화는 텍스트 데이터를 작은 단위로 나누는 과정입니다. 예를 들어, 문장을 단어로 나누거나 단어를 문자로 나눌 수 있습니다."
sentences = sent_tokenize(text)
print(sentences)

# 단어 토큰화
tokens = word_tokenize(text)
print(tokens)
```

`sent_tokenize` 함수는 주어진 텍스트를 문장으로 나누어서 리스트로 반환하고, `word_tokenize` 함수는 주어진 텍스트를 단어로 나누어서 리스트로 반환합니다.

## 2. 정제 (Cleaning)

정제는 텍스트 데이터에서 불필요한 요소를 제거하는 과정입니다. 예를 들어, 특수 문자, 불용어 등을 제거하여 데이터를 깨끗하게 만들 수 있습니다. Python에서는 정규 표현식을 사용하여 정제를 수행할 수 있습니다.

```python
import re

# 특수 문자 제거
cleaned_text = re.sub('[^a-zA-Z0-9가-힣\s]', '', text)

# 불용어 제거
stopwords = ['은', '는', '이', '가', '을', '를'] # 불용어 예시
filtered_text = [word for word in tokens if word not in stopwords]
```

`re.sub` 함수를 사용하여 특정 패턴과 일치하는 문자열을 대체합니다. 예제에서는 알파벳, 숫자, 한글, 공백 문자를 제외한 모든 문자를 삭제하였습니다. 불용어 제거의 경우, 미리 정의된 불용어 리스트를 사용하여 단어를 걸러냅니다.

## 3. 정규화 (Normalization)

정규화는 텍스트 데이터를 동일한 형태로 변환하는 과정입니다. 예를 들어, 대소문자 통일, 어간 추출, 표제어 추출 등이 있습니다. Python에서는 `nltk` 라이브러리의 `PorterStemmer`와 `WordNetLemmatizer`를 사용하여 정규화를 수행할 수 있습니다.

```python
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import wordnet

# 어간 추출
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in tokens]

# 표제어 추출
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens]
```

`PorterStemmer`를 사용하여 단어의 어간을 추출할 수 있으며, `WordNetLemmatizer`를 사용하여 단어의 표제어를 추출할 수 있습니다. 표제어 추출의 경우, 단어의 품사를 지정하여 사용합니다.

## 4. 인코딩 (Encoding)

인코딩은 텍스트 데이터를 컴퓨터가 이해할 수 있는 형식으로 변환하는 과정입니다. 일반적으로 원핫 인코딩, TF-IDF 인코딩 등이 사용됩니다. Python에서는 `OneHotEncoder`와 `TfidfVectorizer`를 사용하여 인코딩을 수행할 수 있습니다.

```python
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

# 원핫 인코딩
onehot_encoder = OneHotEncoder()
onehot_encoded = onehot_encoder.fit_transform(tokens)

# TF-IDF 인코딩
tfidf_vectorizer = TfidfVectorizer()
tfidf_encoded = tfidf_vectorizer.fit_transform(tokens)
```

`OneHotEncoder`는 단어를 이진 벡터로 인코딩하며, `TfidfVectorizer`는 단어의 TF-IDF 값을 계산하여 벡터로 인코딩합니다.

## 결론

이상으로 Python을 사용하여 텍스트 데이터를 전처리하는 기본적인 방법을 알아보았습니다. 토큰화, 정제, 정규화, 인코딩의 단계를 차례로 수행하여 텍스트 데이터를 모델에 제공할 준비를 할 수 있습니다. 이러한 전처리 과정은 자연어 처리 모델의 성능 향상에 중요한 역할을 하므로 신경써서 처리해야 합니다.

참고문헌:
- Natural Language Toolkit (NLTK) Documentation: https://www.nltk.org/
- Python Regular Expression (re) Documentation: https://docs.python.org/3/library/re.html
- Scikit-learn Documentation: https://scikit-learn.org/
---
layout: post
title: "[go] 딥러닝에서 사용되는 활성화 함수에 대해 설명해주세요."
description: " "
date: 2023-12-21
tags: [go]
comments: true
share: true
---

활성화 함수 중에서 가장 많이 사용되는 함수 중 하나는 **시그모이드 함수**입니다. 이 함수는 비선형 변환을 가능하게 하며, 출력값을 0과 1 사이로 제한합니다. 하지만, 시그모이드 함수는 큰 값 혹은 작은 값에 대한 그래디언트가 0에 가까워져 그래디언트 소실 문제를 유발할 수 있습니다.

따라서, 시그모이드 함수의 대안으로 **렐루 함수(ReLU)**가 자주 사용됩니다. 이 함수는 음수 입력값에 대해 0을 출력하여 그래디언트 소실 문제를 해결합니다. 또한, 렐루 함수는 계산이 간단하여 학습 속도를 향상시키는 이점이 있습니다.

그 외에도 **하이퍼볼릭 탄젠트 함수(tanh)**나 **리키 렐루 함수(Leaky ReLU)** 등의 다양한 활성화 함수가 있으며, 각각의 함수는 다양한 상황에 적합한 성질을 가지고 있습니다.

더 자세한 내용은 [활성화 함수](https://ko.wikipedia.org/wiki/%ED%99%9C%EC%84%B1%ED%99%94_%ED%95%A8%EC%88%98)에 대해 참고할 수 있습니다.
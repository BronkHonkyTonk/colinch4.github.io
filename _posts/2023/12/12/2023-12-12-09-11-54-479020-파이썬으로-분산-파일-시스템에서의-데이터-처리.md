---
layout: post
title: "[python] 파이썬으로 분산 파일 시스템에서의 데이터 처리"
description: " "
date: 2023-12-12
tags: [python]
comments: true
share: true
---

분산 파일 시스템은 데이터를 여러 노드에 분산하여 저장하고 처리하는 시스템입니다. 파이썬은 강력한 프로그래밍 언어로 다양한 분산 파일 시스템과 통합되어 데이터 처리를 수행할 수 있습니다. 이번 글에서는 파이썬을 사용하여 Hadoop HDFS 및 AWS S3와 같은 분산 파일 시스템에서 데이터를 읽고 쓰는 방법에 대해 알아보겠습니다.

## 목차

1. Hadoop HDFS에서 데이터 처리하기
2. AWS S3에서 데이터 처리하기

## 1. Hadoop HDFS에서 데이터 처리하기

Hadoop은 대규모 데이터 세트를 처리하기 위한 분산 파일 시스템으로, HDFS(Hadoop Distributed File System)를 사용합니다. 파이썬에서는 `hdfs` 라이브러리를 사용하여 HDFS와 상호작용할 수 있습니다.

다음은 HDFS에서 데이터를 읽고 쓰는 간단한 예제 코드입니다.

```python
from hdfs import InsecureClient

# HDFS 클라이언트 생성
client = InsecureClient('http://hadoop-namenode:50070', user='hadoop')

# HDFS에 파일 쓰기
with client.write('/data/example.txt', encoding='utf-8') as writer:
    writer.write('Hello, HDFS!')

# HDFS에서 파일 읽기
with client.read('/data/example.txt', encoding='utf-8') as reader:
    data = reader.read()
    print(data)
```

## 2. AWS S3에서 데이터 처리하기

AWS S3는 안정적이고 확장 가능한 객체 스토리지 서비스이며, 파이썬에서는 `boto3` 라이브러리를 사용하여 S3와 상호작용할 수 있습니다.

다음은 AWS S3에서 데이터를 읽고 쓰는 간단한 예제 코드입니다.

```python
import boto3

# S3 클라이언트 생성
s3 = boto3.client('s3')

# S3에 파일 업로드
with open('example.txt', 'rb') as data:
    s3.upload_fileobj(data, 'my-bucket', 'example.txt')

# S3에서 파일 다운로드
with open('downloaded-example.txt', 'wb') as data:
    s3.download_fileobj('my-bucket', 'example.txt', data)
```

## 결론

파이썬을 사용하여 Hadoop HDFS 및 AWS S3와 같은 분산 파일 시스템에서 데이터를 처리하는 방법을 살펴보았습니다. 이러한 라이브러리와 기술을 사용하여 데이터를 읽고 쓰는 등의 작업을 효율적으로 수행할 수 있습니다.

## 참고 자료

- [hdfs Python Library - PyPI](https://pypi.org/project/hdfs/)
- [boto3 Documentation - AWS SDK for Python](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
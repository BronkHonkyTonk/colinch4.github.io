---
layout: post
title: "[python] 파이썬과 Hadoop을 이용한 분산 데이터 처리"
description: " "
date: 2023-12-12
tags: [python]
comments: true
share: true
---

본 블로그에서는 파이썬과 Hadoop을 이용하여 대용량 데이터를 분산 처리하는 방법에 대해 소개하고자 합니다.

## 목차
1. [Hadoop이란?](#hadoop이란)
2. [파이썬과 Hadoop 연동 방법](#파이썬과-hadoop-연동-방법)
3. [분산 데이터 처리 예시](#분산-데이터-처리-예시)
4. [결론](#결론)

## Hadoop이란?

**Hadoop**은 대량의 데이터를 분산 처리하기 위한 오픈 소스 프레임워크로, 대규모 데이터를 안정적으로 저장하고 빠르게 처리할 수 있습니다. Hadoop은 주로 **HDFS(Hadoop Distributed File System)**와 **MapReduce**라는 분산 처리 시스템으로 구성되어 있습니다.

## 파이썬과 Hadoop 연동 방법

파이썬과 Hadoop을 연동하여 데이터를 처리하려면, **Hadoop Streaming**을 사용할 수 있습니다. Hadoop Streaming은 사용자가 작성한 맵(Mapper)과 리듀스(Reducer) 프로그램을 Hadoop 클러스터에서 실행할 수 있도록 지원합니다. 이를 통해 파이썬 스크립트를 Hadoop에서 실행하여 대용량 데이터를 처리할 수 있습니다.

Hadoop Streaming을 사용하기 위해서는 다음과 같은 명령어를 사용합니다.

```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.X.X.jar -mapper mapper.py -reducer reducer.py -input input_data -output output_directory
```

## 분산 데이터 처리 예시

다음은 파이썬으로 작성한 간단한 매퍼와 리듀서 예시입니다.

### 매퍼 (mapper.py)

```python
#!/usr/bin/env python

import sys

for line in sys.stdin:
    words = line.strip().split()
    for word in words:
        print('%s\t%s' % (word, 1))
```

### 리듀서 (reducer.py)

```python
#!/usr/bin/env python

import sys

current_word = None
current_count = 0

for line in sys.stdin:
    word, count = line.strip().split('\t')
    count = int(count)

    if current_word == word:
        current_count += count
    else:
        if current_word:
            print('%s\t%s' % (current_word, current_count))
        current_word = word
        current_count = count

if current_word == word:
    print('%s\t%s' % (current_word, current_count))
```

이제 이 매퍼와 리듀서를 Hadoop 클러스터에서 실행하면, 대용량 데이터를 효율적으로 처리할 수 있습니다.

## 결론

이를 통해 파이썬과 Hadoop을 연동하여 대규모 데이터를 분산 처리하는 방법에 대해 알아보았습니다. 파이썬을 사용하여 Hadoop 클러스터에서 데이터를 처리하는 기술은 대용량 데이터 처리에 유용한 방법 중 하나입니다. Hadoop을 통해 데이터를 분산 처리함으로써 빠르고 효율적으로 대용량 데이터를 다룰 수 있습니다.

이상으로 파이썬과 Hadoop을 이용한 분산 데이터 처리에 대한 내용을 마치도록 하겠습니다.

감사합니다.

## References
- Hadoop Apache. (https://hadoop.apache.org/)
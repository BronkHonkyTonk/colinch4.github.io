---
layout: post
title: "[python] 하이퍼파라미터 튜닝"
description: " "
date: 2023-12-12
tags: [python]
comments: true
share: true
---

하이퍼파라미터 튜닝은 머신러닝 모델의 성능을 향상시키기 위한 중요한 단계입니다. 모델의 학습 과정 중에 조정할 수 있는 파라미터들을 말하며, 이를 최적화하여 모델의 성능을 극대화할 수 있습니다.

## 1. 하이퍼파라미터란?

머신러닝 모델은 학습을 통해 가중치 및 편향을 조정하여 데이터에 대한 최적의 예측을 수행합니다. 한편 하이퍼파라미터는 모델의 학습 과정을 제어하는데 사용되는 파라미터로, 학습률, 은닉층의 개수, 배치 사이즈 등이 여기에 해당합니다.

## 2. 그리드 서치(Grid Search)를 이용한 튜닝

가장 기본적인 방법으로 가능한 모든 조합을 대입하여 최적의 조합을 찾는 방법입니다. 예를 들어, 다양한 학습률과 은닉층의 개수에 대해 모든 조합을 시도하여 가장 성능이 좋은 조합을 선택합니다.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
param_grid = {'C': [1, 10, 100], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}
grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)
grid.fit(X_train, y_train)
print(grid.best_params_)
```

## 3. 랜덤 서치(Random Search)를 이용한 튜닝

랜덤 서치는 가능한 모든 조합을 대입하는 그리드 서치와 달리, 무작위로 선택한 조합을 시도하는 방법입니다. 이를 통해 더 많은 다양성을 갖는 하이퍼파라미터 조합을 찾을 수 있습니다.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
param_dist = {'C': uniform(1, 100), 'gamma': uniform(0.0001, 0.1), 'kernel': ['rbf']}
random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=10, refit=True, verbose=3)
random_search.fit(X_train, y_train)
print(random_search.best_params_)
```

## 4. 베이즈 최적화를 이용한 튜닝

베이즈 최적화는 기존 결과를 바탕으로 하이퍼파라미터 공간을 탐색하는 데 초점을 맞춘 방법입니다. 이를 통해 더 높은 성능을 얻을 수 있습니다.

```python
from sklearn.model_selection import BayesSearchCV
from skopt import BayesSearchCV
param_dist = {'C': (1, 100), 'gamma': (0.0001, 0.1), 'kernel': ['rbf']}
bayes_search = BayesSearchCV(SVC(), search_spaces=param_dist, n_iter=10, refit=True, verbose=3)
bayes_search.fit(X_train, y_train)
print(bayes_search.best_params_)
```

## 결론

하이퍼파라미터 튜닝은 모델의 성능을 극대화하는 중요한 단계이며, 그리드 서치, 랜덤 서치, 베이즈 최적화 등 다양한 방법을 통해 최적의 하이퍼파라미터 조합을 찾을 수 있습니다. 이를 통해 더 높은 정확도와 더 빠른 학습 속도를 얻을 수 있습니다.

원문: [Hyperparameter Tuning in Machine Learning](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models)
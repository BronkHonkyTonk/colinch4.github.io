---
layout: post
title: "[swift] Swift에서 카메라로 객체 인식하기"
description: " "
date: 2023-12-12
tags: [swift]
comments: true
share: true
---

오늘은 Swift를 사용하여 iOS 앱에서 카메라를 사용해 객체를 인식하는 방법을 살펴보겠습니다. Vision 프레임워크를 사용하여 실시간으로 사물을 감지하고 식별하는 기능을 구현할 수 있습니다.

## Vision 프레임워크 소개

Vision 프레임워크는 기기의 카메라 또는 사진에서 물체를 식별하고 분석하는 기능을 제공합니다. Core ML 모델을 사용하여 이미지 분석 작업을 실행할 수 있으며, 실시간으로 감지하기 위한 강력한 API를 제공합니다.

## 카메라 액세스 권한 요청

먼저, 카메라를 사용하기 위해 Info.plist에 카메라 액세스에 대한 권한을 요청해야 합니다. 아래 예제는 Info.plist 파일에 카메라 액세스 권한을 추가하는 방법을 보여줍니다.

```xml
<key>NSCameraUsageDescription</key>
<string>카메라 액세스 권한이 필요합니다.</string>
```

## 카메라로 실시간 객체 감지 구현

이제 카메라를 사용하여 실시간 객체 인식을 구현해보겠습니다. 아래는 실시간으로 카메라에서 객체를 감지하는 코드의 예시입니다.

```swift
import UIKit
import AVFoundation
import Vision

class ViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate {
    private var captureSession = AVCaptureSession()
    private var videoPreviewLayer: AVCaptureVideoPreviewLayer?
    
    override func viewDidLoad() {
        super.viewDidLoad()
        
        guard let captureDevice = AVCaptureDevice.default(for: .video) else { return }
        
        do {
            let input = try AVCaptureDeviceInput(device: captureDevice)
            captureSession.addInput(input)
            
            let output = AVCaptureVideoDataOutput()
            output.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
            captureSession.addOutput(output)
            captureSession.startRunning()
            
            videoPreviewLayer = AVCaptureVideoPreviewLayer(session: captureSession)
            videoPreviewLayer?.frame = view.layer.bounds
            videoPreviewLayer?.videoGravity = .resizeAspectFill
            view.layer.addSublayer(videoPreviewLayer!)
        } catch {
            print(error)
        }
    }
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer: CVPixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        do {
            let model = try VNCoreMLModel(for: YourCoreMLModel().model)
            let request = VNCoreMLRequest(model: model, completionHandler: { [weak self] request, error in
                // 처리 결과 핸들링
            })
            try VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:]).perform([request])
        } catch {
            print(error)
        }
    }
}
```

위 코드는 AVFoundation을 사용하여 카메라에서 영상을 캡처하고, Vision과 Core ML을 사용하여 객체를 감지하는 기능을 구현한 것입니다.

이제, 앱을 빌드하고 카메라를 통해 객체를 인식하고 분석하는 기능을 테스트할 수 있을 것입니다.

## 마무리

오늘은 Swift를 사용하여 카메라로 객체를 식별하는 방법에 대해 알아보았습니다. Vision 프레임워크와 Core ML을 결합하여 실시간 객체 감지와 분석 기능을 구현할 수 있습니다. 이러한 기술을 적용하여 멋진 iOS 애플리케이션을 개발해보세요!

참고 자료:
- [Apple Developer Documentation - Vision](https://developer.apple.com/documentation/vision)
- [Apple Developer Documentation - Core ML](https://developer.apple.com/documentation/coreml)
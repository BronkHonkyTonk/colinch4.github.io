---
layout: post
title: "[python] 파이썬 스파크로 로그 분석하기"
description: " "
date: 2023-12-07
tags: [python]
comments: true
share: true
---

## 목차
1. [소개](#소개)
2. [스파크 설치](#스파크-설치)
3. [로그 데이터 가져오기](#로그-데이터-가져오기)
4. [로그 분석하기](#로그-분석하기)
5. [결론](#결론)

## 소개
로그 분석은 대규모 데이터를 처리하고 가공하는데 유용한 기술입니다. 스파크는 파이썬에서 대용량 데이터를 분석하는데 효과적인 도구입니다. 이번 블로그에서는 파이썬 스파크를 사용하여 로그 데이터를 분석하는 방법에 대해 알아보겠습니다.

## 스파크 설치
먼저, 파이썬 스파크를 설치해야 합니다. 아래 명령을 터미널에서 실행하여 스파크를 설치합니다.

```python
pip install pyspark
```

## 로그 데이터 가져오기
로그 분석을 위해 샘플 로그 데이터를 가져와야 합니다. 예를 들어, `access.log`라는 이름의 로그 파일을 사용하겠습니다.

```python
from pyspark import SparkContext

# 스파크 컨텍스트 생성
sc = SparkContext("local", "Log Analysis")

# 로그 파일 로드
logs = sc.textFile("access.log")
```

## 로그 분석하기
이제 로그 데이터를 분석할 준비가 되었습니다. 가장 간단한 분석인 로그 라인 수 세기부터 시작해보겠습니다.

```python
# 로그 라인 수 세기
line_count = logs.count()

print("총 로그 라인 수:", line_count)
```

로그 데이터에는 IP 주소, 시간, 요청 URL 등 다양한 정보가 포함되어 있습니다. 이 중에서 특정 IP 주소의 로그 라인 수를 세는 것도 가능합니다.

```python
# 특정 IP 주소의 로그 라인 수 세기
ip_address = "123.45.67.89"
ip_count = logs.filter(lambda line: ip_address in line).count()

print("'{0}' IP 주소의 로그 라인 수: {1}".format(ip_address, ip_count))
```

로그 데이터를 분석하는 다른 방법으로는 HTTP 상태 코드의 분포를 확인하는 것이 있습니다. 예를 들어, 200 OK, 404 Not Found, 500 Internal Server Error 등의 상태 코드를 분석할 수 있습니다.

```python
# HTTP 상태 코드의 분포 확인
status_codes = logs.map(lambda line: line.split(" ")[-2])
status_count = status_codes.countByValue()

print("HTTP 상태 코드 분포:")
for code, count in status_count.items():
    print("{0}: {1} 번".format(code, count))
```

## 결론
이번 블로그에서는 파이썬 스파크를 사용하여 로그 데이터를 분석하는 방법을 알아보았습니다. 스파크는 대용량 데이터를 처리하고 가공하는데 효과적인 도구로 사용할 수 있습니다. 이를 통해 로그 분석 작업을 효율적으로 수행할 수 있습니다.

참고: [Apache Spark 공식 홈페이지](https://spark.apache.org/)
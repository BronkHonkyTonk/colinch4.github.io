---
layout: post
title: "[python] 파이썬 스파크 프로그래밍에서의 작업 스케줄링"
description: " "
date: 2023-12-07
tags: [python]
comments: true
share: true
---

스파크(Spark)는 대용량 데이터 처리를 위한 오픈 소스 클러스터 컴퓨팅 프레임워크이며, 파이썬을 통해 스파크 작업을 프로그래밍할 수 있습니다. 스파크에서는 작업 스케줄링을 통해 작업을 효율적으로 실행할 수 있습니다. 이번 글에서는 파이썬 스파크 프로그래밍에서 작업 스케줄링에 대해 알아보도록 하겠습니다.

## 작업 스케줄링이란?

작업 스케줄링은 스파크에서 작업을 어떻게 실행할지 결정하는 과정입니다. 스파크는 효율적으로 작업을 처리하기 위해 작업 스케줄링 기법을 사용합니다. 스파크의 작업 스케줄링은 다음과 같은 단계로 이루어집니다.

1. 작업 분할(Divide): 입력 데이터를 작은 조각으로 나누어 작업하기 쉽게 분할합니다.
2. 작업 계획(Plan): 각 작업에 대한 실행 계획을 수립합니다. 이때 스파크의 실행 계획은 유향 그래프로 표현됩니다.
3. 작업 스케줄링(Schedule): 작업들을 어떤 순서로 실행할지 결정합니다. 스파크에서는 DAG(Direct Acyclic Graph) 스케줄링 알고리즘을 사용합니다.

## 스파크 작업 스케줄링 알고리즘

### FIFO 스케줄링

FIFO(First In First Out) 스케줄링은 작업을 도착한 순서대로 실행하는 알고리즘입니다. 스파크의 기본 스케줄링 알고리즘이며, 간단하고 직관적인 방식으로 작업을 실행합니다. 하지만 시스템 부하가 있는 경우 성능에 영향을 줄 수 있습니다.

```python
from pyspark import SparkConf, SparkContext

# 스파크 설정
conf = SparkConf().setAppName("FIFOSchedulingExample")
sc = SparkContext(conf=conf)

# 작업 생성
rdd = sc.parallelize(range(1000))
result = rdd.map(lambda x: x * 2).collect()

# 결과 출력
print(result)

# 스파크 종료
sc.stop()
```

### 우선순위 스케줄링

우선순위 스케줄링은 작업마다 우선순위를 지정하여 실행하는 알고리즘입니다. 각 작업에 우선순위를 매기고, 우선순위가 높은 작업부터 실행됩니다. 우선순위 스케줄링은 작업간의 선후관계가 없는 경우에 사용됩니다. 스파크에서는 작업 우선순위를 설정하기 위해 `spark.job.priority` 매개변수를 사용할 수 있습니다.

```python
from pyspark import SparkConf, SparkContext

# 스파크 설정
conf = SparkConf().setAppName("PrioritySchedulingExample")
conf.set("spark.job.priority", "HIGH")
sc = SparkContext(conf=conf)

# 작업 생성
rdd = sc.parallelize(range(1000))
result = rdd.map(lambda x: x * 2).collect()

# 결과 출력
print(result)

# 스파크 종료
sc.stop()
```

## 결론

작업 스케줄링은 스파크에서 작업을 효율적으로 실행하기 위해 중요한 단계입니다. 스파크는 기본적으로 FIFO 스케줄링 알고리즘을 사용하며, 우선순위 스케줄링을 통해 작업마다 우선순위를 지정할 수도 있습니다. 올바른 작업 스케줄링을 통해 스파크 작업의 성능을 향상시킬 수 있습니다.

## 참고 자료

- [스파크 스케줄링](https://spark.apache.org/docs/latest/job-scheduling.html)
- [스파크 우선순위 설정](https://spark.apache.org/docs/latest/configuration.html#scheduling)
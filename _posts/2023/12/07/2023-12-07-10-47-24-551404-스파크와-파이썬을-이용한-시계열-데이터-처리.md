---
layout: post
title: "[python] 스파크와 파이썬을 이용한 시계열 데이터 처리"
description: " "
date: 2023-12-07
tags: [python]
comments: true
share: true
---

## 소개
스파크(Spark)는 대규모 데이터 처리를 위한 오픈소스 클러스터 컴퓨팅 프레임워크입니다. 파이썬은 가독성이 좋고 직관적인 문법으로 인해 데이터 처리 작업에 많이 사용됩니다. 이번 블로그 포스트에서는 스파크와 파이썬을 함께 사용하여 시계열 데이터를 처리하는 방법에 대해 알아보겠습니다.

## 시계열 데이터란?
시계열 데이터는 일정한 시간 간격으로 측정된 데이터의 연속적인 시퀀스를 말합니다. 주식 시장 데이터, 기상 데이터, 주가 데이터 등은 모두 시계열 데이터의 예입니다. 시계열 데이터는 다양한 분석과 예측 작업에 사용되는데, 대량의 데이터를 실시간으로 처리해야 하므로 스파크와 파이썬 조합은 이를 위한 좋은 선택입니다.

## pandas와 spark의 차이점
파이썬에서 시계열 데이터 처리를 위해 주로 사용되는 라이브러리인 pandas와 스파크의 차이점을 알아보겠습니다.

- pandas는 단일 머신에서 작동하는 데이터 분석 도구로, 작은 규모의 데이터 세트에 적합합니다.
- 스파크는 분산 컴퓨팅에 최적화된 프레임워크로, 대규모 데이터 세트에 적합합니다.

## 스파크를 사용하여 시계열 데이터 처리하기
스파크에서 시계열 데이터를 처리하기 위해 먼저 spark 패키지를 설치해야 합니다. Spark의 shell 모드에서 다음과 같이 입력하여 패키지를 설치할 수 있습니다.

```
$SPARK_HOME/bin/pyspark --packages com.databricks:spark-csv_2.11:1.5.0
```

먼저, 시계열 데이터를 로드하기 위해 pandas의 DataFrame을 사용하여 데이터를 읽어옵니다.

```python
import pandas as pd

df = pd.read_csv('data.csv')
```

다음으로, Spark DataFrame으로 변환하기 위해 pandas DataFrame을 스파크의 DataFrame으로 변환해야 합니다.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
spark_df = spark.createDataFrame(df)
```

이제 스파크의 DataFrame을 사용하여 시계열 데이터를 처리할 수 있습니다.

## 시계열 데이터 처리 예시
간단한 예시를 통해 시계열 데이터 처리를 실습해보겠습니다. 주식 데이터를 사용하여 매일의 종가를 예측하는 예제입니다.

```python
from pyspark.sql.functions import lag

# 이전 날짜의 종가와 현재 날짜의 종가의 차이 계산
spark_df = spark_df.withColumn("lag_close", lag(spark_df["close"]).over(Window.orderBy("date")))
spark_df = spark_df.withColumn("close_diff", spark_df["close"] - spark_df["lag_close"])

# 차이가 양수인 경우 1로 레이블링, 음수인 경우 0으로 레이블링
spark_df = spark_df.withColumn("label", spark_df["close_diff"].apply(lambda x: 1 if x > 0 else 0))

# 데이터셋 분할
train_data, test_data = spark_df.randomSplit([0.8, 0.2])

# 예측 모델 훈련
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=['close_diff'], outputCol='features')
train_data = assembler.transform(train_data)

lr = LogisticRegression(featuresCol='features', labelCol='label')
lr_model = lr.fit(train_data)

# 예측 결과 확인
test_data = assembler.transform(test_data)
predictions = lr_model.transform(test_data)

predictions.show()
```

이 예제에서는 이전 날짜의 종가와 현재 날짜의 종가의 차이를 계산하고, 차이가 양수인 경우 1로 레이블링하고 음수인 경우 0으로 레이블링합니다. 그 후 데이터를 훈련 세트와 테스트 세트로 분할하고 로지스틱 회귀 모델을 훈련시켜 예측 결과를 확인합니다.

## 결론
스파크와 파이썬을 함께 사용하면 대규모의 시계열 데이터를 효율적으로 처리할 수 있습니다. 스파크를 사용하여 데이터를 분산처리하고, 파이썬을 사용하여 데이터를 간편하게 다룰 수 있습니다. 이를 통해 시계열 데이터 처리 작업을 더욱 효율적으로 수행할 수 있습니다.

## References
- [Apache Spark](https://spark.apache.org/)
- [pandas](https://pandas.pydata.org/)
- [Spark MLlib](https://spark.apache.org/mllib/)
- [pyspark.sql](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)
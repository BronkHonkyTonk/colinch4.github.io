---
layout: post
title: "[python] 모델 평가하기"
description: " "
date: 2023-12-11
tags: [python]
comments: true
share: true
---

모델 훈련 후에는 모델이 예측을 얼마나 정확하게 수행하는지를 평가해야 합니다. 모델 평가는 모델이 훈련된 데이터 이외의 데이터에서 얼마나 잘 작동하는지를 확인하는 과정입니다. 이 글에서는 다양한 모델 평가 기준과 방법에 대해 알아보겠습니다.

## 평가 지표 선택

모델을 평가하기 위해 선택할 수 있는 다양한 지표가 있습니다. 이진 분류 문제의 경우 정확도, 정밀도, 재현율, F1 점수, ROC 곡선의 AUC 등이 널리 사용됩니다. 다중 클래스 분류의 경우에는 이러한 지표들을 클래스별로 평균을 내어 사용할 수 있습니다. 회귀 문제의 경우에는 평균 제곱 오차(MSE)나 결정 계수(R-squared) 등을 사용합니다.

## 교차 검증

교차 검증은 모델의 일반화 성능을 더 정확하게 예측하기 위해 사용됩니다. 주로 k-fold cross-validation이 많이 사용되며 scikit-learn과 같은 머신 러닝 라이브러리에서 쉽게 사용할 수 있습니다.

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
```

## 과적합 확인

모델이 훈련 데이터에 너무 맞춰져 있어 새로운 데이터에서 성능이 좋지 않을 수 있는 과적합을 확인해야 합니다. 이를 위해 검증 데이터셋을 사용하거나 교차 검증을 수행할 수 있습니다.

## 하이퍼파라미터 튜닝

모델을 평가하고 나면 모델의 성능을 더 높이기 위해 하이퍼파라미터를 튜닝할 필요가 있습니다. 그리드 서치와 같은 기법을 사용하여 최적의 하이퍼파라미터 조합을 찾을 수 있습니다.

## 요약

모델을 평가하는 것은 모델의 성능을 이해하고 개선하기 위해 매우 중요한 단계입니다. 적절한 평가 지표를 선택하고 교차 검증을 통해 모델의 일반화 성능을 파악하며, 과적합을 확인하고 하이퍼파라미터를 튜닝하여 모델의 성능을 향상시킬 수 있습니다.

더 많은 정보는 [Scikit-learn documentation](https://scikit-learn.org/stable/documentation.html)를 참고하세요.

## 참조

- https://scikit-learn.org/stable/documentation.html
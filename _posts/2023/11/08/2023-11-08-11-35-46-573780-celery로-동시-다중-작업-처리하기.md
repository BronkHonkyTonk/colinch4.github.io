---
layout: post
title: "Celery로 동시 다중 작업 처리하기"
description: " "
date: 2023-11-08
tags: []
comments: true
share: true
---

지금까지 단일 작업만을 처리하던 경우에는 문제가 없지만, 대부분의 시스템에서는 동시에 여러 작업을 처리해야 하는 경우가 많습니다. 이러한 상황에서 Celery를 사용하면 간편하게 동시 다중 작업을 처리할 수 있습니다.

Celery는 파이썬으로 작성된 분산 작업 큐 시스템입니다. 메시지 브로커를 통해 작업을 큐에 넣고, 작업자(worker)가 큐에서 작업을 가져와 처리하는 방식으로 동작합니다. 이를 통해 애플리케이션에서 비동기적으로 작업을 처리할 수 있습니다.

## Celery 설정

Celery를 사용하기 위해서는 몇 가지 설정이 필요합니다. 먼저, Celery 패키지를 설치해야 합니다. pip를 사용하여 설치할 수 있습니다.

```python
pip install celery
```

그리고 Celery 작업자(worker)를 실행하기 위해 브로커(Broker)를 설정해야 합니다. Celery에서는 Redis, RabbitMQ, SQS 등 다양한 브로커를 지원합니다. 이 예제에서는 Redis를 사용해보겠습니다. Redis도 pip를 사용하여 설치할 수 있습니다.

```python
pip install redis
```

Celery 설정 파일을 만들어야 합니다. 예를 들어 `celeryconfig.py` 파일을 생성하고 다음과 같이 작성할 수 있습니다.

```python
broker_url = 'redis://localhost:6379/0'
result_backend = 'redis://localhost:6379/0'
```

이제 Celery를 사용할 준비가 되었습니다.

## 작업 정의하기

다음으로 작업을 정의해야 합니다. 작업은 `@task` 데코레이터를 이용해 정의할 수 있습니다. `@task` 데코레이터 아래에 작업을 처리하는 함수를 작성합니다.

```python
from celery import Celery

app = Celery('myapp', broker='redis://localhost:6379/0')

@app.task
def process_task(param):
    # 작업 처리 로직
    return result
```

위의 코드에서 `process_task` 함수는 작업 처리를 담당하는 함수입니다. `param`은 작업에 필요한 매개변수입니다. 작업 처리 결과는 `return` 문으로 반환할 수 있습니다.

## 작업 실행하기

작업을 실행하기 위해서는 Celery 작업자(worker)를 실행해야 합니다. 작업자는 다음과 같이 명령행에서 실행할 수 있습니다.

```python
celery -A myapp worker --loglevel=info
```

`-A myapp` 옵션은 `myapp`이라는 Celery 애플리케이션을 사용한다는 것을 지정합니다. `--loglevel=info` 옵션은 로그 레벨을 설정하는 것입니다.

작업을 큐에 넣고 실행하기 위해서는 다음과 같이 코드를 작성할 수 있습니다.

```python
from myapp import process_task

result = process_task.delay(param)
```

`delay` 메서드를 호출하면 작업이 큐에 넣어집니다. 작업의 결과는 `result.get()` 메서드를 사용하여 얻을 수 있습니다.

## 마무리

이제 Celery로 동시 다중 작업을 처리하는 방법에 대해 알아보았습니다. Celery를 사용하면 비동기적으로 작업을 처리할 수 있으며, 작업자(worker)를 쉽게 실행시킬 수 있습니다. Celery의 다양한 설정 옵션과 기능을 활용하여 애플리케이션에 맞게 작업 처리를 구성할 수 있습니다.

#celery #동시작업
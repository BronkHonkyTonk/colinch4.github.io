---
layout: post
title: "Celery로 분산 처리하기"
description: " "
date: 2023-11-08
tags: [Celery]
comments: true
share: true
---

컴퓨터 시스템에서 많은 작업을 동시에 처리해야 할 때, 분산 처리는 매우 유용한 방법입니다. 하나의 서버에서 작업을 처리하는 것보다 여러 대의 서버에 작업을 분산시키는 것이 성능과 확장성을 향상시킬 수 있습니다. 

이번 포스트에서는 분산 처리를 위해 Celery를 사용하는 방법에 대해 알아보겠습니다. Celery는 Python 기반의 분산 작업 큐이며, 간단하고 강력한 인터페이스를 제공합니다. 

## Celery 설치하기

먼저, Celery를 설치하기 위해 다음 명령을 실행해주세요:

```bash
pip install celery
```

## Celery 설정하기

Celery를 사용하기 위해서는 설정 파일을 만들어야 합니다. 설정 파일은 Celery 작업자(worker)들에게 필요한 정보를 제공합니다. 일반적으로 `celery.py` 또는 `celery_config.py`와 같은 이름으로 설정 파일을 작성합니다.

```python
# celery_config.py

from celery import Celery

app = Celery('myapp', broker='redis://localhost:6379/0')

app.config_from_object('myapp.celeryconfig')
```

위의 예시에서는 Redis를 Celery 브로커로 사용하고 있습니다. 필요한 경우 다른 브로커를 사용할 수도 있습니다. 

## 작업 정의하기

Celery에서 작업을 정의하기 위해서는 `@app.task` 데코레이터를 사용합니다. 작업은 독립적으로 실행되어야 할 함수입니다.

```python
# tasks.py

from celery import Celery

app = Celery('myapp', broker='redis://localhost:6379/0')

@app.task
def add(x, y):
    return x + y
```

위의 예시에서 `add` 함수는 두 개의 인자를 받아서 더한 결과를 반환합니다. 이 함수는 분산 처리될 수 있도록 Celery 작업으로 선언되었습니다.

## 작업 실행하기

Celery 작업을 실행하기 위해서는 작업자(worker)를 시작해야 합니다. 다음 명령으로 작업자를 시작할 수 있습니다:

```bash
celery -A myapp worker --loglevel=info
```

위의 명령에서 `-A` 옵션은 Celery 설정 파일을 지정합니다. `--loglevel` 옵션은 로그 레벨을 설정합니다.

작업을 예약하려면 `apply_async` 메서드를 사용합니다:

```python
from myapp.tasks import add

result = add.apply_async((4, 8))
```

위의 예시에서 `add` 작업에 인자로 (4, 8)을 전달하고, `apply_async` 메서드를 사용하여 비동기로 작업을 예약합니다.

이외에도 Celery는 큐, 태스크 상태 조회 등 다양한 기능을 제공합니다. 자세한 내용은 [Celery 문서](http://docs.celeryproject.org/en/latest/)를 참고해주세요.

#celery #분산처리
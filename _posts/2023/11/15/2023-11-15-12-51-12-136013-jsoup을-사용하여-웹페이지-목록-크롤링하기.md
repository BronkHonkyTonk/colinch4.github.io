---
layout: post
title: "[java] Jsoup을 사용하여 웹페이지 목록 크롤링하기"
description: " "
date: 2023-11-15
tags: [java]
comments: true
share: true
---

Java에서 웹 스크래핑을 할 때 Jsoup은 매우 유용한 툴이다. Jsoup은 HTML 문서를 가져오고 파싱하는 기능을 제공하여 웹 페이지의 내용을 추출하거나 조작하는 데 사용된다. 이번 블로그 포스트에서는 Jsoup을 사용하여 웹페이지 목록을 크롤링하는 방법을 알아보겠다.

## Jsoup 라이브러리 추가하기

먼저, Maven, Gradle 또는 직접 Jsoup JAR 파일을 다운로드하여 프로젝트에 추가해야 한다. Maven을 사용하는 경우 `pom.xml` 파일에 다음 종속성을 추가한다.

```xml
<dependency>
    <groupId>org.jsoup</groupId>
    <artifactId>jsoup</artifactId>
    <version>1.14.1</version>
</dependency>
```

Gradle을 사용하는 경우 `build.gradle` 파일에 다음 종속성을 추가한다.

```groovy
implementation 'org.jsoup:jsoup:1.14.1'
```

## 웹페이지 목록 크롤링하기

이제 Jsoup을 사용하여 웹페이지 목록을 크롤링해보자.

```java
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

public class WebPageCrawler {
    public static void main(String[] args) {
        try {
            // 크롤링할 웹페이지 URL
            String url = "http://example.com";

            // Jsoup을 사용하여 웹페이지 HTML 가져오기
            Document doc = Jsoup.connect(url).get();

            // 웹페이지에서 원하는 요소 선택하기
            Elements links = doc.select("a");

            // 선택한 요소들 순회하면서 출력하기
            for (Element link : links) {
                System.out.println(link.attr("href"));
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

위의 예제에서는 `http://example.com` 웹페이지의 모든 링크를 가져와서 출력한다. `Jsoup.connect(url).get()`를 사용하여 웹페이지의 HTML을 가져오고, `doc.select("a")`를 사용하여 웹페이지의 모든 링크 요소를 선택한다. 선택한 요소들을 순회하면서 각 링크의 `href` 속성을 출력한다.

## 결론

Jsoup은 Java 개발자들에게 웹 스크래핑을 쉽게 할 수 있는 강력한 도구이다. 이 블로그 포스트에서는 Jsoup을 사용하여 웹페이지 목록을 크롤링하는 방법을 간단히 살펴보았다. Jsoup을 사용하여 다양한 웹 스크래핑 작업을 수행할 수 있으므로, 필요한 경우 공식 문서와 예제 코드를 참고하는 것이 좋다.

## 참고 자료

- [Jsoup 공식 웹사이트](https://jsoup.org/)
- [Jsoup GitHub 저장소](https://github.com/jhy/jsoup)
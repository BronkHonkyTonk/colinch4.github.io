---
layout: post
title: "[python] 파이썬 PyTorch에서 학습 중 모델의 학습 속도를 동적으로 조절하는 방법은?"
description: " "
date: 2023-11-22
tags: [python]
comments: true
share: true
---

PyTorch는 딥 러닝 모델을 학습시키는 과정에서 학습 속도를 조절하는 다양한 방법을 제공합니다. 이 가이드에서는 PyTorch에서 모델의 학습 속도를 동적으로 조절하는 몇 가지 방법을 알아보겠습니다.

## 1. Learning Rate Scheduler 사용하기
PyTorch에는 `torch.optim.lr_scheduler`라는 모듈을 사용하여 학습률을 동적으로 스케줄링할 수 있습니다. 이 모듈을 사용하면 학습률을 에포크나 스텝마다 자동으로 조절할 수 있습니다.

```python
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

# 모델 정의
model = MyModel()

# 학습률 스케줄러 정의
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# 에포크 수 만큼 반복
for epoch in range(num_epochs):
    # 훈련 코드
    # ...
    
    # 학습률 업데이트
    scheduler.step()
```

위의 예제에서 `StepLR` 스케줄러는 주어진 `step_size`마다 학습률을 `gamma`배로 감소시킵니다. 따라서 위의 예제에서는 10번째 에포크마다 학습률이 0.1배씩 줄어들게 됩니다.

다른 종류의 스케줄러들도 사용 가능하며, 자세한 내용은 [공식 문서](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)를 참조하세요.

## 2. 학습률 사용자 정의하기
스케줄러를 사용하는 대신 직접 학습률을 조절하는 방법도 있습니다. 이 방법은 모델의 특정 조건에 따라 학습률을 동적으로 업데이트 할 수 있는 장점이 있습니다.

```python
import torch
import torch.optim as optim

# 모델 정의
model = MyModel()

# 옵티마이저 정의
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 에포크 수 만큼 반복
for epoch in range(num_epochs):
    # 훈련 코드
    # ...
    
    # 학습률 업데이트
    if epoch % 10 == 0:
        for param_group in optimizer.param_groups:
            param_group['lr'] *= 0.1
```

위의 예제에서는 에포크가 10의 배수일 때마다 학습률을 0.1배로 줄이는 방식입니다. 이렇게 직접 학습률을 업데이트하는 방법은 모델의 특정 조건에 따라 동적으로 학습률을 조절하는 데 유용합니다.

이 외에도 PyTorch에서는 다양한 학습률 스케줄링 기법을 사용할 수 있습니다. 

참조:
- [PyTorch 공식 문서 - torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)
- [PyTorch 공식 문서 - torch.optim](https://pytorch.org/docs/stable/optim.html)
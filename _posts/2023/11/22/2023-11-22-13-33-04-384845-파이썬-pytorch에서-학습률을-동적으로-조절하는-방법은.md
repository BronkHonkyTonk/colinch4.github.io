---
layout: post
title: "[python] 파이썬 PyTorch에서 학습률을 동적으로 조절하는 방법은?"
description: " "
date: 2023-11-22
tags: [python]
comments: true
share: true
---

딥 러닝 모델을 학습할 때 올바른 학습률을 선택하는 것은 매우 중요합니다. 학습률이 너무 작으면 모델이 수렴하는 데 오랜 시간이 걸리거나 지역 최소값에 빠질 수 있고, 학습률이 너무 크면 일정 수준 이상에서는 모델의 성능이 개선되지 않거나 발산할 수 있습니다.

PyTorch는 학습률을 동적으로 조절하는 여러 가지 방법을 제공합니다. 그 중 몇 가지 방법을 살펴보겠습니다.

## 1. 학습률 스케줄러 사용하기

PyTorch에서는 `torch.optim.lr_scheduler` 모듈을 사용하여 학습률을 스케줄링할 수 있습니다. 다양한 스케줄러를 제공하며, 가장 간단한 방법은 StepLR 스케줄러를 사용하는 것입니다.

StepLR 스케줄러는 주어진 epoch 수마다 학습률을 주어진 비율로 감소시킵니다. 예를 들어, 각 10개의 epoch마다 학습률을 0.1배씩 축소하려면 다음과 같이 코드를 작성할 수 있습니다.

```python
import torch
from torch.optim.lr_scheduler import StepLR

# optimizer 초기화 및 StepLR 스케줄러 생성
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# 학습 루프 내에서 스케줄러 업데이트
for epoch in range(num_epochs):
    train(...)
    validate(...)
    scheduler.step()  # 스케줄러에게 현재 epoch를 알려줌
```

이렇게 하면 각각 10개의 epoch마다 학습률이 0.1배씩 줄어들게 됩니다.

## 2. ReduceLROnPlateau 스케줄러 사용하기

학습 중에 모델의 성능이 좋아지지 않거나 개선이 멈추면 학습률을 조절해야 할 때가 있습니다. 이러한 경우에는 ReduceLROnPlateau 스케줄러를 사용할 수 있습니다.

ReduceLROnPlateau 스케줄러는 주어진 metric의 개선이 멈추면 학습률을 동적으로 조절합니다. 아래의 코드 예제와 같이 사용할 수 있습니다.

```python
import torch
from torch.optim.lr_scheduler import ReduceLROnPlateau

# optimizer 초기화 및 ReduceLROnPlateau 스케줄러 생성
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5)

# 학습 루프 내에서 스케줄러 업데이트
for epoch in range(num_epochs):
    train(...)
    loss = validate(...)
    scheduler.step(loss)  # 스케줄러에게 현재 metric 값을 알려줌
```

이러한 방식으로 ReduceLROnPlateau 스케줄러는 주어진 metric의 개선이 멈추면 학습률을 감소시키는 동적 조절을 수행합니다.

## 3. 직접 학습률 업데이트하기

학습률을 직접 업데이트하는 방법도 있습니다. 이 방법은 특정 조건에 따라 학습률을 동적으로 조절해야 할 때 유용합니다. 예를 들어, 특정 epoch에서 학습률을 줄이는 경우에는 다음과 같이 코드를 작성할 수 있습니다.

```python
import torch

# optimizer 초기화
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# 학습 루프 내에서 직접 학습률 업데이트
for epoch in range(num_epochs):
    train(...)
    validate(...)
    
    if epoch % 10 == 0:
        for param_group in optimizer.param_groups:
            param_group['lr'] *= 0.1  # 학습률을 0.1배로 줄임
```

이러한 방식으로 학습 루프 내에서 직접 학습률을 업데이트할 수 있습니다.

이외에도 PyTorch에서는 학습률을 동적으로 조절할 수 있는 다양한 방법을 제공합니다. 적절한 방법을 선택하여 모델의 성능을 개선할 수 있습니다.

---

참고: 
- [PyTorch 공식 문서 - Learning Rate Scheduling](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler)
- [PyTorch 공식 문서 - reduce learning rate on plateau](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau)
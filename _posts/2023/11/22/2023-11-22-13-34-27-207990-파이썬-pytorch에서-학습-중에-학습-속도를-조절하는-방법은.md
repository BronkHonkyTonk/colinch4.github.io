---
layout: post
title: "[python] 파이썬 PyTorch에서 학습 중에 학습 속도를 조절하는 방법은?"
description: " "
date: 2023-11-22
tags: [python]
comments: true
share: true
---

# PyTorch에서 학습 속도 조절하기

PyTorch는 딥러닝 모델을 구축하고 학습시키는 데 사용되는 인기있는 프레임워크입니다. 학습 중에 학습 속도를 조절하는 것은 중요한 부분이며, PyTorch에서는 다양한 방법을 제공합니다. 이 기능들을 사용하여 학습 속도를 조절할 수 있습니다.

## 1. Learning Rate Schedulers 사용하기

`torch.optim.lr_scheduler` 모듈을 사용하여 학습 속도를 동적으로 조절할 수 있습니다. 이 모듈에는 여러 가지 스케줄러가 포함되어 있습니다. 각 스케줄러는 일정한 규칙에 따라 학습 속도를 조정합니다.

```python
import torch
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler

model = MyModel()
optimizer = optim.Adam(model.parameters(), lr=0.1)
scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

for epoch in range(num_epochs):
    # 학습 코드 작성
    scheduler.step()
```

위의 코드에서는 `StepLR` 스케줄러를 사용하여 학습 속도를 조절합니다. `step_size`와 `gamma` 매개변수를 통해 조절합니다. `step_size`는 학습 속도를 감소시키는 주기를 결정하고, `gamma`는 학습 속도를 감소시키는 비율을 결정합니다. 이 코드는 매 5번의 에폭마다 학습 속도를 0.1배씩 줄입니다.

## 2. Learning Rate를 직접 수정하기

`optimizer` 객체를 사용하여 학습 속도를 직접 수정할 수도 있습니다. `optimizer.param_groups`를 통해 현재 학습 속도를 가져올 수 있고, 이 값을 직접 변경하여 속도를 조절할 수 있습니다.

```python
import torch
import torch.optim as optim

model = MyModel()
optimizer = optim.Adam(model.parameters(), lr=0.1)

for epoch in range(num_epochs):
    # 학습 코드 작성
    if epoch == 10:
        for param_group in optimizer.param_groups:
            param_group['lr'] = 0.01
```

위의 코드에서는 에폭이 10일 때 학습 속도를 0.01로 변경하는 코드입니다. `optimizer.param_groups`를 사용하여 현재 학습 속도를 가져온 후, `param_group['lr']` 값을 변경하여 학습 속도를 조절합니다.

## 3. 다른 Learning Rate Schedulers 사용하기

PyTorch에는 위에서 언급한 `StepLR` 이외에도 다양한 스케줄러가 있습니다. 예를 들어 `MultiStepLR`, `ExponentialLR`, `ReduceLROnPlateau` 등 다양한 스케줄러를 사용할 수 있습니다. 상황에 맞게 적절한 스케줄러를 사용하여 학습 속도를 조절할 수 있습니다.

더 자세한 내용은 [공식 PyTorch 문서](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)를 참조하시기 바랍니다.

이렇게 PyTorch에서 학습 속도를 조절하는 방법을 알아보았습니다. 적절한 학습 속도 조절은 더 빠른 학습과 모델 성능 개선에 도움이 될 수 있습니다.
---
layout: post
title: "[python] Requests 라이브러리를 이용해 사이트 크롤링하는 방법은?"
description: " "
date: 2023-11-22
tags: [python]
comments: true
share: true
---
[Requests]: https://requests.readthedocs.io/en/latest/
[BeautifulSoup]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/

크롤링은 웹 페이지에서 데이터를 추출하는 프로세스를 말합니다. 파이썬에서는 `Requests` 라이브러리를 사용하여 웹 사이트를 크롤링할 수 있습니다. `Requests`는 HTTP 요청을 보내고 응답을 받는 기능을 제공하는 라이브러리입니다.

먼저, `Requests` 라이브러리를 설치해야 합니다. 파이썬 개발 환경에서 다음 명령을 사용하여 `pip`를 이용해 `Requests`를 설치합니다.

```shell
$ pip install requests
```

설치가 완료되면 다음과 같이 `Requests`를 import하여 웹 사이트에서 데이터를 가져올 수 있습니다.

```python
import requests
```

이제 `requests.get()` 메서드를 사용하여 웹 페이지의 HTML 문서를 요청하고, 이를 `response` 변수에 저장합니다. 다음으로 `response.text`를 사용하여 HTML 문서의 내용을 가져옵니다.

```python
url = "http://example.com"
response = requests.get(url)
html_content = response.text
```

이렇게 가져온 HTML 문서를 파싱하여 필요한 정보를 추출하기 위해 `BeautifulSoup` 라이브러리를 사용할 수 있습니다. `BeautifulSoup`는 HTML 문서를 파싱하고 조작하는 기능을 제공하는 라이브러리입니다.

`BeautifulSoup`를 사용하기 위해서는 먼저 라이브러리를 설치해야 합니다. 위에서 설명한 것과 같은 방법으로 `pip`를 이용해 `BeautifulSoup`를 설치할 수 있습니다.

```shell
$ pip install beautifulsoup4
```

설치가 완료되면 다음과 같이 `BeautifulSoup`를 import하여 HTML 문서를 파싱할 수 있습니다.

```python
from bs4 import BeautifulSoup
```

`BeautifulSoup`의 `find()` 메서드를 사용하여 원하는 태그를 찾을 수 있습니다. 예를 들어, `find()` 메서드를 이용해 `h1` 태그를 찾아서 텍스트를 출력하는 코드는 다음과 같습니다.

```python
soup = BeautifulSoup(html_content, "html.parser")
h1_tag = soup.find("h1")
print(h1_tag.text)
```

위의 코드에서 `html.parser`는 `BeautifulSoup`의 파서를 설정하는 부분입니다. 이 파서를 사용해 HTML 문서를 파싱할 수 있습니다.

이제 `Requests`와 `BeautifulSoup`를 이용해 웹 사이트를 크롤링하는 방법을 간단히 알아보았습니다. 원하는 데이터를 추출하기 위해 더 다양한 기능과 메서드를 사용할 수 있습니다. 자세한 내용은 [Requests][Requests]와 [BeautifulSoup][BeautifulSoup] 공식 문서를 참고하시기 바랍니다.
---
layout: post
title: "[python] Celery로 비동기적으로 주기적인 웹 스크레이핑을 처리하는 방법은 어떻게 되는가?"
description: " "
date: 2023-11-21
tags: [python]
comments: true
share: true
---

Celery는 Python으로 작성된 분산 작업 큐 시스템으로, 비동기 작업을 처리하는 데 사용됩니다. Celery를 사용하여 주기적으로 웹 스크레이핑 작업을 처리하는 방법에 대해 알아보겠습니다.

## Celery 설치

먼저 Celery를 설치해야 합니다. 아래 명령을 사용하여 Celery를 설치할 수 있습니다.

```bash
pip install celery
```

## Celery 프로젝트 설정

Celery를 사용하기 위해 프로젝트를 설정해야 합니다. 프로젝트 루트 디렉토리에 `celery.py` 파일을 생성하고 아래 코드를 작성합니다.

```python
from celery import Celery

# Celery 인스턴스 생성
app = Celery('myapp', broker='amqp://guest@localhost//', backend='rpc://')

# 스크레이퍼 작업 정의
@app.task
def scrape(url):
    # 스크레이핑 작업 수행
    # ...

```

`broker`에는 메시지 브로커의 URL을 설정합니다. `backend`에는 작업 결과를 저장할 백엔드 URL을 설정합니다.

## 스크레이핑 작업 예약

스크레이핑 작업을 예약하기 위해 `celery.py` 파일에 아래 코드를 추가합니다.

```python
from datetime import timedelta
from celery.schedules import crontab

# 주기적으로 스크레이핑 작업 예약
app.conf.beat_schedule = {
    'scrape-every-hour': {
        'task': 'myapp.scrape',
        'schedule': timedelta(hours=1),
        'args': ('http://example.com',),
    },
    'scrape-every-day': {
        'task': 'myapp.scrape',
        'schedule': crontab(hour=0, minute=0),
        'args': ('http://example.com',),
    },
}
```

`beat_schedule`에 주기적으로 실행할 작업을 정의할 수 있습니다. `schedule`에 작업을 실행할 간격을 설정하고, `args`에 작업에 전달할 인자를 설정합니다. 위의 예제에서는 매 시간마다 스크레이핑 작업을 수행하고, 매일 자정에도 스크레이핑 작업을 수행합니다.

## Celery 실행

Celery를 실행하기 위해 아래 명령을 실행합니다.

```bash
celery -A myapp worker --loglevel=info
```

이 명령은 background에서 Celery worker를 실행합니다.

주기적으로 예약된 작업을 실행하기 위해 아래 명령을 실행합니다.

```bash
celery -A myapp beat --loglevel=info
```

위의 명령을 실행하면 Celery Beat가 주기적으로 예약된 작업을 실행합니다.

## 참고 자료

- Celery 공식 문서: [http://docs.celeryproject.org](http://docs.celeryproject.org)
- Celery GitHub 레포지토리: [https://github.com/celery/celery](https://github.com/celery/celery)
---
layout: post
title: "[python] 웹 페이지 자동화 테스트에서 스크래핑 데이터 저장하기"
description: " "
date: 2023-11-21
tags: [python]
comments: true
share: true
---

웹 페이지 스크래핑은 자동화된 테스트 과정에서 많이 사용되는 기술입니다. 테스트를 수행할 때 웹 페이지의 데이터를 스크래핑하여 저장하면, 이후에 해당 데이터를 분석하거나 다른 용도로 사용할 수 있습니다. 이번 포스트에서는 Python을 사용하여 웹 페이지 자동화 테스트에서 스크래핑한 데이터를 저장하는 방법을 알아보겠습니다.

## 1. 필요한 라이브러리 설치하기

스크래핑을 위해 우선 필요한 라이브러리를 설치해야 합니다. Python에서 가장 많이 사용되는 스크래핑 라이브러리인 Beautiful Soup과 Requests를 설치해보겠습니다.

```python
pip install beautifulsoup4
pip install requests
```

## 2. 웹 페이지 스크래핑하기

스크래핑할 웹 페이지의 URL을 알고 있다면, Requests 라이브러리를 사용하여 해당 페이지의 HTML을 가져올 수 있습니다. Beautiful Soup을 사용하여 HTML을 구문 분석하고, 원하는 데이터를 추출할 수 있습니다.

```python
import requests
from bs4 import BeautifulSoup

# 스크래핑할 웹 페이지의 URL
url = "https://example.com"

# 웹 페이지의 HTML 가져오기
response = requests.get(url)
html = response.text

# HTML을 구문 분석하기
soup = BeautifulSoup(html, "html.parser")

# 원하는 데이터 추출하기
title = soup.find("h1").text
content = soup.find("div", class_="content").text
```

위 코드에서는 `requests.get()` 함수를 사용하여 웹 페이지의 HTML을 가져왔습니다. 그리고 `BeautifulSoup` 객체를 생성하여 HTML을 구문 분석하고, 원하는 데이터를 추출하는 예제입니다.

## 3. 데이터 저장하기

스크래핑한 데이터를 저장하기 위해서는 적절한 형식으로 데이터를 가공하고 저장해야 합니다. 예를들어, CSV 파일, JSON 파일 등 다양한 형식으로 데이터를 저장할 수 있습니다.

```python
import csv
import json

# CSV 파일로 데이터 저장하기
with open("data.csv", "w", newline="", encoding="utf-8") as file:
    writer = csv.writer(file)
    writer.writerow(["title", "content"])
    writer.writerow([title, content])

# JSON 파일로 데이터 저장하기
data = {
    "title": title,
    "content": content
}

with open("data.json", "w", encoding="utf-8") as file:
    json.dump(data, file, ensure_ascii=False)
```

위 코드에서는 `csv.writer()` 함수를 사용하여 CSV 파일에 데이터를 쓰고, `json.dump()` 함수를 사용하여 JSON 파일에 데이터를 쓰는 예제입니다.

## 마무리

이번 포스트에서는 Python을 사용하여 웹 페이지 자동화 테스트에서 스크래핑한 데이터를 저장하는 방법을 알아보았습니다. 웹 스크래핑은 많은 웹 애플리케이션에서 데이터 분석이나 자동화된 테스트에 유용하게 사용될 수 있는 기술이므로, 필요한 경우에 적극적으로 활용해 보시기 바랍니다.
---
layout: post
title: "[python] aiohttp를 사용하여 비동기적으로 웹 크롤러 결과를 CSV 파일로 가져오기"
description: " "
date: 2023-11-21
tags: [python]
comments: true
share: true
---

비동기 프로그래밍은 웹 크롤러와 같은 네트워크 연산을 수행할 때 매우 유용합니다. aiohttp는 asyncio 라이브러리와 함께 사용되는 파이썬의 비동기 웹 프레임워크입니다. 이를 사용하여 비동기적으로 웹 크롤러 결과를 CSV 파일로 저장하는 방법을 알아보겠습니다.

## 필요한 패키지 설치

먼저 `aiohttp` 패키지를 설치해야 합니다. 다음 명령을 사용하여 설치할 수 있습니다:

```
pip install aiohttp
```

또한, csv 파일에 결과를 저장하기 위해 `csv` 모듈도 import 해야 합니다:

```python
import csv
```

## 비동기 웹 크롤러 작성

다음은 aiohttp를 사용하여 비동기 웹 크롤러를 작성하는 예제입니다:

```python
import aiohttp
import asyncio

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def crawl(url_list):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for url in url_list:
            task = asyncio.ensure_future(fetch(session, url))
            tasks.append(task)
        return await asyncio.gather(*tasks)

# 크롤링할 URL 목록
urls = ['https://example.com', 'https://google.com', 'https://apple.com']

# 크롤러 실행
loop = asyncio.get_event_loop()
results = loop.run_until_complete(crawl(urls))
```

위의 코드에서 `fetch` 함수는 주어진 URL에서 데이터를 가져오는 비동기 함수입니다. `crawl` 함수는 주어진 URL 목록에 대해 비동기적으로 `fetch` 함수를 호출하고 결과를 반환합니다.

## 결과를 CSV 파일로 저장하기

다음은 aiohttp를 사용하여 비동기 크롤링 결과를 CSV 파일로 저장하는 예제입니다:

```python
# CSV 파일에 결과 저장
with open('results.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['URL', 'Result'])
    for url, result in zip(urls, results):
        writer.writerow([url, result])
```

위의 코드에서는 `open` 함수를 사용하여 CSV 파일을 열고, `csv.writer`를 사용하여 CSV 파일에 작성합니다. 각 행에는 URL과 해당 URL에서 가져온 결과가 저장됩니다.

## 결론

aiohttp를 사용하여 비동기적으로 웹 크롤러 결과를 CSV 파일로 저장하는 방법을 살펴보았습니다. 비동기적으로 작업을 수행하는 것은 크롤러와 같은 네트워크 연산에 매우 유용하며, 결과를 CSV 파일로 저장하는 것은 나중에 데이터 분석이나 가시화에 사용될 수 있습니다.
---
layout: post
title: "[python] NLTK를 사용해 문서의 동기를 분석하는 방법은 무엇인가요?"
description: " "
date: 2023-11-30
tags: [python]
comments: true
share: true
---

1. 토큰화(Tokenization): 문서를 단어, 문장 또는 다른 의미 단위로 나누는 작업입니다. NLTK의 `sent_tokenize`과 `word_tokenize` 함수를 사용하여 문장과 단어를 추출할 수 있습니다.

```python
from nltk.tokenize import sent_tokenize, word_tokenize

text = "문서의 동기를 분석하는 방법을 알고 싶습니다."
sentences = sent_tokenize(text)
words = word_tokenize(text)

print(sentences)
print(words)
```

2. 어휘 분석(Lexical Analysis): 텍스트의 어휘적 구성 요소를 분석합니다. 품사 태깅(Part-of-speech tagging)은 각 단어의 품사를 태깅하는 작업입니다. NLTK의 `pos_tag` 함수를 사용하여 품사 태깅을 수행할 수 있습니다.

```python
from nltk.tag import pos_tag

tagged_words = pos_tag(words)
print(tagged_words)
```

3. 구문 분석(Syntactic Parsing): 텍스트의 문장 구조를 분석하는 작업입니다. NLTK의 `RecursiveDescentParser`와 같은 구문 분석기를 사용하여 문장 구조를 추출할 수 있습니다.

```python
from nltk.parse import RecursiveDescentParser

grammar = nltk.CFG.fromstring("""
    S -> NP VP
    NP -> DT NN
    VP -> VB NN
    DT -> '문서의'
    NN -> '동기를' | '분석하는' | '방법을' | '알고' | '싶습니다'
    VB -> '알고' | '싶습니다'
""")
parser = RecursiveDescentParser(grammar)

trees = parser.parse(sentences[0].split())
for tree in trees:
    tree.pretty_print()
```

4. 의미 분석(Semantic Analysis): 텍스트의 의미와 관련된 작업입니다. NLTK에서는 WordNet과 같은 의미 데이터베이스를 활용하여 단어 간의 관계를 분석하고 유의어, 반의어 등을 추출할 수 있습니다.

위의 단계를 따라가면 문서의 동기를 분석하는 기본적인 방법을 구현할 수 있습니다. NLTK의 다양한 기능과 도구를 활용하여 보다 정교한 자연어 처리 작업을 수행할 수 있습니다.

자세한 내용은 NLTK 공식 문서(https://www.nltk.org/)를 참조하시기 바랍니다.
---
layout: post
title: "[java] Akka를 사용한 웹 크롤링 및 스크래핑 프로그래밍"
description: " "
date: 2023-11-30
tags: [java]
comments: true
share: true
---

이번 글에서는 Akka를 사용하여 웹 크롤링과 스크래핑 프로그래밍을 어떻게 할 수 있는지 알아보겠습니다.

## Akka란?

Akka는 높은 확장성과 견고성을 제공하는 액터 모델 기반의 분산 컴퓨팅 플랫폼입니다. 액터 모델은 동시에 실행되는 독립적인 액터들이 메세지를 주고받아 상호작용하는 모델로, 액터 간에는 상태를 공유하지 않고 비동기적으로 통신합니다.

## 웹 크롤링과 스크래핑

웹 크롤링은 웹 페이지를 주기적으로 방문하고, 웹 페이지의 내용을 추출하여 데이터를 수집하는 작업을 말합니다. 스크래핑은 웹 페이지에서 원하는 데이터를 추출하는 작업을 말합니다.

## Akka를 사용한 웹 크롤링 및 스크래핑

Akka의 액터 모델을 활용하면 병렬적으로 웹 페이지를 크롤링하여 빠르게 데이터를 수집할 수 있습니다. 액터 간에 메세지를 주고받아서 작업을 분산시키고, 결과를 수집하는 등의 다양한 기능을 구현할 수 있습니다.

아래는 Akka를 사용한 웹 크롤링 및 스크래핑을 위한 예제 코드입니다.

```java
import akka.actor.AbstractActor;
import akka.actor.ActorRef;
import akka.actor.ActorSystem;
import akka.actor.Props;
import akka.routing.RoundRobinPool;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

import java.io.IOException;

public class WebCrawler {

    public static class CrawlActor extends AbstractActor {
        @Override
        public Receive createReceive() {
            return receiveBuilder()
                    .match(String.class, url -> {
                        // 웹 페이지 크롤링 작업 수행
                        try {
                            Document doc = Jsoup.connect(url).get();
                            Elements links = doc.select("a[href]");
                            for (Element link : links) {
                                System.out.println(link.attr("abs:href"));
                            }
                        } catch (IOException e) {
                            e.printStackTrace();
                        }
                    })
                    .build();
        }
    }

    public static void main(String[] args) {
        ActorSystem system = ActorSystem.create("WebCrawlerSystem");
        ActorRef crawlActor = system.actorOf(Props.create(CrawlActor.class).withRouter(new RoundRobinPool(5)), "crawlActor");

        // 크롤링할 웹 페이지 URL 리스트
        String[] urls = {
                "https://example.com",
                "https://example.net",
                "https://example.org"
        };

        // 액터에 웹 페이지 URL 전달
        for (String url : urls) {
            crawlActor.tell(url, ActorRef.noSender());
        }

        system.terminate();
    }
}
```

이 예제 코드에서는 웹 페이지 크롤링을 담당하는 액터 `CrawlActor`를 생성하고, `RoundRobinPool`을 사용하여 액터를 병렬로 실행합니다. 액터는 받은 URL을 기반으로 해당 웹 페이지를 크롤링하고, 추출한 링크들을 출력합니다.

Akka를 사용하는 경우, 병렬로 여러 웹 페이지를 크롤링하거나 데이터를 스크래핑하는 작업을 효율적으로 처리할 수 있습니다.

## 결론

이번 글에서는 Akka를 사용하여 웹 크롤링과 스크래핑 프로그래밍을 어떻게 할 수 있는지 알아보았습니다. Akka의 액터 모델을 활용하면 병렬적으로 여러 웹 페이지를 처리하고, 데이터를 수집할 수 있습니다. Akka를 사용하면 확장성과 견고성이 높은 웹 크롤러 및 스크래퍼를 개발할 수 있습니다.

---

참고자료:
- [Akka Documentation](https://akka.io/docs/)
- [Jsoup - Java HTML Parser](https://jsoup.org/)
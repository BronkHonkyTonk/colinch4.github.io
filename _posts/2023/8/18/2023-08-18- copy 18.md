---
layout: post
title: "[python] Gradient Boosting"
description: " "
date: 2023-08-18
tags: [Python,SVM]
comments: true
share: true
---


Gradient Boosting은 앙상블 학습 기법 중 하나로, 여러 개의 약한 학습기(weak learner)를 조합하여 강력한 예측 모델을 생성하는 방법입니다. Gradient Boosting의 핵심 개념은 다음과 같습니다:

1.  **부스팅 (Boosting):** Gradient Boosting은 부스팅 알고리즘의 한 종류로, 약한 학습기를 순차적으로 학습하여 이전 모델의 오차를 보완하도록 설계되었습니다. 각각의 학습 단계에서 이전 모델이 잘못 예측한 샘플에 더 많은 가중치를 부여하고, 이를 이용하여 새로운 학습 모델을 생성합니다.
    
2.  **경사 하강법 (Gradient Descent):** Gradient Boosting에서는 손실 함수를 최소화하기 위해 경사 하강법을 사용합니다. 모델의 오차를 줄이기 위해 손실 함수의 그래디언트(기울기)를 계산하고, 그 방향으로 모델을 업데이트하여 손실을 최소화하는 방향으로 진행합니다.
    
3.  **약한 학습기 (Weak Learner):** Gradient Boosting에서 사용되는 각각의 모델은 약한 학습기입니다. 이는 간단한 모델이며, 높은 편향과 낮은 분산을 가지고 있습니다. 예를 들어, 의사결정트리의 한 종류인 "회귀 트리"나 "분류 트리"가 약한 학습기로 사용될 수 있습니다.
    
4.  **Residual Learning:** Gradient Boosting에서는 이전 모델의 예측값과 실제 타겟 값 사이의 잔차(residual)를 예측하는 방식으로 모델을 학습합니다. 이 잔차를 예측하는 새로운 모델을 생성하고, 이를 이전 모델의 예측값에 더하여 오차를 줄입니다.
    
5.  **앙상블의 결합:** Gradient Boosting은 순차적으로 모델을 학습하면서 각 모델의 예측 결과를 결합하여 최종 예측 결과를 생성합니다. 각 모델의 가중치를 조정하여 최종 예측을 수행합니다.
    

Gradient Boosting은 분류와 회귀 문제에서 모두 사용될 수 있으며, 일반적으로 고차원 데이터와 작은 샘플 크기에서도 좋은 성능을 보이며, 다른 앙상블 방법과 함께 머신러닝에서 매우 효과적으로 사용되는 기법 중 하나입니다.
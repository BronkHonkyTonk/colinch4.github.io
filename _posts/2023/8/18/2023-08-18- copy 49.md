---
layout: post
title: "[python] 워드 임베딩(Word Embedding) 개념과 예제"
description: " "
date: 2023-08-18
tags: [Python,딥러닝]
comments: true
share: true
---

워드 임베딩(Word Embedding)은 자연어 처리에서 단어를 벡터로 표현하는 기법으로, 단어의 의미와 관련성을 수치화하여 컴퓨터가 처리하기 쉬운 형태로 변환합니다. 워드 임베딩은 단어 간 유사도, 의미적 관계, 문맥을 반영하여 단어를 공간상에 표현하는데 사용됩니다.

간단한 워드 임베딩 예제로 Word2Vec 모델을 사용해보겠습니다. 이 예제에서는 Gensim 라이브러리를 활용합니다.

```python
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# 텍스트 데이터 준비
corpus = [
    "I enjoy reading books",
    "Natural language processing is fun",
    "Word embeddings capture semantic meaning"
]

# 간단한 전처리와 토큰화
tokenized_corpus = [simple_preprocess(sentence) for sentence in corpus]

# Word2Vec 모델 학습
model = Word2Vec(tokenized_corpus, vector_size=10, window=3, min_count=1, sg=1)

# 단어 'reading'의 임베딩 벡터 확인
embedding = model.wv['reading']
print("Embedding vector for 'reading':", embedding)` 
```
위의 코드에서는 Gensim 라이브러리를 사용하여 간단한 텍스트 데이터를 준비하고, Word2Vec 모델을 학습합니다. `vector_size`는 임베딩 벡터의 차원을 나타냅니다. 학습된 모델을 통해 단어 'reading'의 임베딩 벡터를 확인할 수 있습니다.

워드 임베딩은 단어 간 의미적 유사도를 계산하고, 단어들의 관계를 파악하는 데 유용합니다. 이를 통해 컴퓨터가 텍스트 데이터의 의미를 이해하고 처리할 수 있게 됩니다. Word2Vec 외에도 GloVe, FastText 등 다양한 워드 임베딩 알고리즘이 있습니다.
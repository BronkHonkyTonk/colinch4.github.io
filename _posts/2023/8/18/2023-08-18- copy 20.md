---
layout: post
title: "[python] 경사 하강법(Gradient Descent) 개념과 예제"
description: " "
date: 2023-08-18
tags: [Python,SVM]
comments: true
share: true
---


경사 하강법(Gradient Descent)은 함수의 최솟값을 찾는 최적화 알고리즘 중 하나로, 기울기(gradient) 정보를 사용하여 함수의 기울기가 감소하는 방향으로 이동하여 최솟값을 찾는 방법입니다. 머신러닝과 딥러닝에서 많이 사용되며, 모델의 파라미터를 조정하여 손실 함수를 최소화하는데 활용됩니다.

간단한 경사 하강법 예제로 설명해보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt

# 목적 함수 정의
def objective_function(x):
    return x**2 + 5 * x + 6

# 목적 함수의 미분 정의
def gradient(x):
    return 2 * x + 5

# 초기값 설정
initial_x = -7
learning_rate = 0.1
num_iterations = 50

# 경사 하강법 수행
x_values = [initial_x]
for i in range(num_iterations):
    current_x = x_values[-1]
    new_x = current_x - learning_rate * gradient(current_x)
    x_values.append(new_x)

# 결과 출력
final_x = x_values[-1]
print("Final x:", final_x)
print("Objective function at final x:", objective_function(final_x))

# 경사 하강법 경로 시각화
x_range = np.linspace(-10, 2, 100)
y_range = objective_function(x_range)
plt.plot(x_range, y_range, label='Objective Function')
plt.scatter(x_values, [objective_function(x) for x in x_values], c='r', label='Gradient Descent Path')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent')
plt.legend()
plt.show()` 
```
위의 코드에서는 간단한 1차 함수인 "f(x) = x^2 + 5x + 6"의 최솟값을 경사 하강법을 사용하여 찾습니다. 초기값을 설정하고, 미분한 함수의 기울기(gradient) 정보를 사용하여 반복적으로 파라미터를 조정하여 최솟값을 찾아갑니다.

시각화 부분에서는 목적 함수와 경사 하강법이 이동하는 경로를 함께 그래프로 표시하여 경사 하강법의 동작을 시각적으로 확인할 수 있습니다.
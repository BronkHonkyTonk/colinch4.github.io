---
layout: post
title: "[python] 앙상블 학습(Ensemble Learning) 개념과 예제"
description: " "
date: 2023-08-18
tags: [Python,SVM]
comments: true
share: true
---


앙상블 학습(Ensemble Learning)은 여러 개별 모델을 결합하여 더 강력하고 일반화된 모델을 만드는 기계 학습 기법입니다. 단일 모델보다 더 나은 성능을 얻기 위해 다양한 모델을 조합하는 방식으로 작동합니다. 앙상블 학습은 높은 예측 성능과 안정성을 제공하며, 다양한 종류의 데이터 분류 및 회귀 문제에서 사용됩니다.

앙상블 학습은 크게 두 가지 방식으로 구현됩니다:

1.  **보팅(Voting):** 다양한 모델을 학습하고, 각 모델의 예측 결과를 종합하여 최종 예측을 만드는 방식입니다. 보팅은 주로 분류 문제에 사용되며, 하드 보팅과 소프트 보팅 두 가지 방식이 있습니다.
    
2.  **부스팅(Boosting):** 여러 개의 약한 학습기(Weak Learner)를 순차적으로 학습하고 예측합니다. 각 모델은 이전 모델이 잘못 분류한 데이터에 더 가중치를 주어 학습합니다. 부스팅은 모델 간의 의존성이 있는 방식으로 작동합니다. 대표적인 부스팅 알고리즘으로는 AdaBoost, Gradient Boosting, XGBoost, LightGBM 등이 있습니다.
    

아래는 scikit-learn 라이브러리를 사용하여 보팅과 부스팅 앙상블 학습을 구현하는 예제 코드입니다.

**보팅 앙상블 학습 예제:**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# 데이터 로드
iris = load_iris()
X = iris.data
y = iris.target

# 분류기 생성
clf1 = LogisticRegression(random_state=1)
clf2 = DecisionTreeClassifier(random_state=1)
clf3 = SVC(probability=True, random_state=1)

# 보팅 분류기 생성
ensemble_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='soft')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# 보팅 분류기 학습 및 평가
ensemble_clf.fit(X_train, y_train)
accuracy = ensemble_clf.score(X_test, y_test)
print("Voting Classifier Accuracy:", accuracy)` 
```
**부스팅 앙상블 학습 예제:**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# 데이터 로드
iris = load_iris()
X = iris.data
y = iris.target

# 기본 분류기 생성
base_clf = DecisionTreeClassifier(max_depth=1)

# 부스팅 분류기 생성
boosting_clf = AdaBoostClassifier(base_estimator=base_clf, n_estimators=50, random_state=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# 부스팅 분류기 학습 및 평가
boosting_clf.fit(X_train, y_train)
accuracy = boosting_clf.score(X_test, y_test)
print("Boosting Classifier Accuracy:", accuracy)` 
```
위의 코드에서는 보팅과 부스팅 앙상블 학습을 구현하고 붓꽃(iris) 데이터를 사용하여 예측 성능을 평가합니다. 두 방식 모두 다양한 분류기를 결합하여 예측 결과를 향상시키는 방식으로 작동합니다.
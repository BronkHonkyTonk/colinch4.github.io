---
layout: post
title: "[python] 앙상블의 다양성과 일반화를 설명하기 위해 간단한 예제"
description: " "
date: 2023-08-18
tags: [Python,SVM]
comments: true
share: true
---


앙상블의 다양성과 일반화를 설명하기 위해 간단한 예제를 살펴보겠습니다. 아래 예제는 여러 개의 의사결정트리를 생성하여 다양성을 높이고 모델의 일반화 성능을 개선하는 것을 보여줍니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# 데이터 생성
X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=1)

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# 다양한 의사결정트리 생성
tree1 = DecisionTreeClassifier(random_state=1, max_depth=2)
tree2 = DecisionTreeClassifier(random_state=2, max_depth=2)
tree3 = DecisionTreeClassifier(random_state=3, max_depth=2)

# 의사결정트리 학습
tree1.fit(X_train, y_train)
tree2.fit(X_train, y_train)
tree3.fit(X_train, y_train)

# 예측 성능 평가
accuracy_tree1 = tree1.score(X_test, y_test)
accuracy_tree2 = tree2.score(X_test, y_test)
accuracy_tree3 = tree3.score(X_test, y_test)

print("Accuracy of Tree 1:", accuracy_tree1)
print("Accuracy of Tree 2:", accuracy_tree2)
print("Accuracy of Tree 3:", accuracy_tree3)

# 시각화
plt.figure(figsize=(12, 4))

plt.subplot(131)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, s=25)
plt.title("Original Data")

plt.subplot(132)
plt.scatter(X[:, 0], X[:, 1], c=tree1.predict(X), cmap=plt.cm.RdYlBu, s=25)
plt.title("Tree 1 Prediction")

plt.subplot(133)
plt.scatter(X[:, 0], X[:, 1], c=tree2.predict(X), cmap=plt.cm.RdYlBu, s=25)
plt.title("Tree 2 Prediction")

plt.tight_layout()
plt.show()` 
```
위의 코드에서는 간단한 2차원 데이터를 생성하고, 다양한 의사결정트리를 생성하여 학습 및 예측을 수행합니다. 각 의사결정트리는 서로 다른 random_state와 max_depth를 가지며, 이를 통해 다양성을 높이고 모델의 일반화 성능을 개선합니다.

시각화 부분에서는 원본 데이터와 두 개의 의사결정트리가 예측한 결과를 시각화하여 데이터의 분포와 의사결정 경계를 확인할 수 있습니다. 이를 통해 다양한 트리들이 서로 다른 예측 경계를 가지며, 이는 앙상블의 다양성을 나타내는 것을 확인할 수 있습니다.
---
layout: post
title: "[TensorFlow] Batch Normalization Layer 예제"
description: " "
date: 2023-08-17
tags: [tensorflow]
comments: true
share: true
---


`tf.keras.layers.BatchNormalization` 레이어는 학습 중에 각 미니 배치의 평균과 분산을 정규화하여 학습을 안정화시키는 레이어입니다. 이를 통해 학습 속도를 향상시키고, 더 깊은 신경망을 효과적으로 훈련시킬 수 있습니다. 아래는 `tf.keras.layers.BatchNormalization` 레이어를 사용한 예제입니다.

```python
import tensorflow as tf

## 입력 데이터 크기: 5 (예시로)
input_size = 5

## BatchNormalization 레이어 정의
batchnorm_layer = tf.keras.layers.BatchNormalization()

## 더미 입력 데이터 생성
dummy_input = tf.random.normal(shape=(1, input_size))  # 배치 크기 1로 가정

## BatchNormalization 레이어 실행
output = batchnorm_layer(dummy_input, training=True)  # training=True는 학습 모드에서 BatchNormalization을 활성화

print("Input shape:", dummy_input.shape)
print("Output shape:", output.shape)` 
```
위 예제에서는 `tf.keras.layers.BatchNormalization` 레이어를 사용하여 입력 크기가 5인 더미 데이터에 대한 레이어를 정의하고 실행합니다. `training=True`로 설정하여 학습 중에 BatchNormalization을 활성화합니다.

실행 결과:

```yaml
Input shape: (1, 5)
Output shape: (1, 5)` 
```
위에서 확인할 수 있듯이, 입력 데이터의 형태가 `(1, 5)`이며 출력 데이터의 형태도 동일합니다. BatchNormalization 레이어는 학습 중에만 활성화되므로, 학습이 아닌 추론(inference) 단계에서는 BatchNormalization이 적용되지 않습니다.

BatchNormalization은 주로 신경망 모델의 안정성과 학습 속도를 향상시키는 데 사용됩니다.
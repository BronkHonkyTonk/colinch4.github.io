---
layout: post
title: "[TensorFlow] Activation Layer 예제 "
description: " "
date: 2023-08-17
tags: [TensorFlow]
comments: true
share: true
---


`tf.keras.layers.Activation` 레이어는 다양한 활성화 함수를 적용하기 위해 사용됩니다. 활성화 함수는 뉴런의 출력을 변환하는 함수로, 신경망의 비선형성을 도입하고 모델의 표현력을 향상시키는 역할을 합니다. 아래는 `tf.keras.layers.Activation` 레이어를 사용한 예제입니다.

```python
import tensorflow as tf

## 더미 데이터 생성
dummy_data = tf.random.normal(shape=(4, 8))

## Activation 레이어 정의 (ReLU 활성화 함수)
activation_layer = tf.keras.layers.Activation('relu')

## Activation 레이어 실행
activated_data = activation_layer(dummy_data)

print("Input data:")
print(dummy_data)
print("\nActivated data:")
print(activated_data)` 
```
위 예제에서는 `tf.keras.layers.Activation` 레이어를 사용하여 4x8 크기의 더미 데이터에 대한 레이어를 정의하고 실행합니다. `'relu'`로 설정하여 ReLU (Rectified Linear Unit) 활성화 함수를 적용합니다.

실행 결과 예시:

```lua
Input data:
[[-0.26600003 -1.2244318   1.1742276  -0.39295354 -0.4721867   0.09440519  0.3388326   1.134619  ]
 [ 0.4711468  -0.12247454 -1.4646811  -1.0220486   0.49504653  0.05365299 -0.50607544  1.3647636 ]
 [ 1.3306162  -1.0787169   0.06737287  1.6701592   0.8705249   0.16784191 -0.00591818 -1.0294815 ]
 [ 0.31209332  0.02744072  0.5697868  -0.25555646 -1.503792    0.5978992  -0.3693774  -1.0019941 ]]

Activated data:
[[0.         0.         1.1742276  0.         0.         0.09440519 0.3388326  1.134619  ]
 [0.4711468  0.         0.         0.         0.49504653 0.05365299 0.         1.3647636 ]
 [1.3306162  0.         0.06737287 1.6701592  0.8705249  0.16784191 0.         0.        ]
 [0.31209332 0.02744072 0.5697868  0.         0.         0.5978992  0.         0.        ]]` 
```
위에서 확인할 수 있듯이, 입력 데이터와 활성화 함수를 적용한 데이터가 출력됩니다. `tf.keras.layers.Activation` 레이어를 사용하여 다양한 활성화 함수를 적용하여 모델을 구성할 수 있습니다.
---
layout: post
title: "[TensorFlow] Gated Recurrent Unit Layer 예제"
description: " "
date: 2023-08-17
tags: [Python,TensorFlow]
comments: true
share: true
---


`tf.keras.layers.GRU` 레이어는 LSTM과 유사한 순환 신경망 레이어로, 기억 상태 관리를 위해 게이트 메커니즘을 사용합니다. LSTM보다 간단한 구조로 사용되며, 시간 단계가 긴 시퀀스 처리에 효과적입니다. 아래는 `tf.keras.layers.GRU` 레이어를 사용한 예제입니다.

```python
import tensorflow as tf

# 입력 시퀀스 길이와 특성 차원 (예시로)
sequence_length = 10
feature_dim = 5

# GRU 레이어 정의
gru_layer = tf.keras.layers.GRU(units=8, return_sequences=True, input_shape=(sequence_length, feature_dim))

# 더미 입력 시퀀스 생성
dummy_input = tf.random.normal(shape=(1, sequence_length, feature_dim))  # 배치 크기 1로 가정

# GRU 레이어 실행
output = gru_layer(dummy_input)

print("Input shape:", dummy_input.shape)
print("Output shape:", output.shape)` 
```
위 예제에서는 `tf.keras.layers.GRU` 레이어를 사용하여 길이가 10인 시퀀스에 대한 레이어를 정의하고 실행합니다. `units` 매개변수로 GRU 뉴런의 수를 지정하며, `return_sequences=True`로 설정하여 출력을 시퀀스 형태로 반환합니다.

실행 결과:

```yaml
Input shape: (1, 10, 5)
Output shape: (1, 10, 8)` 
```
위에서 확인할 수 있듯이, 입력 시퀀스의 형태가 `(1, 10, 5)`이고 출력 시퀀스의 형태가 `(1, 10, 8)`임을 확인할 수 있습니다. GRU 레이어는 입력 시퀀스의 각 타임 스텝마다 정보를 갱신하고 출력을 생성하는 데 사용됩니다.

GRU는 주로 시계열 데이터나 문장 처리와 같은 순차적인 데이터에서 사용되며, LSTM보다 계산 효율성과 학습 속도에서 우수한 결과를 보이는 경우가 있습니다.
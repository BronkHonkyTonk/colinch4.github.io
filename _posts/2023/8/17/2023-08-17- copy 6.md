---
layout: post
title: "[TensorFlow] Dropout Layer 예제"
description: " "
date: 2023-08-17
tags: [Python,TensorFlow]
comments: true
share: true
---


`tf.keras.layers.Dropout` 레이어는 과적합을 방지하기 위해 무작위로 일부 뉴런을 제거하는 레이어입니다. 이를 통해 모델이 특정 뉴런에 지나치게 의존하지 않도록 하고, 일종의 앙상블 효과를 가져와 일반화 능력을 향상시킵니다. 아래는 `tf.keras.layers.Dropout` 레이어를 사용한 예제입니다.

```python
import tensorflow as tf

## 입력 데이터 크기: 10 (예시로)
input_size = 10

## Dropout 레이어 정의
dropout_layer = tf.keras.layers.Dropout(rate=0.5)

## 더미 입력 데이터 생성
dummy_input = tf.random.normal(shape=(1, input_size))  # 배치 크기 1로 가정

## Dropout 레이어 실행
output = dropout_layer(dummy_input, training=True)  # training=True는 학습 모드에서 Dropout을 활성화

print("Input shape:", dummy_input.shape)
print("Output shape:", output.shape)` 
```
위 예제에서는 `tf.keras.layers.Dropout` 레이어를 사용하여 입력 크기가 10인 더미 데이터에 대한 레이어를 정의하고 실행합니다. `rate` 매개변수로 제거할 뉴런의 비율을 설정합니다. `training=True`로 설정하여 학습 중에 Dropout을 활성화합니다.

실행 결과:

```yaml
Input shape: (1, 10)
Output shape: (1, 10)` 
```
위에서 확인할 수 있듯이, 입력 데이터의 형태가 `(1, 10)`이며 출력 데이터의 형태도 동일합니다. Dropout 레이어는 학습 중에만 활성화되므로, 학습이 아닌 추론(inference) 단계에서는 Dropout이 적용되지 않습니다.

Dropout은 주로 신경망 모델의 복잡도를 줄이고 과적합을 방지하기 위해 사용됩니다.
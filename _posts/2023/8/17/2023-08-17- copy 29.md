---
layout: post
title: "[TensorFlow] TimeDistributed 레이어 예제"
description: " "
date: 2023-08-17
tags: [Python,TensorFlow]
comments: true
share: true
---


`tf.keras.layers.TimeDistributed` 레이어는 시퀀스 내 각 시간 단계에 대해 동일한 레이어를 적용하는 데 사용됩니다. 주로 시계열 데이터나 시퀀스 데이터의 각 요소에 대해 동일한 레이어를 적용하고자 할 때 활용됩니다. 아래는 `TimeDistributed` 레이어를 사용한 예제입니다.

```python
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, TimeDistributed
from tensorflow.keras.models import Model

# 더미 시퀀스 데이터 생성
dummy_sequence = tf.random.normal(shape=(4, 10, 32))  # 4개의 시퀀스, 각 시퀀스 길이: 10, 특성 개수: 32

# 입력 레이어 정의
input_layer = Input(shape=(10, 32))

# TimeDistributed Dense 레이어 정의
time_distributed_layer = TimeDistributed(Dense(64, activation='relu'))(input_layer)

# 모델 생성
model = Model(inputs=input_layer, outputs=time_distributed_layer)

# 모델 컴파일 (TimeDistributed 레이어에는 별도의 컴파일이 필요하지 않음)
model.compile(optimizer='adam', loss='mse')

# 모델 요약
model.summary()

# 모델 실행
output = model.predict(dummy_sequence)

print("Input shape:", dummy_sequence.shape)
print("Output shape:", output.shape)` 
```
위 예제에서는 입력 레이어와 `TimeDistributed` 레이어를 사용하여 시퀀스 내 각 시간 단계에 대해 `Dense` 레이어를 적용합니다. 이렇게 하면 각 시간 단계마다 독립적인 `Dense` 레이어가 생성되며, 시퀀스의 모든 단계에 대해 같은 가중치를 사용합니다.

실행 결과 예시:

```yaml
Input shape: (4, 10, 32)
Output shape: (4, 10, 64)` 
```
위에서 확인할 수 있듯이, `TimeDistributed` 레이어를 사용하여 입력 시퀀스의 각 시간 단계에 대해 `Dense` 레이어가 적용되어 출력이 생성됩니다.